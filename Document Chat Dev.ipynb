{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavesh/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz\n",
    "from llm_utils import to_markdown\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import MarkdownNodeParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"pdfs/2403.19887v1.Jamba__A_Hybrid_Transformer_Mamba_Language_Model.pdf\"\n",
    "paper_pdf = fitz.open(pdf_path)\n",
    "md_text = to_markdown(paper_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create document object from the markdown text\n",
    "document_paper = Document(text=md_text)\n",
    "\n",
    "# Breakdown the document into nodes using the MarkdownNodeParser\n",
    "parser = MarkdownNodeParser()\n",
    "nodes = parser.get_nodes_from_documents([document_paper])  # Returns a list of nodes\n",
    "\n",
    "# Remove empty nodes or nodes with less than 10 characters\n",
    "nodes = [node for node in nodes if len(node.text.strip()) > 10]\n",
    "\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed PDF Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Embedding\n",
    "response = oai_client.embeddings.create(\n",
    "    input=\"This is a sample text\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    response = oai_client.embeddings.create(\n",
    "        input=node.get_content(metadata_mode=\"all\"),\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    node.embedding = response.data[0].embedding\n",
    "\n",
    "    # Add original text to metadata as well for reference\n",
    "    node.metadata['text'] = node.text\n",
    "    node.id_ = i  # Assign straightforward index as ids for now\n",
    "\n",
    "# Note:\n",
    "# This is a simple example of how to index the nodes using openai embeddings one by one.\n",
    "# In practice, you should batch the embeddings and index them in bulk and not use a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Header_2': 'A Hybrid Transformer-Mamba Language Model',\n",
       " 'text': 'A Hybrid Transformer-Mamba Language Model\\n\\n**Opher Lieber** _∗_ **Barak Lenz** _∗_ **Hofit Bata** **Gal Cohen** **Jhonathan Osin**\\n**Itay Dalmedigos** **Erez Safahi** **Shaked Meirom** **Yonatan Belinkov**\\n**Shai Shalev-Shwartz** **Omri Abend** **Raz Alon** **Tomer Asida**\\n**Amir Bergman** **Roman Glozman** **Michael Gokhman** **Avashalom Manevich**\\n**Nir Ratner** **Noam Rozen** **Erez Shwartz** **Mor Zusman** **Yoav Shoham**'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"text\": \"A Hybrid Transformer-Mamba Language Model\\\\n\\\\n**Opher Lieber** _\\\\u2217_ **Barak Lenz** _\\\\u2217_ **Hofit Bata** **Gal Cohen** **Jhonathan Osin**\\\\n**Itay Dalmedigos** **Erez Safahi** **Shaked Meirom** **Yonatan Belinkov**\\\\n**Shai Shalev-Shwartz** **Omri Abend** **Raz Alon** **Tomer Asida**\\\\n**Amir Bergman** **Roman Glozman** **Michael Gokhman** **Avashalom Manevich**\\\\n**Nir Ratner** **Noam Rozen** **Erez Shwartz** **Mor Zusman** **Yoav Shoham**\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"Abstract\", \"text\": \"Abstract\\\\n\\\\nWe present Jamba, a new base large language model based on a novel hybrid\\\\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\\\\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of both\\\\nmodel families. MoE is added in some of these layers to increase model capacity\\\\nwhile keeping active parameter usage manageable. This flexible architecture allows\\\\nresource- and objective-specific configurations. In the particular configuration we\\\\nhave implemented, we end up with a powerful model that fits in a single 80GB\\\\nGPU. Built at large scale, Jamba provides high throughput and small memory\\\\nfootprint compared to vanilla Transformers, and at the same time state-of-the-art\\\\nperformance on standard language model benchmarks and long-context evaluations.\\\\nRemarkably, the model presents strong results for up to 256K tokens context length.\\\\nWe study various architectural decisions, such as how to combine Transformer and\\\\nMamba layers, and how to mix experts, and show that some of them are crucial\\\\nin large scale modeling. We also describe several interesting properties of these\\\\narchitectures which the training and evaluation of Jamba have revealed, and plan to\\\\nrelease checkpoints from various ablation runs, to encourage further exploration\\\\nof this novel architecture. We make the weights of our implementation of Jamba\\\\npublicly available under a permissive license.\\\\n\\\\n\\\\n**Model:** `https://huggingface.co/ai21labs/Jamba-v0.1`\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"1 ### Introduction\", \"text\": \"1 ### Introduction\\\\n\\\\nWe introduce Jamba, a new publicly available large language model. Jamba is based on a novel\\\\nhybrid architecture, which combines Transformer layers [ 46 ] with Mamba layers [ 16 ], a recent\\\\nstate-space model [ 17 , 18 ], as well as a mixture-of-experts (MoE) component [ 13 , 41 ]. Jamba thus\\\\ncombines two orthogonal architectural designs that together give it improved performance and higher\\\\nthroughput, while maintaining a manageable memory footprint. The 7B-based Jamba model (12B\\\\nactive parameters, 52B total available parameters) we are releasing was designed to fit in a single\\\\n80GB GPU, but the Jamba architecture supports other design choices, depending on one\\\\u2019s hardware\\\\nand performance requirements.\\\\n\\\\n_\\\\u2217_ Equal contribution.\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\nThe fundamental novelty of Jamba is its hybrid Transformer-Mamba architecture (though see mention\\\\nbelow of recent related efforts). Despite the immense popularity of the Transformer as the predominant\\\\narchitecture for language models, it suffers from two main drawbacks. First, its high memory and\\\\ncompute requirements hinders the processing of long contexts, where the key-value (KV) cache size\\\\nbecomes a limiting factor. Second, its lack of a single summary state entails slow inference and low\\\\nthroughput, since each generated token performs a computation on the entire context. In contrast,\\\\nolder recurrent neural network (RNN) models, which summarize an arbitrarily long context in a\\\\nsingle hidden state, do not suffer from these limitations. RNN models have their own shortcomings,\\\\nhowever. They are costly to train since training cannot be parallelized across time steps. And they\\\\nstruggle with long distance relationships, which the hidden state captures to only a limited extent.\\\\n\\\\nRecent state space models (SSMs) like Mamba are more efficient to train than RNNs and are more\\\\ncapable at handling long distance relationships, but still lag behind the performance of comparably\\\\nsized Transformer language models. Taking advantage of both model families, Jamba combines\\\\nTransformer and Mamba layers, at a certain ratio. Varying the ratio of Transformer/Mamba layers\\\\nallows balancing memory usage, efficient training, and long context capabilities.\\\\n\\\\nA few other recent attempts to combine Attention and SSM modules are worth noting. [ 50 ] mixes\\\\nan S4 layer [ 17 ] with a local attention layer, followed by a sequence of local attention layers; it\\\\nshows experiments with small models and simple tasks. [ 16 ] reports that interleaving Mamba and\\\\nattention layers is only slightly better than pure Mamba in terms of perplexity, with models up to\\\\n1.3B parameters. [ 33 ] starts with an SSM layer followed by chunk-based Transformers, with models\\\\nup to 1.3B showing improved perplexity. [ 12 ] adds an SSM layer before the self-attention in a\\\\nTransformer layer, while [ 38 ] adds the SSM after the self-attention, both showing improvements on\\\\nspeech recognition. [ 32 ] replaces the MLP layers in the Transformer by Mamba layers, and shows\\\\nbenefits in simple tasks. These efforts are different from Jamba both in the particular way in which\\\\nthe SSM component is mixed with the attention one, and in the scale of implementation. Closest are\\\\nperhaps H3 [ 14 ], a specially designed SSM that enables induction capabilities, and a generalization\\\\ncalled Hyena [ 35 ]. The former proposed a hybrid architecture that replaces the second and middle\\\\nlayers with self-attention, and was implemented with up to 2.7B parameters and 400B training tokens.\\\\nHowever, as shown in [ 16 ], its perfomance lags that of pure Mamba. Based on Hyena, StripedHyena\\\\n[ 36 ] interleaves attention and SSM layers in a 7B parameter model. However, it lags behind the\\\\nAttention-only Mistral-7B [ 22 ]. All of this renders Jamba the first production-grade Attention-SSM\\\\nhybrid model. Scaling the hybrid Jamba architecture required overcoming several obstacles, which\\\\nwe dicsuss in Section 6 .\\\\n\\\\nJamba also includes MoE layers [ 13 , 41 ], which allow increasing the model capacity (total number of\\\\navailable parameters) without increasing compute requirements (number of active parameters). MoE\\\\nis a flexible approach that enables training extremely large models with strong performance [ 23 ]. In\\\\nJamba, MoE is applied to some of the MLP layers. The more MoE layers, and the more experts in\\\\neach MoE layer, the larger the total number of model parameters. In contrast, the more experts we use\\\\nat each forward pass, the larger the number of active parameters as well as the compute requirement.\\\\nIn our implementation of Jamba, we apply MoE at every other layer, with 16 experts and the top-2\\\\nexperts used at each token (a more detailed discussion of the model architecture is provided below).\\\\n\\\\nWe evaluated our implementation of Jamba on a wide range of benchmarks and found it performs\\\\ncomparably to Mixtral-8x7B [ 23 ], which has a similar number of parameters, and also to the larger\\\\nLlama-2 70B [ 45 ]. In addition, our model supports a context length of 256K tokens \\\\u2013 the longest\\\\nsupported context length for production-grade publicly available models. On long-context evaluations,\\\\nJamba outperformes Mixtral on most of the evaluated datasets. At the same time, Jamba is extremely\\\\nefficient; for example, its throughput is 3x that of Mixtral-8x7B for long contexts. Moreover, our\\\\nmodel fits in a single GPU (with 8bit weights) even with contexts of over 128K tokens, which is\\\\nimpossible with similar-size attention-only models such as Mixtral-8x7B.\\\\n\\\\nSomewhat unusually for a new architecture, we release Jamba (12B active parameters, 52B total avail-\\\\nable parameters) under Apache 2.0 license: `https://huggingface.co/ai21labs/Jamba-v0.1` .\\\\nWe do so since we feel that the novel architecture of Jamba calls for further study, experimentation,\\\\nand optimization by the community. Our design was based on various ablation experiments we\\\\nconducted to explore the effect of different tradeoffs and design choices, and insights gleaned from\\\\nthose. These ablations were performed at scales of up to 7B parameters, and training runs of up to\\\\n250B tokens. We plan to release model checkpoints from these runs.\\\\n\\\\n2\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\nMLP\\\\n\\\\nMoE\\\\n\\\\nMamba MoE layer\\\\n\\\\nRMSNorm\\\\n\\\\nRMSNorm\\\\n\\\\nMamba layer\\\\n\\\\nAttention\\\\n\\\\nAttention\\\\n\\\\nMamba MoE layer\\\\n\\\\nRMSNorm\\\\n\\\\nRMSNorm\\\\n\\\\nTransformer layer Attention MoE layer\\\\n\\\\nTransformer layer\\\\n\\\\nMamba MoE layer\\\\n\\\\nMLP\\\\n\\\\nMoE\\\\n\\\\nRMSNorm\\\\n\\\\nRMSNorm\\\\n\\\\nMamba layer\\\\n\\\\nMamba\\\\n\\\\nMamba\\\\n\\\\nMamba MoE layer\\\\n\\\\nRMSNorm\\\\n\\\\nRMSNorm\\\\n\\\\nMamba layer\\\\n\\\\nMamba layer Mamba MoE layer\\\\n\\\\n**(a)** Jamba block **(b)** Different types of layers\\\\n\\\\nFigure 1: **(a)** A single Jamba block. **(b)** Different types of layers. The implementation shown here is\\\\nwith _l_ = 8 , _a_ : _m_ = 1 : 7 ratio of attention-to-Mamba layers, and MoE applied every _e_ = 2 layers.\\\\n\\\\n**_Important notice_** _: The Jamba model released is a pretrained_ **_base_** _model, which did not go through_\\\\n_alignment or instruction tuning, and does not have moderation mechanisms. It should not be used in_\\\\n_production environments or with end users without additional adaptation._\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"2 ### Model Architecture\", \"text\": \"2 ### Model Architecture\\\\n\\\\nJamba is a hybrid decoder architecture that mixes Transformer layers [ 46 ] with Mamba layers [ 16 ], a\\\\nrecent state-space model (SSM) [ 17 , 18 ], in addition to a mixture-of-experts (MoE) module [ 13 , 41 ].\\\\nWe call the combination of these three elements a Jamba block. See Figure 1 for an illustration.\\\\n\\\\nCombining Transformer, Mamba, and MoE elements allows flexibility in balancing among the\\\\nsometimes conflicting objectives of low memory usage, high throughput, and high quality. In terms\\\\nof memory usage, note that comparing the total size of the model parameters can be misleading.\\\\nIn an MoE model, the number of active parameters that participate in any given forward step may\\\\nbe much smaller than the total number of parameters. Another important consideration is the KV\\\\ncache \\\\u2013 the memory required to store the attention keys and values in the context. When scaling\\\\nTransformer models to long contexts, the KV cache becomes a limiting factor. Trading off attention\\\\nlayers for Mamba layers reduces the total size of the KV cache. Our architecture aims to provide\\\\n\\\\n3\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\nnot only a small number of active parameters but also an 8x smaller KV cache compared to a vanilla\\\\nTransformer. Table 1 compares Jamba with recent publicly available models, showing its advantage\\\\nin maintaining a small KV cache even with 256K token contexts.\\\\n\\\\nAvailable params Active params KV cache (256K context, 16bit)\\\\n\\\\nLLAMA-2 6.7B 6.7B 128GB\\\\nMistral 7.2B 7.2B 32GB\\\\nMixtral 46.7B 12.9B 32GB\\\\nJamba 52B 12B 4GB\\\\n\\\\nTable 1: Comparison of Jamba and recent open models in terms of total available parameters, active\\\\nparameters, and KV cache memory on long contexts. Jamba provides a substantial reduction in the\\\\nKV cache memory requirements.\\\\n\\\\nIn terms of throughput, with short sequences, attention operations take up a small fraction of the\\\\ninference and training FLOPS [ 6 ]. However, with long sequences, attention hogs most of the compute.\\\\nIn contrast, Mamba layers are more compute-efficient. Thus, increasing the ratio of Mamba layers\\\\nimproves throughput especially for long sequences.\\\\n\\\\nHere is a description of the main configuration, which provides improved performance and efficiency.\\\\nSection 6 contains results from ablation experiments supporting the design choices.\\\\n\\\\nThe basic component is a Jamba block, which may be repeated in sequence. Each Jamba block is a\\\\ncombination of Mamba or Attention layers. Each such layer contains either an attention or a Mamba\\\\nmodule, followed by a multi-layer perceptron (MLP). The different possible types of layers are shown\\\\nin Figure 1 (b). 2  A Jamba block contains _l_ layers, which are mixed at a ratio of _a_ : _m_ , meaning _a_\\\\nattention layers for every _m_ Mamba layers.\\\\n\\\\nIn Jamba, some of the MLPs may be replaced by MoE layers, which helps increase the model capacity\\\\nwhile keeping the active number of parameters, and thus the compute, small. The MoE module may\\\\nbe applied to MLPs every _e_ layers. When using MoE, there are _n_ possible experts per layer, with a\\\\nrouter choosing the top _K_ experts at each token. In summary, the different degrees of freedom in the\\\\nJamba architecture are:\\\\n\\\\n- _l_ : The number of layers.\\\\n\\\\n- _a_ : _m_ : ratio of attention-to-Mamba layers.\\\\n\\\\n- _e_ : how often to use MoE instead of a single MLP.\\\\n\\\\n- _n_ : total number of experts per layer.\\\\n\\\\n- _K_ : number of top experts used at each token.\\\\n\\\\nGiven this design space, Jamba provides flexibility in preferring certain properties over others. For\\\\nexample, increasing _m_ and decreasing _a_ , that is, increasing the ratio of Mamba layers at the expense\\\\nof attention layers, reduces the required memory for storing the key-value cache. This reduces the\\\\noverall memory footprint, which is especially important for processing long sequences. Increasing the\\\\nratio of Mamba layers also improves throughput, especially at long sequences. However, decreasing\\\\n_a_ might lower the model\\\\u2019s capabilities.\\\\n\\\\nAdditionally, balancing _n_ , _K_ , and _e_ affects the relationship between active parameters and total\\\\navailable parameters. A larger _n_ increases the model capacity at the expense of memory footprint,\\\\nwhile a larger _K_ increases the active parameter usage and the compute requirement. In contrast, a\\\\nlarger _e_ decreases the model capacity, while decreasing both compute (when _K_ &gt;1) and memory\\\\nrequirements, and allowing for less communication dependencies (decreasing memory transfers as\\\\nwell as inter-GPU communication during expert-parallel training and inference).\\\\n\\\\nJamba\\\\u2019s implementation of Mamba layers incorporate several normalizations that help stabilize\\\\ntraining in large model scales. In particular, we apply RMSNorm [ 48 ] in the Mamba layers.\\\\n\\\\n2 The figure shows a potential Attention MoE layer, which our architecture does not use, but future variants\\\\ncould.\\\\n\\\\n4\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\nWe found that with the Mamba layer, positional embeddings or mechanisms like RoPE [ 42 ] are not\\\\nnecessary, and so we do not use any explicit positional information.\\\\n\\\\nOther architecture details are standard, including grouped-query attention (GQA), SwiGLU activation\\\\nfunction [ 6 , 40 , 45 ], and load balancing for the MoE [ 13 ]. The vocabulary size is 64K. The tokenizer\\\\nis trained with BPE [ 15 , 29 , 39 ] and each digit is a separate token [ 6 ]. We also remove the dummy\\\\nspace used in Llama and Mistral tokenizers for more consistent and reversible tokenization.\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"3 ### Reaping the Benefits\", \"text\": \"3 ### Reaping the Benefits\\\\n\\\\n**3.1** **Jamba Implementation for a Single 80GB GPU**\\\\n\\\\nThe specific configuration in our implementation was chosen to fit in a single 80GB GPU, while\\\\nachieving best performance in the sense of quality and throughput. In our implementation we have a\\\\nsequence of 4 Jamba blocks. Each Jamba block has the following configuration:\\\\n\\\\n- _l_ = 8 : The number of layers.\\\\n\\\\n- _a_ : _m_ = 1 : 7 : ratio attention-to-Mamba layers.\\\\n\\\\n- _e_ = 2 : how often to use MoE instead of a single MLP.\\\\n\\\\n- _n_ = 16 : total number of experts.\\\\n\\\\n- _K_ = 2 : number of top experts used at each token.\\\\n\\\\nThe _a_ : _m_ = 1 : 7 ratio was chosen according to preliminary ablations, as shown in Section 6 , since\\\\nthis ratio was the most compute-efficient variant amongst the best performing variants in terms of\\\\nquality.\\\\n\\\\nThe configuration of the experts was chosen to enable the model to fit in a single 80GB GPU (with\\\\nint8 weights), while including sufficient memory for the inputs. In particular, _n_ and _e_ were balanced\\\\nto have an average of _\\\\u223c_ 8 experts per layer. In addition, we balanced _n_ , _K_ , and _e_ to allow for\\\\nhigh quality, while keeping both compute requirements and communication dependencies (memory\\\\ntransfers) checked. Accordingly, we chose to replace the MLP module with MoE on every other\\\\nlayer, as well as have a total of 16 experts, two of which are used at each token. These choices were\\\\ninspired by prior work on MoE [ 7 , 49 ] and verified in preliminary experiments.\\\\n\\\\nFigure 2 shows the maximal context length that fits a single 80GB GPU with our Jamba implementa-\\\\ntion compared to Mixtral 8x7B and Llama-2-70B. Jamba provides 2x the context length of Mixtral\\\\nand 7x that of Llama-2-70B.\\\\n\\\\nContext length fitting a single 80GB A100 GPU\\\\n\\\\n140K\\\\n\\\\n120K\\\\n\\\\n100K\\\\n\\\\n80K\\\\n\\\\n60K\\\\n\\\\n\\\\n40K\\\\n\\\\n20K\\\\n\\\\n0K\\\\n\\\\nLlama-2 70B Mixtral 8x7B Jamba\\\\n\\\\nFigure 2: Comparison of maximum context length fitting in a single A100 80GB GPU. Jamba enables\\\\n2x the context length of Mixtral and 7x that of Llama-2-70B.\\\\n\\\\nOverall, our Jamba implementation was successfully trained on context lengths of up to 1M tokens.\\\\nThe released model supports lengths of up to 256K tokens.\\\\n\\\\n5\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n**3.2** **Throughput Analysis**\\\\n\\\\nFor concreteness, we present results of the throughput in two specific settings. 3  In the first setting, we\\\\nhave varying batch size, a single A100 80 GB GPU, int8 quantization, 8K context length, generating\\\\noutput of 512 tokens. As Figure 3a shows, Jamba allows processing of large batches, leading to a 3x\\\\nincrease in throughput (tokens/second) over Mixtral, which does not fit with a batch of 16 despite\\\\nhaving a similar number of active parameters.\\\\n\\\\nIn the second setting, we have a single batch, 4 A100 GPUs, no quantization, varying context lengths,\\\\ngenerating output of 512 tokens. As demonstrated in Figure 3b , at small context lengths all models\\\\nhave a similar throughput. Jamba excels at long contexts; with 128K tokens its throughput is 3x that\\\\nof Mixtral. Note that this is despite the fact that Jamba has not yet enjoyed optimizations of the kind\\\\nthe community has developed for pure Transformer models over the past six years. We can expect\\\\nthe throughut gap to increase as such optimizations are developed also for Jamba.\\\\n\\\\nThroughput (4 A100 GPUs)\\\\n\\\\nLlama-2 13B Llama-2 70B Mixtral 8x7B Jamba\\\\n\\\\nThroughput (single GPU)\\\\n\\\\n2000\\\\n\\\\nLlama-2 13B Llama-2 70B Mixtral 8x7B Jamba\\\\n\\\\n2000\\\\n\\\\n1500\\\\n\\\\n1500\\\\n\\\\n1000\\\\n\\\\n1000\\\\n\\\\n\\\\n500\\\\n\\\\n500\\\\n\\\\n\\\\n0\\\\n\\\\n0\\\\n\\\\n1K 2K 4K 8K 16K 32K 64K 128K\\\\n\\\\n1 2 4 8 16\\\\n\\\\nContext Window\\\\n\\\\nBatch size\\\\n\\\\n(a) Throughput at different batch sizes (single A100\\\\nGPU, 8K context length). Jamba allows processing\\\\nlarge batches, with a throughput 3x greater than Mix-\\\\ntral.\\\\n\\\\n(b) Throughput at different context lengths (single\\\\nbatch, 4 A100 GPUs). With a context of 128K to-\\\\nkens, Jamba obtains 3x the throughput of Mixtral, while\\\\nLlama-2-70B does not fit with this long context.\\\\n\\\\nFigure 3: Comparison of throughput (tokens/second) with Jamba and recent open models.\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"4 ### Training Infrastructure and Dataset\", \"text\": \"4 ### Training Infrastructure and Dataset\\\\n\\\\nThe model was trained on NVIDIA H100 GPUs. We used an in-house proprietary framework\\\\nallowing efficient large-scale training including FSDP, tensor parallelism, sequence parallelism, and\\\\nexpert parallelism.\\\\n\\\\nJamba is trained on an in-house dataset that contains text data from the Web, books, and code, with\\\\nthe last update in March 2024. Our data processing pipeline includes quality filters and deduplication.\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"5 ### Evaluation\", \"text\": \"5 ### Evaluation\\\\n\\\\nIn general we approach benchmarks cautiously, as they correlate only partially with what matters\\\\nin real applications, and furthermore invite gaming the system in order to boast vanity numbers.\\\\nNevertheless, we present several indicative results.\\\\n\\\\n**5.1** **Academic Benchmarks**\\\\n\\\\nWe report results with a wide range of standard academic benchmarks:\\\\n\\\\n**Common sense reasoning:** HellaSwag (10-shot) [ 47 ], WinoGrande (5-shot) [ 37 ], ARC-E (0-shot)\\\\nand ARC-Challenge (25-shot) [ 9 ], and PIQA (zero-shot) [ 3 ].\\\\n\\\\n**Reading Comprehension:** BoolQ (10-shots) [ 8 ] and QuAC (zero-shot) [ 5 ].\\\\n**Others:** GSM8K (3-shot CoT) [ 10 ], HumanEval (pass@1) [ 4 ], Natural Questions closed-book (NQ;\\\\n5-shot) [ 26 ], and TruthfulQA (zero-shot) [ 27 ].\\\\n\\\\n**Aggregate benchmarks:** MMLU (5-shot) [ 20 ] and BBH (3-shot) [ 43 ].\\\\n\\\\n3 Referring to end-to-end throughput (encoding+decoding). The results should be taken relatively rather than\\\\nabsolutely, as they are without possible optimizations.\\\\n\\\\n6\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n**Reasoning**\\\\n\\\\n**HellaSwag** **WinoGrande** **ARC-E** **ARC-C** **PIQA** **NQ** **TruthfulQA**\\\\n\\\\n**Llama-2 13B** 80.7 72.8 77.3 59.4 80.5 37.7 37.4\\\\n**Llama-2 70B** 85.3 80.2 80.2 **67.3** 82.8 **46.9** 44.9\\\\n**Gemma** 81.2 72.3 **81.5** 53.2 81.2 32.6 44.8\\\\n**Mixtral** 86.7 81.2 77.6 66 83 44.8 **46.8**\\\\n\\\\n**Jamba** **87.1** **82.5** 73.5 64.4 **83.2** 45.9 46.4\\\\n\\\\n**Comprehension** **Aggregate**\\\\n\\\\n**BoolQ** **QuAC** **GSM8K** **HumanEval** **MMLU** **BBH**\\\\n\\\\n**Llama-2 13B** 81.7 **42.7** 34.7 18.3 54.8 39.4\\\\n**Llama-2 70B** 85 42.4 55.3 29.9 69.8 51.2\\\\n**Gemma** 87.2 39.2 54.5 32.3 64.3 **55.1**\\\\n**Mixtral** **88.4** 40.9 **60.4** **34.8** **70.6** 50.3\\\\n\\\\n**Jamba** 88.2 40.9 59.9 29.3 67.4 45.4\\\\n\\\\nTable 2: Comparison of Jamba with other publicly available models. Jamba obtains similar or better\\\\nperformance with much better throughput.\\\\n\\\\nTable 2 compares Jamba to several publicly available models on common academic benchmarks for\\\\nevaluating language models. We compare with Llama-2 13B [ 45 ], which has about the same number\\\\nof active paramters as our model, Llama-2 70B, which is larger than our model, Gemma [ 44 ], which\\\\nhas 7B parameters, and Mixtral [ 23 ], which has about the same number of active and total parameters\\\\nas our model.\\\\n\\\\nNoticebly, Jamba performs comparably to the leading publicly available models of similar or larger\\\\nsize, including Llama-2 70B and Mixtral. At the same time, our model has a smaller number of total\\\\navailable parameters than Llama-2 (52B compared to 70B). Moreover, as a sparse model, Jamba\\\\nhas only 12B active parameters, similar to Mixtral\\\\u2019s 12.9B active parameters. However, as a fully-\\\\nattentional model, Mixtral has a large memory footprint with long sequences, requiring 32GB for the\\\\nKV cache with 256K tokens. In contrast, thanks to its hybrid Attention-Mamba architecture, Jamba\\\\u2019s\\\\nKV cache takes only 4GB even at such a long context (Section 2 ). Importantly, our Jamba achieves\\\\nsuch a strong performance while having much better throughput than Llama-2 70B and Mixtral, up\\\\nto 3x improvement (Section 3.2 ).\\\\n\\\\nIn summary, Jamba demostrates the ability of hybrid architectures to reach the performance of\\\\nstate-of-the-art Transformer based models of the same size class, while having the benefits of an\\\\nSSM.\\\\n\\\\n**5.2** **Long-Context Evaluations**\\\\n\\\\nWe have successfully trained Jamba models with context lengths of up to 1M tokens. The released\\\\nmodel handles context lengths of up to 256K tokens. In this section, we evaluate it on synthetic and\\\\nnaturalistic benchmarks that test is long-context capabilities.\\\\n\\\\n**5.2.1** **Needle-in-a-haystack**\\\\n\\\\nAs Figure 4 shows, Jamba has excellent performance in the needle-in-a-haystack evaluation, which\\\\nrequires retrieving a simple statement planted in a long context window [ 24 ]. This result is noteworthy\\\\nespecially given that our implementation of Jamba uses only 4 attention layers.\\\\n\\\\n**5.2.2** **Naturalistic long-context evaluation**\\\\n\\\\nWe evaluate Jamba\\\\u2019s ability to handle long contexts using question-answering benchmarks, consisting\\\\nof long inputs. To this end, we repurpose five of the longest-context datasets from L-Eval [ 2 ], by\\\\nstructuring them in a few-shot format (we use 3-shots in all experiments here). Specifically, we\\\\nevaluated the models on the following datasets: NarrativeQA (QA on narratives; [ 25 ]), LongFQA\\\\n(finance; [ 2 ]), Natural Questions (NQ; Wikipedia; [ 26 ]), CUAD (law; [ 21 ]), and SFiction (science\\\\n\\\\n7\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\nRetrieval Across Context Lengths (\\\\\"Needle-in-a-haystack\\\\\")\\\\n\\\\n1.0\\\\n\\\\n0.0\\\\n\\\\n10.0\\\\n\\\\n0.8\\\\n\\\\n20.0\\\\n\\\\n30.0\\\\n\\\\n0.6\\\\n\\\\n40.0\\\\n\\\\n50.0\\\\n\\\\n\\\\n\\\\n60.0\\\\n\\\\n0.4\\\\n\\\\n70.0\\\\n\\\\n80.0\\\\n\\\\n0.2\\\\n\\\\n90.0\\\\n\\\\n100.0\\\\n\\\\n0.0\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nTokens\\\\n\\\\nFigure 4: A needle-in-a-haystack evaluation showing Jamba\\\\u2019s ability to recall statements placed in\\\\nthe middle of contexts of up to 256K tokens length.\\\\n\\\\nfiction). The average input length in these datasets ranges from 6K to 62K tokens. These context\\\\nlengths are further highly expanded by the few-shot format.\\\\n\\\\nTable 3 summarizes the evaluation results, in terms of F1. Jamba outperforms Mixtral on most of the\\\\ndatasets as well as on average. In addition, as these long-context tasks require substantial computation,\\\\nhere Jamba\\\\u2019s efficiency shines, with much better throughput with long contexts (Section 3.2 ).\\\\n\\\\n\\\\n|LongFQA CUAD NarrativeQA NQ SFiction|Avg|\\\\n|---|---|\\\\n\\\\n\\\\n|Mixtral 0.42 0.46 0.29 0.58 0.42 Jamba 0.44 0.44 0.30 0.60 0.40|0.43 0.44|\\\\n|---|---|\\\\n\\\\n\\\\nTable 3: Results (F1) on long-context QA benchmarks, with a 3-shot format.\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"6 ### Ablations and Insights\", \"text\": \"6 ### Ablations and Insights\\\\n\\\\nThis section discusses ablation experiments we ran for different design choices in our implementation\\\\nof the Jamba architecture. First we show the benefit of combining attention and Mamba layers,\\\\nat which ratio they should be combined, and how to interleave them. We investigate cases where\\\\npure Mamba fails, suggesting that it struggles to develop in-context learning capabilities, while\\\\nthe Attention\\\\u2013Mamba hybrid exhibits in-context learning similar to vanilla Transformers. Then we\\\\nshow the benefit of adding MoE on top of a hybrid Attention\\\\u2013Mamba model. Finally, we share two\\\\nadditional learnings that we found useful: explicit positional information is not needed in Jamba, and\\\\nMamba layers necessitate special normalization to stabilize training at large scale. 4\\\\n\\\\nFor these ablations, we report the following measures, which exhibit informative performance even at\\\\nsmall data or model scale.\\\\n\\\\n- Academic benchmarks: HellaSwag (10-shot) [ 47 ], WinoGrande (5-shot) [ 37 ], Natural\\\\nQuestions closed-book (NQ; 5-shot) [ 26 ].\\\\n\\\\n- HuggingFace OpenLLM leaderboard (OLLM) [ 11 ]: a summary statistic of several datasets.\\\\nWe report results with our reproduction.\\\\n\\\\n- Perplexity evaluations: we report log-prob (per byte) on texts from three domains: C4,\\\\nBooks, and code.\\\\n\\\\n4 In all the ablation experiments, \\\\u201cpure Mamba\\\\u201d refers to models with Mamba layers interleaved with MLP\\\\nlayers.\\\\n\\\\n8\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n**6.1** **Benefits of combining Attention and Mamba**\\\\n\\\\nWe first investigate the ratio of Attention to Mamba layers ( _a_ : _m_ ), with 1.3B parameters models\\\\ntrained for 250B tokens. As Table 4 shows, the hybrid Jamba model outperforms the pure attention\\\\nor Mamba models. The ratio of attention-to-Mamba layers may be 1:3 or 1:7 with virtually no\\\\nperformance difference. Figure 5 shows the training loss of these models, where Jamba exhibits\\\\nimproved loss during training. Given that a 1:7 ratio is more compute-efficient and shows similar\\\\nperformance, we opt for it in our larger-scale experiments.\\\\n\\\\nlog-prob\\\\n\\\\nHella\\\\nSwag\\\\nWino\\\\nGrande\\\\n\\\\nOLLM NQ C4 Books Code\\\\n\\\\nAttention 36.4 62.4 59.6 14.5 -0.543 -0.659 -0.331\\\\nMamba 36.1 62.6 59.4 14.5 -0.543 -0.661 -0.334\\\\nJamba ( _a_ : _m_ = 1 : 3 , no MoE) 37.2 65.1 61.7 16.5 -0.533 -0.649 -0.321\\\\nJamba ( _a_ : _m_ = 1 : 7 , no MoE) 37.2 65.1 61.7 16.0 -0.533 -0.650 -0.321\\\\n\\\\nTable 4: Results on academic benchmarks and log-probability evaluations showing an improved\\\\nperformance of Attention-Mamba (no MoE) compared to vanilla Attention and Mamba models.\\\\nThere is no substantial difference between 1:3 and 1:7 ratios of Attention-to-Mamba layers. Models\\\\nare 1.3B parameters, trained for 250B tokens.\\\\n\\\\nFigure 5: Training loss curves for pure Attention, pure Mamba, and Attention-Mamba hybrids (no\\\\nMoE), with ratios _a_ : _m_ of 1:3 and 1:4. All models are 1.3B parameters. The two hybrids achieve\\\\nbetter loss throughout this training run, without any noticeable difference between the different\\\\nAttention/Mamba ratios.\\\\n\\\\nNext, we compare performance of vanilla Transformer, vanilla Mamba, and Attention-Mamba hybrid\\\\nmodels, at 7B model size, after training on 50B tokens. As Table 5 shows, the pure Mamba layer is\\\\nquite competitive, but lags slightly behind pure Attention. The hybrid Attention-Mamba (without\\\\nMoE) outperforms the pure models while obtaining better throughput than the vanilla Transformer\\\\n(Section 3.2 ).\\\\n\\\\nlog-prob\\\\n\\\\nHella\\\\nSwag\\\\nWino\\\\nGrande\\\\n\\\\nOLLM NQ C4 Books Code\\\\n\\\\nAttention 36.1 60.4 59.7 13.7 -0.555 -0.666 -0.347\\\\nMamba 35.3 60.2 55.8 14.0 -0.554 -0.667 -0.355\\\\nJamba ( _a_ : _m_ = 1 : 7 , no MoE) 36.6 62.5 58.8 15.4 -0.547 -0.658 -0.340\\\\n\\\\nTable 5: Results on academic benchmarks and log-prob evaluations, comparing pure Attention, pure\\\\nMamba, and Attention-Mamba hybrid (no MoE). Models are 7B parameters, trained for 50B tokens.\\\\n\\\\n9\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\nFigure 6 shows the training loss of the three architectures. While the pure Transformer and Mamba\\\\nmodels have a similar convergence, the hybrid Jamba (no MoE) has a lower loss throughout this run.\\\\n\\\\nFigure 6: Training loss curves for pure Attention, pure Mamba, and an Attention-Mamba hybrid (no\\\\nMoE). All models are 7B parameters. the hybrid achives better loss throughout this training run.\\\\n\\\\n**6.2** **Why does the Combination Work?**\\\\n\\\\nThe pure Mamba model showed fairly good results in most tasks early on, including in general\\\\nperplexity evaluations. However, it performed substantially worse than the pure Attention model\\\\nin three common benchmark tasks: IMDB [ 28 ], QuAC [ 5 ], and NarrativeQA [ 25 ]. In contrast, the\\\\nhybrid Attention-Mamba performed similarly to the Attention model on these datasets. Table 6 shows\\\\nthe results for 1.3B models after 250B tokens.\\\\n\\\\nIMDB QuAC NarrativeQA\\\\n\\\\nAttention 84.1 27.9 45.8\\\\nMamba 48.8 20.2 27.7\\\\nAttention-Mamba 90.9 26.6 43.7\\\\n\\\\nTable 6: Mamba performs poorly on certain datasets, while the Attention-Mamba hybrid performs on\\\\npar with the Attention model.\\\\n\\\\nLooking into these results further, we found out that the pure Mamba model often does not follow the\\\\ncorrect format. For instance, in the IMDB dataset, answer choices are \\\\u201cPositive\\\\u201d or \\\\u201cNegative\\\\u201d. While\\\\nthe Attention model adheres to this format, the pure Mamba model often produces other answers,\\\\nsuch as \\\\u201cVery Good\\\\u201d, \\\\u201cVery Positive\\\\u201d, \\\\u201cFunny\\\\u201d, \\\\u201cBad\\\\u201d, \\\\u201cPoor\\\\u201d, and \\\\u201c3/10\\\\u201d. While these may be\\\\nconsidered correct answers, the difficulty of Mamba to adhere to the format suggests a potential\\\\nproblem. Indeed, to perform successful in-context learning, it is important for models to capture the\\\\ninput-output format [ 30 ]. The hybrid Attention-Mamba model follows the format successfully, just\\\\nlike the pure Attention model.\\\\n\\\\nWe hypothesize that this phenomenon points to a limitation of SSMs \\\\u2013 a potential difficulty in\\\\nin-context learning (ICL). Indeed, the ability to perform ICL has been linked to the emergence of so-\\\\ncalled induction heads in Transformer language models during training, which perform approximate\\\\ncopying operations that are supportive of ICL [ 31 ]. We conjecture that the lack of an attention\\\\nmechanism in the pure Mamba model makes it difficult for it to learn in-context. While Mamba\\\\nmay learn to copy and perform simple ICL when explicitly trained to do so ([ 16 , 32 ], it is not\\\\nclear if ICL is an emergent capability in SSM as is typical of Transformer models. In contrast, the\\\\nhybrid Attention\\\\u2013Mamba model does perform successful ICL, even when only 1 out of 8 layers is an\\\\nAttention one.\\\\n\\\\nAs anecdotal evidence of an emergent induction mechanism, we visualize in Figure 7 the attention\\\\nof an example head from a 1.3B Attention-Mamba hybrid model (no MoE), on an IMDB example\\\\nwhere the pure Mamba failed and the hybrid succeeded. Clearly, the attention from the last token\\\\n\\\\n10\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n(\\\\u201c:\\\\u201d) is focused on the labels from the few-shot examples. We have found 12 such heads in our hybrid\\\\nmodel, in all three attention layers (which correspond to layers 4, 12, 20 in the model).\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"6 ### Ablations and Insights\", \"Header_4\": \"[\\\\u2026]\", \"text\": \"[\\\\u2026]\\\\n\\\\nFigure 7: Example induction head (H3, first attention layer) from a hybrid Attention-Mamba model.\\\\nHighlighted words reflect strong attention from the last token, \\\\u201c:\\\\u201d, just before the model is about to\\\\npredict the label. We see that the attention is focused on label tokens from the few-shot examples.\\\\n\\\\nFuture work can further investigate the emergence of ICL in hybrid models at large scale. Our\\\\nreleased checkpoints would hopefully facilitate such investigations. Finally, very recent work has\\\\nattempted to extract attention-like scores from state-space models like Mamba [ 1 ], which opens\\\\nanother direction to search for induction capabilities in state-space models.\\\\n\\\\n**6.3** **The Effect of Mixture-of-Experts (MoE)**\\\\n\\\\nRecent work has shown that MoE improves Transformer language models while keeping compute\\\\nmanageable [ 23 ]. 5  However, it is not clear if MoE integrates well with state-space models at a\\\\nlarge scale, and specifically with our hybrid Attention\\\\u2013Mamba architecture. Indeed, Table 7 shows\\\\nthat MoE improves the performance of the hybrid Attention-Mamba architecture at large scale (7B\\\\nparameters trained on 50B tokens). The MoE variant has _n_ = 16 total experts, _K_ = 2 experts used at\\\\neach token, and MoE is applied every _e_ = 2 layers, as described in Section 3.1 .\\\\n\\\\nlog-prob\\\\n\\\\nHella\\\\nSwag\\\\nWino\\\\nGrande\\\\n\\\\nOLLM NQ C4 Books Code\\\\n\\\\nJamba (no MoE) 36.6 62.5 58.8 15.4 -0.547 -0.658 -0.340\\\\nJamba+MoE 38.1 66.0 61.2 18.9 -0.534 -0.645 -0.326\\\\n\\\\nTable 7: Mixture-of-experts improves the Attention-Mamba hybrid.\\\\n\\\\n5 There is also initial evidence that MoE helps Mamba layers, albeit at small model and data scale [ 34 ].\\\\n\\\\n11\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n**6.4** **Stabilizing Mamba at large scale**\\\\n\\\\nWhen training Jamba models of up to 1.3B parameters, we observed stable training without special\\\\nproblems. However, when scaling to the largest model released here (7B-based, which has 12B/52B\\\\nactive/total parameters), we encountered large loss spikes. Investigating this revealed that inner parts\\\\nof the Mamba layers suffer from large activation values, leading to the spikes. We therefore added\\\\nRMSNorm [ 48 ] to internal activations. As Figure 8 shows, this stabilized training and prevented\\\\nadditional loss spikes.\\\\n\\\\nFigure 8: Adding RMSNorm to Mamba layers prevents loss spikes.\\\\n\\\\n**6.5** **Jamba does not Require Explicit Positional Information**\\\\n\\\\nTable 8 shows results of the Jamba architecture (with MoE) with no positional information and when\\\\napplying RoPE [ 42 ] in the attention layers (1.3B parameter models, 250B tokens). The results are\\\\nsimilar, suggesting that explicit positional information may not be required for the hybrid architecture.\\\\nPresumably, the Mamba layers, which are placed before attention layers, provide implicit position\\\\ninformation. 6\\\\n\\\\nlog-prob\\\\n\\\\nHella\\\\nSwag\\\\nWino\\\\nGrande\\\\nNarrative\\\\nQA\\\\n\\\\nOLLM ARC-C NQ BoolQ C4 Books Code\\\\n\\\\nJamba 39.6 71.5 64.2 40.7 50.5 22.2 68.9 -0.516 -0.623 -0.299\\\\nJamba+RoPE 40.1 71.8 65.5 40.4 46.2 22.2 67.9 -0.516 -0.623 -0.299\\\\n\\\\nTable 8: Comparison of Jamba with and without explicit positional information.\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"7 ### Conclusion\", \"text\": \"7 ### Conclusion\\\\n\\\\nWe presented Jamba, a novel architecture which combines Attention and Mamba layers, with MoE\\\\nmodules, and an open implementation of it, reaching state-of-the-art performance and supporting\\\\nlong contexts. We showed how Jamba provides flexibility for balancing performance and memory\\\\nrequirements, while maintaining a high throughput. We experimented with several design choices\\\\nsuch as the ratio of Attention-to-Mamba layers and discussed some discoveries made during the\\\\ndevelopment process, which will inform future work on hybrid attention\\\\u2013state-space models. To\\\\nfacilitate such research, we plan to release model checkpoints from smaller-scale training runs.\\\\nThe largest model we provide with this release has 12B active and 52B total available parameters,\\\\nsupporting context lengths of up to 256K tokens and fitting in a single 80GB GPU even when\\\\nprocessing 140K-token texts.\\\\n\\\\n6 Some prior evidence suggested that Transformer decoder models do not need positional encodings [ 19 ].\\\\nHowever, all existing large scale models do use some sort of explicit position information.\\\\n\\\\n12\\\\n\\\\n\\\\n\\\\n-----\"}',\n",
       " '{\"Header_2\": \"A Hybrid Transformer-Mamba Language Model\", \"Header_3\": \"References\", \"text\": \"References\\\\n\\\\n[1] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. _arXiv_\\\\n_preprint arXiv:2403.01590_ , 2024.\\\\n\\\\n[2] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng\\\\nQiu. L-Eval: Instituting standardized evaluation for long context language models. _arXiv_\\\\n_preprint arXiv:2307.11088_ , 2023.\\\\n\\\\n[3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical\\\\ncommonsense in natural language. In _Proceedings of the AAAI Conference on Artificial_\\\\n_Intelligence_ , volume 34, pages 7432\\\\u20137439, 2020.\\\\n\\\\n[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\\\nlanguage models trained on code. _arXiv preprint arXiv:2107.03374_ , 2021.\\\\n\\\\n[5] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang,\\\\nand Luke Zettlemoyer. QuAC: Question answering in context. In _Proceedings of the 2018_\\\\n_Conference on Empirical Methods in Natural Language Processing_ , pages 2174\\\\u20132184, 2018.\\\\n\\\\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\\\nScaling language modeling with pathways. _Journal of Machine Learning Research_ , 24(240):1\\\\u2013\\\\n113, 2023.\\\\n\\\\n[7] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\\\\nHoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified\\\\nscaling laws for routed language models. In _International conference on machine learning_ ,\\\\npages 4057\\\\u20134086. PMLR, 2022.\\\\n\\\\n[8] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\\\\nKristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In\\\\n_Proceedings of the 2019 Conference of the North American Chapter of the Association for_\\\\n_Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_ ,\\\\npages 2924\\\\u20132936, 2019.\\\\n\\\\n[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\\\nand Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning\\\\nchallenge. _arXiv preprint arXiv:1803.05457_ , 2018.\\\\n\\\\n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\\\\nsolve math word problems. _arXiv preprint arXiv:2110.14168_ , 2021.\\\\n\\\\n[11] Hugging Face. Open LLM leaderboard. ```https://huggingface.co/spaces/\\\\n```\\\\n`HuggingFaceH4/open_llm_leaderboard` , 2024.\\\\n\\\\n[12] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar,\\\\nChunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. Multi-head state\\\\nspace model for speech recognition. In _Proceedings of INTERSPEECH 2023_ , pages 241\\\\u2013245,\\\\n2023.\\\\n\\\\n[13] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\\\\nparameter models with simple and efficient sparsity. _Journal of Machine Learning Research_ ,\\\\n23(120):1\\\\u201339, 2022.\\\\n\\\\n[14] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re.\\\\nHungry hungry hippos: Towards language modeling with state space models. In _The Eleventh_\\\\n_International Conference on Learning Representations_ , 2022.\\\\n\\\\n[15] Philip Gage. A new algorithm for data compression. _The C Users Journal_ , 12(2):23\\\\u201338, 1994.\\\\n\\\\n13\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n[16] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.\\\\n_arXiv preprint arXiv:2312.00752_ , 2023.\\\\n\\\\n[17] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured\\\\nstate spaces. In _International Conference on Learning Representations_ , 2021.\\\\n\\\\n[18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\\\\u00e9.\\\\nCombining recurrent, convolutional, and continuous-time models with linear state space layers.\\\\n_Advances in neural information processing systems_ , 34:572\\\\u2013585, 2021.\\\\n\\\\n[19] Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models\\\\nwithout positional encodings still learn positional information. In _Findings of the Association_\\\\n_for Computational Linguistics: EMNLP 2022_ , pages 1382\\\\u20131390, 2022.\\\\n\\\\n[20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\\\nJacob Steinhardt. Measuring massive multitask language understanding. In _International_\\\\n_Conference on Learning Representations_ , 2020.\\\\n\\\\n[21] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. CUAD: An expert-annotated NLP\\\\ndataset for legal contract review. In _Thirty-fifth Conference on Neural Information Processing_\\\\n_Systems Datasets and Benchmarks Track (Round 1)_ , 2021.\\\\n\\\\n[22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\\\\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\\\nSaulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_ , 2023.\\\\n\\\\n[23] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\\\\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,\\\\net al. Mixtral of experts. _arXiv preprint arXiv:2401.04088_ , 2024.\\\\n\\\\n[24] Greg Kamradt. Needle in a haystack - pressure testing llms. ```https://github.com/\\\\n```\\\\n`gkamradt/LLMTest_NeedleInAHaystack/` , 2023.\\\\n\\\\n[25] Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Ga-\\\\nbor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge.\\\\n_Transactions of the Association for Computational Linguistics_ , 6:317\\\\u2013328, 2018.\\\\n\\\\n[26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\\\nbenchmark for question answering research. _Transactions of the Association for Computational_\\\\n_Linguistics_ , 7:452\\\\u2013466, 2019.\\\\n\\\\n[27] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic\\\\nhuman falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Compu-_\\\\n_tational Linguistics (Volume 1: Long Papers)_ , pages 3214\\\\u20133252, Dublin, Ireland, May 2022.\\\\nAssociation for Computational Linguistics.\\\\n\\\\n[28] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher\\\\nPotts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th annual meeting_\\\\n_of the association for computational linguistics: Human language technologies_ , pages 142\\\\u2013150,\\\\n2011.\\\\n\\\\n[29] Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gall\\\\u00e9,\\\\nArun Raja, Chenglei Si, Wilson Y Lee, Beno\\\\u00eet Sagot, et al. Between words and charac-\\\\nters: A brief history of open-vocabulary modeling and tokenization in NLP. _arXiv preprint_\\\\n_arXiv:2112.10508_ , 2021.\\\\n\\\\n[30] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\\\\nLuke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning\\\\nwork? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language_\\\\n_Processing_ , pages 11048\\\\u201311064, 2022.\\\\n\\\\n14\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n[31] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\\\\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning\\\\nand induction heads. _arXiv preprint arXiv:2209.11895_ , 2022.\\\\n\\\\n[32] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,\\\\nKangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative\\\\nstudy on in-context learning tasks. _arXiv preprint arXiv:2402.04248_ , 2024.\\\\n\\\\n[33] Jonathan Pilault, Mahan Fathi, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, and Ross\\\\nGoroshin. Block-state transformers. In _Thirty-seventh Conference on Neural Information_\\\\n_Processing Systems_ , 2023.\\\\n\\\\n[34] Maciej Pi\\\\u00f3ro, Kamil Ciebiera, Krystian Kr\\\\u00f3l, Jan Ludziejewski, and Sebastian Jaszczur.\\\\nMoE-Mamba: Efficient selective state space models with mixture of experts. _arXiv preprint_\\\\n_arXiv:2401.04081_ , 2024.\\\\n\\\\n[35] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua\\\\nBengio, Stefano Ermon, and Christopher R\\\\u00e9. Hyena hierarchy: Towards larger convolutional\\\\nlanguage models. In _International Conference on Machine Learning_ , pages 28043\\\\u201328078.\\\\nPMLR, 2023.\\\\n\\\\n[36] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and\\\\nArmin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing\\\\nModels. `https://github.com/togethercomputer/stripedhyena` , 2023.\\\\n\\\\n[37] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\\\\nadversarial winograd schema challenge at scale. In _Proceedings of the AAAI Conference on_\\\\n_Artificial Intelligence_ , volume 34, pages 8732\\\\u20138740, 2020.\\\\n\\\\n[38] George Saon, Ankit Gupta, and Xiaodong Cui. Diagonal state space augmented transformers\\\\nfor speech recognition. In _ICASSP 2023-2023 IEEE International Conference on Acoustics,_\\\\n_Speech and Signal Processing (ICASSP)_ , pages 1\\\\u20135. IEEE, 2023.\\\\n\\\\n[39] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\\\\nwords with subword units. In _Proceedings of the 54th Annual Meeting of the Association for_\\\\n_Computational Linguistics (Volume 1: Long Papers)_ , pages 1715\\\\u20131725, 2016.\\\\n\\\\n[40] Noam Shazeer. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_ , 2020.\\\\n\\\\n[41] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. In _International Conference on Learning Representations_ , 2017.\\\\n\\\\n[42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\\\\nEnhanced transformer with rotary position embedding. _Neurocomputing_ , 568:127063, 2024.\\\\n\\\\n[43] Mirac Suzgun, Nathan Scales, Nathanael Sch\\\\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won\\\\nChung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging BIG-\\\\nBench tasks and whether chain-of-thought can solve them. In _Findings of the Association for_\\\\n_Computational Linguistics: ACL 2023_ , pages 13003\\\\u201313051, 2023.\\\\n\\\\n[44] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya\\\\nPathak, Laurent Sifre, Morgane Rivi\\\\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open\\\\nmodels based on gemini research and technology. _arXiv preprint arXiv:2403.08295_ , 2024.\\\\n\\\\n[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\\\\nfoundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ , 2023.\\\\n\\\\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\\\n\\\\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information_\\\\n_processing systems_ , 30, 2017.\\\\n\\\\n15\\\\n\\\\n\\\\n\\\\n-----\\\\n\\\\n\\\\n[47] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can\\\\na machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the_\\\\n_Association for Computational Linguistics_ , pages 4791\\\\u20134800, 2019.\\\\n\\\\n[48] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _Advances in Neural_\\\\n_Information Processing Systems_ , 32, 2019.\\\\n\\\\n[49] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer,\\\\nand William Fedus. ST-MoE: Designing stable and transferable sparse expert models. _arXiv_\\\\n_preprint arXiv:2202.08906_ , 2022.\\\\n\\\\n[50] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng\\\\nGao. Efficient long sequence modeling via state space augmented transformer. _arXiv preprint_\\\\n_arXiv:2212.08136_ , 2022.\\\\n\\\\n16\\\\n\\\\n\\\\n\\\\n-----\"}']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[json.dumps(node.metadata) for node in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the Embeddings to a Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upstash_vector import Index, Vector\n",
    "\n",
    "index = Index.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the nodes to vector objects\n",
    "vectors = []\n",
    "\n",
    "for node in nodes:\n",
    "    vector = Vector(\n",
    "        id=node.node_id,\n",
    "        vector=node.embedding,\n",
    "        metadata=node.metadata\n",
    "    )\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsert the vectors to the index\n",
    "index.upsert(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Jamba language model?\"\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings for the query\n",
    "query_vector = oai_client.embeddings.create(\n",
    "    input=query,\n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "query_result = index.query(\n",
    "    vector=query_vector,\n",
    "    include_metadata=True,\n",
    "    include_vectors=False,\n",
    "    top_k=TOP_K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8411506\n",
      "ID: 1\n",
      "Metadata: {'Header_2': 'A Hybrid Transformer-Mamba Language Model', 'Header_3': 'Abstract', 'text': 'Abstract\\n\\nWe present Jamba, a new base large language model based on a novel hybrid\\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of both\\nmodel families. MoE is added in some of these layers to increase model capacity\\nwhile keeping active parameter usage manageable. This flexible architecture allows\\nresource- and objective-specific configurations. In the particular configuration we\\nhave implemented, we end up with a powerful model that fits in a single 80GB\\nGPU. Built at large scale, Jamba provides high throughput and small memory\\nfootprint compared to vanilla Transformers, and at the same time state-of-the-art\\nperformance on standard language model benchmarks and long-context evaluations.\\nRemarkably, the model presents strong results for up to 256K tokens context length.\\nWe study various architectural decisions, such as how to combine Transformer and\\nMamba layers, and how to mix experts, and show that some of them are crucial\\nin large scale modeling. We also describe several interesting properties of these\\narchitectures which the training and evaluation of Jamba have revealed, and plan to\\nrelease checkpoints from various ablation runs, to encourage further exploration\\nof this novel architecture. We make the weights of our implementation of Jamba\\npublicly available under a permissive license.\\n\\n\\n**Model:** `https://huggingface.co/ai21labs/Jamba-v0.1`'}\n",
      "\n",
      "Score: 0.8318833\n",
      "ID: 5\n",
      "Metadata: {'Header_2': 'A Hybrid Transformer-Mamba Language Model', 'Header_3': '4 ### Training Infrastructure and Dataset', 'text': '4 ### Training Infrastructure and Dataset\\n\\nThe model was trained on NVIDIA H100 GPUs. We used an in-house proprietary framework\\nallowing efficient large-scale training including FSDP, tensor parallelism, sequence parallelism, and\\nexpert parallelism.\\n\\nJamba is trained on an in-house dataset that contains text data from the Web, books, and code, with\\nthe last update in March 2024. Our data processing pipeline includes quality filters and deduplication.'}\n",
      "\n",
      "Score: 0.8093958\n",
      "ID: 9\n",
      "Metadata: {'Header_2': 'A Hybrid Transformer-Mamba Language Model', 'Header_3': '7 ### Conclusion', 'text': '7 ### Conclusion\\n\\nWe presented Jamba, a novel architecture which combines Attention and Mamba layers, with MoE\\nmodules, and an open implementation of it, reaching state-of-the-art performance and supporting\\nlong contexts. We showed how Jamba provides flexibility for balancing performance and memory\\nrequirements, while maintaining a high throughput. We experimented with several design choices\\nsuch as the ratio of Attention-to-Mamba layers and discussed some discoveries made during the\\ndevelopment process, which will inform future work on hybrid attention–state-space models. To\\nfacilitate such research, we plan to release model checkpoints from smaller-scale training runs.\\nThe largest model we provide with this release has 12B active and 52B total available parameters,\\nsupporting context lengths of up to 256K tokens and fitting in a single 80GB GPU even when\\nprocessing 140K-token texts.\\n\\n6 Some prior evidence suggested that Transformer decoder models do not need positional encodings [ 19 ].\\nHowever, all existing large scale models do use some sort of explicit position information.\\n\\n12\\n\\n\\n\\n-----'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in query_result:\n",
    "    print(\"Score:\", result.score)\n",
    "    print(\"ID:\", result.id)\n",
    "    print(\"Metadata:\", result.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_result_str(metadata):\n",
    "    '''\n",
    "    Build a string from the metadata dictionary of a query result for adding to the context of the LLM.\n",
    "    '''\n",
    "    text = metadata['text']\n",
    "    _meta = {\n",
    "        k: v for k, v in metadata.items() if k != 'text'\n",
    "    }\n",
    "    \n",
    "    meta_str = \"\\n\".join([f\"{k}: {v}\" for k, v in _meta.items()])\n",
    "    return f\"{meta_str}\\n\\n{text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header_2: A Hybrid Transformer-Mamba Language Model\n",
      "Header_3: 4 ### Training Infrastructure and Dataset\n",
      "\n",
      "4 ### Training Infrastructure and Dataset\n",
      "\n",
      "The model was trained on NVIDIA H100 GPUs. We used an in-house proprietary framework\n",
      "allowing efficient large-scale training including FSDP, tensor parallelism, sequence parallelism, and\n",
      "expert parallelism.\n",
      "\n",
      "Jamba is trained on an in-house dataset that contains text data from the Web, books, and code, with\n",
      "the last update in March 2024. Our data processing pipeline includes quality filters and deduplication.\n"
     ]
    }
   ],
   "source": [
    "print(build_result_str(query_result[1].metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = \"\"\"Retrieved context to answer the query is as follows:\n",
    "{context_str}\n",
    "\"\"\"\n",
    "\n",
    "def build_context_prompt(retrieval_results):\n",
    "    context_str = \"\\n\\n---------------------\\n\\n\".join([build_result_str(r.metadata) for r in retrieval_results])\n",
    "    return context_prompt.format(\n",
    "        context_str=context_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context to answer the query is as follows:\n",
      "Header_2: A Hybrid Transformer-Mamba Language Model\n",
      "Header_3: Abstract\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present Jamba, a new base large language model based on a novel hybrid\n",
      "Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\n",
      "interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both\n",
      "model families. MoE is added in some of these layers to increase model capacity\n",
      "while keeping active parameter usage manageable. This flexible architecture allows\n",
      "resource- and objective-specific configurations. In the particular configuration we\n",
      "have implemented, we end up with a powerful model that fits in a single 80GB\n",
      "GPU. Built at large scale, Jamba provides high throughput and small memory\n",
      "footprint compared to vanilla Transformers, and at the same time state-of-the-art\n",
      "performance on standard language model benchmarks and long-context evaluations.\n",
      "Remarkably, the model presents strong results for up to 256K tokens context length.\n",
      "We study various architectural decisions, such as how to combine Transformer and\n",
      "Mamba layers, and how to mix experts, and show that some of them are crucial\n",
      "in large scale modeling. We also describe several interesting properties of these\n",
      "architectures which the training and evaluation of Jamba have revealed, and plan to\n",
      "release checkpoints from various ablation runs, to encourage further exploration\n",
      "of this novel architecture. We make the weights of our implementation of Jamba\n",
      "publicly available under a permissive license.\n",
      "\n",
      "\n",
      "**Model:** `https://huggingface.co/ai21labs/Jamba-v0.1`\n",
      "\n",
      "---------------------\n",
      "\n",
      "Header_2: A Hybrid Transformer-Mamba Language Model\n",
      "Header_3: 4 ### Training Infrastructure and Dataset\n",
      "\n",
      "4 ### Training Infrastructure and Dataset\n",
      "\n",
      "The model was trained on NVIDIA H100 GPUs. We used an in-house proprietary framework\n",
      "allowing efficient large-scale training including FSDP, tensor parallelism, sequence parallelism, and\n",
      "expert parallelism.\n",
      "\n",
      "Jamba is trained on an in-house dataset that contains text data from the Web, books, and code, with\n",
      "the last update in March 2024. Our data processing pipeline includes quality filters and deduplication.\n",
      "\n",
      "---------------------\n",
      "\n",
      "Header_2: A Hybrid Transformer-Mamba Language Model\n",
      "Header_3: 7 ### Conclusion\n",
      "\n",
      "7 ### Conclusion\n",
      "\n",
      "We presented Jamba, a novel architecture which combines Attention and Mamba layers, with MoE\n",
      "modules, and an open implementation of it, reaching state-of-the-art performance and supporting\n",
      "long contexts. We showed how Jamba provides flexibility for balancing performance and memory\n",
      "requirements, while maintaining a high throughput. We experimented with several design choices\n",
      "such as the ratio of Attention-to-Mamba layers and discussed some discoveries made during the\n",
      "development process, which will inform future work on hybrid attention–state-space models. To\n",
      "facilitate such research, we plan to release model checkpoints from smaller-scale training runs.\n",
      "The largest model we provide with this release has 12B active and 52B total available parameters,\n",
      "supporting context lengths of up to 256K tokens and fitting in a single 80GB GPU even when\n",
      "processing 140K-token texts.\n",
      "\n",
      "6 Some prior evidence suggested that Transformer decoder models do not need positional encodings [ 19 ].\n",
      "However, all existing large scale models do use some sort of explicit position information.\n",
      "\n",
      "12\n",
      "\n",
      "\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(build_context_prompt(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_retrieval(search_query: str) -> str:\n",
    "    '''\n",
    "    This function let's you semantically retrieve relevant context chunks from a given document based on a query.\n",
    "\n",
    "    Arguments:\n",
    "        query (str): The query to search for in the document. Based on the original user query, write a good search query\n",
    "                     which is more logically sound to retrieve the relevant information from the document.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved context chunks from the document based on the search query formatted as a string.\n",
    "    '''\n",
    "    # Get the embeddings for the search query\n",
    "    query_vector = oai_client.embeddings.create(\n",
    "        input=search_query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # Execute the query\n",
    "    query_result = index.query(\n",
    "        vector=query_vector,\n",
    "        include_metadata=True,\n",
    "        include_vectors=False,\n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "    return build_context_prompt(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context to answer the query is as follows:\n",
      "Header_2: A Hybrid Transformer-Mamba Language Model\n",
      "Header_3: 4 ### Training Infrastructure and Dataset\n",
      "\n",
      "4 ### Training Infrastructure and Dataset\n",
      "\n",
      "The model was trained on NVIDIA H100 GPUs. We used an in-house proprietary framework\n",
      "allowing efficient large-scale training including FSDP, tensor parallelism, sequence parallelism, and\n",
      "expert parallelism.\n",
      "\n",
      "Jamba is trained on an in-house dataset that contains text data from the Web, books, and code, with\n",
      "the last update in March 2024. Our data processing pipeline includes quality filters and deduplication.\n",
      "\n",
      "---------------------\n",
      "\n",
      "Header_2: A Hybrid Transformer-Mamba Language Model\n",
      "Header_3: 7 ### Conclusion\n",
      "\n",
      "7 ### Conclusion\n",
      "\n",
      "We presented Jamba, a novel architecture which combines Attention and Mamba layers, with MoE\n",
      "modules, and an open implementation of it, reaching state-of-the-art performance and supporting\n",
      "long contexts. We showed how Jamba provides flexibility for balancing performance and memory\n",
      "requirements, while maintaining a high throughput. We experimented with several design choices\n",
      "such as the ratio of Attention-to-Mamba layers and discussed some discoveries made during the\n",
      "development process, which will inform future work on hybrid attention–state-space models. To\n",
      "facilitate such research, we plan to release model checkpoints from smaller-scale training runs.\n",
      "The largest model we provide with this release has 12B active and 52B total available parameters,\n",
      "supporting context lengths of up to 256K tokens and fitting in a single 80GB GPU even when\n",
      "processing 140K-token texts.\n",
      "\n",
      "6 Some prior evidence suggested that Transformer decoder models do not need positional encodings [ 19 ].\n",
      "However, all existing large scale models do use some sort of explicit position information.\n",
      "\n",
      "12\n",
      "\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "---------------------\n",
      "\n",
      "Header_2: A Hybrid Transformer-Mamba Language Model\n",
      "Header_3: Abstract\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present Jamba, a new base large language model based on a novel hybrid\n",
      "Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\n",
      "interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both\n",
      "model families. MoE is added in some of these layers to increase model capacity\n",
      "while keeping active parameter usage manageable. This flexible architecture allows\n",
      "resource- and objective-specific configurations. In the particular configuration we\n",
      "have implemented, we end up with a powerful model that fits in a single 80GB\n",
      "GPU. Built at large scale, Jamba provides high throughput and small memory\n",
      "footprint compared to vanilla Transformers, and at the same time state-of-the-art\n",
      "performance on standard language model benchmarks and long-context evaluations.\n",
      "Remarkably, the model presents strong results for up to 256K tokens context length.\n",
      "We study various architectural decisions, such as how to combine Transformer and\n",
      "Mamba layers, and how to mix experts, and show that some of them are crucial\n",
      "in large scale modeling. We also describe several interesting properties of these\n",
      "architectures which the training and evaluation of Jamba have revealed, and plan to\n",
      "release checkpoints from various ablation runs, to encourage further exploration\n",
      "of this novel architecture. We make the weights of our implementation of Jamba\n",
      "publicly available under a permissive license.\n",
      "\n",
      "\n",
      "**Model:** `https://huggingface.co/ai21labs/Jamba-v0.1`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context_retrieval(\"on what hardware was jamba trained on?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a Q&A bot. You are here to answer questions based on the context retrieved\n",
    "from a vector index of the chunks of a document. You are prohibited from using prior knowledge and you\n",
    "can only use the context given. If you need more information, please ask the user. If you cannot answer \n",
    "the question from the context, you can tell the user that you cannot answer the question. You can also \n",
    "ask for more information from the user.\"\"\"\n",
    "\n",
    "system_message = {\n",
    "    'role': 'system',\n",
    "    'content': system_prompt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mapping of the tool name to the function that should be called\n",
    "available_functions = {\n",
    "    \"context_retrieval\": context_retrieval,\n",
    "}\n",
    "\n",
    "# Here we have only one function, but you can have multiple as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a JSON schema for the tools that the LLM can use\n",
    "# Here we define a schema for the context_retrieval function\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"context_retrieval\",\n",
    "            \"description\": \"This function let's you semantically retrieve relevant context chunks from a given document based on a query. Based on the original user query, write a good search query which is more logically sound to retrieve the relevant information from the document. You might even have to break down the user query into multiple search queries and call this function multiple times separately. This function finally returns the retrieved context chunks from the document based on the search query formatted as a string.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The sub-query to search for in the document.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_turn(user_message, messages, tools, model='gpt-3.5-turbo', temperature=0.2, max_tokens=512, verbose=True, **kwargs):\n",
    "\n",
    "    # Add user message to messages list\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': user_message\n",
    "    })\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n<< User Message >>\")\n",
    "        print(user_message)\n",
    "    \n",
    "    # Send the conversation and available tools/functions to the model\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "        **kwargs\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    # Add the response to the messages list\n",
    "    messages.append(response_message)\n",
    "\n",
    "    # Check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "\n",
    "        # Call each of the functions\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\n<< Calling Function `{function_name}` with Args: {function_args} >>\")\n",
    "\n",
    "            # Call the function\n",
    "            function_response = function_to_call(**function_args)\n",
    "\n",
    "            # if verbose:\n",
    "            #     print(\"<< Function Response >>\")\n",
    "            #     print(function_response)\n",
    "\n",
    "            # Add the function response to the messages list\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Get a new response from the model based on the function response\n",
    "        second_response = oai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        second_response_message = second_response.choices[0].message\n",
    "        messages.append(second_response_message)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n<< Response >>\")\n",
    "            print(second_response_message.content)\n",
    "\n",
    "        return second_response_message, messages\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n<< Response >>\")\n",
    "        print(response_message.content)\n",
    "\n",
    "    return response_message, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    system_message\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<< User Message >>\n",
      "hardware of jamba and what is the conclusion?\n",
      "\n",
      "<< Calling Function `context_retrieval` with Args: {'search_query': 'hardware of jamba'} >>\n",
      "\n",
      "<< Calling Function `context_retrieval` with Args: {'search_query': 'conclusion of jamba'} >>\n",
      "\n",
      "<< Response >>\n",
      "The hardware used for training Jamba includes NVIDIA H100 GPUs, and a proprietary in-house framework was utilized to enable large-scale training with features like FSDP, tensor parallelism, sequence parallelism, and expert parallelism.\n",
      "\n",
      "The conclusion drawn about Jamba is that it is a novel architecture that combines Attention and Mamba layers with MoE modules, reaching state-of-the-art performance and supporting long contexts. Jamba provides flexibility for balancing performance and memory requirements while maintaining a high throughput. The design choices, such as the ratio of Attention-to-Mamba layers, were experimented with, leading to insights that will guide future work on hybrid attention-state-space models.\n",
      "\n",
      "If you have any more questions or need further details, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response, messages = conversation_turn(\n",
    "    \"hardware of jamba and what is the conclusion?\",\n",
    "    messages,\n",
    "    tools_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
