{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdf2md\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"pdfs/Mamba.pdf\"\n",
    "markdown_text = pdf2md.to_markdown(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Mamba: Linear-Time Sequence Modeling with Selective State Spaces  \\n### Albert Gu *\\n1 and Tri Dao * 2  \\n1 Machine Learning Department, Carnegie Mellon University\\n2 Department of Computer Science, Princeton University\\n`agu@cs.cmu.edu` , `tri@tridao.me`  \\nAbstract  \\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally\\nbased on the Transformer architecture and its core attention module. Many subquadratic-time architectures\\nsuch as linear attention, gated convolution and recurrent models, and structured state space models (SSMs)\\nhave been developed to address Transformers’ computational ineﬃciency on long sequences, but they have not\\nperformed as well as attention on important modalities such as language. We identify that a key weakness of\\nsuch models is their inability to perform content-based reasoning, and make several improvements. First, simply\\nletting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing\\nthe model to selectively propagate or forget information along the sequence length dimension depending on\\nthe current token. Second, even though this change prevents the use of eﬃcient convolutions, we design a\\nhardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliﬁed\\nend-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast\\ninference (5 × higher throughput than Transformers) and linear scaling in sequence length, and its performance\\nimproves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves\\nstate-of-the-art performance across several modalities such as language, audio, and genomics. On language\\nmodeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice\\nits size, both in pretraining and downstream evaluation.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 3': 'Albert Gu *'}),\n",
       " Document(page_content='## 1 ## Introduction  \\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have\\nemerged as an eﬀective paradigm in modern machine learning. The backbone of these FMs are often sequence\\nmodels, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images,\\nspeech, audio, time series, and genomics (Brown et al. 2020 ; Dosovitskiy et al. 2020 ; Ismail Fawaz et al. 2019 ;\\nOord et al. 2016 ; Poli et al. 2023 ; Sutskever, Vinyals, and Quoc V Le 2014 ). While this concept is agnostic to\\na particular choice of model architecture, modern FMs are predominantly based on a single type of sequence\\nmodel: the Transformer (Vaswani et al. 2017 ) and its core attention layer (Bahdanau, Cho, and Bengio 2015 )\\nThe eﬃcacy of self-attention is attributed to its ability to route information densely within a context window,\\nallowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model\\nanything outside of a ﬁnite window, and quadratic scaling with respect to the window length. An enormous body\\nof research has appeared on more eﬃcient variants of attention to overcome these drawbacks (Tay, Dehghani,\\nBahri, et al. 2022 ), but often at the expense of the very properties that makes it eﬀective. As of yet, none of these\\nvariants have been shown to be empirically eﬀective at scale across domains.  \\nRecently, structured state space sequence models (SSMs) (Gu, Goel, and Ré 2022 ; Gu, Johnson, Goel, et al. 2021 )\\nhave emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a\\ncombination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration\\nfrom classical state space models (Kalman 1960 ). This class of models can be computed very eﬃciently as either a\\nrecurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled  \\n* Equal contribution.  \\n1  \\n-----  \\nmechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020 ) in certain data modalities, and have\\ndominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021 ). Many ﬂavors of\\nSSMs (Gu, Goel, and Ré 2022 ; Gu, Gupta, et al. 2022 ; Gupta, Gu, and Berant 2022 ; Y. Li et al. 2023 ; Ma et al.\\n2023 ; Orvieto et al. 2023 ; Smith, Warrington, and Linderman 2023 ) have been successful in domains involving\\ncontinuous signal data such as audio and vision (Goel et al. 2022 ; Nguyen, Goel, et al. 2022 ; Saon, Gupta, and Cui\\n2023 ). However, they have been less eﬀective at modeling discrete and information-dense data such as text.  \\nWe propose a new class of selective state space models, that improves on prior work on several axes to achieve the\\nmodeling power of Transformers while scaling linearly in sequence length.  \\n**Selection Mechanism.** First, we identify a key limitation of prior models: the ability to eﬃciently select\\ndata in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on\\nimportant synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by\\nparameterizing the SSM parameters based on the input. This allows the model to ﬁlter out irrelevant information\\nand remember relevant information indeﬁnitely.  \\n**Hardware-aware Algorithm.** This simple change poses a technical challenge for the computation of the model;\\nin fact, all prior SSMs models must be time- and input-invariant in order to be computationally eﬃcient. We\\novercome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of\\nconvolution, but does not materialize the expanded state in order to avoid IO access between diﬀerent levels of the\\nGPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling\\nlinearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware\\n(up to 3 × faster on A100 GPUs).  \\n**Architecture.** We simplify prior deep sequence model architectures by combining the design of prior SSM\\narchitectures (Dao, Fu, Saab, et al. 2023 ) with the MLP block of Transformers into a single block, leading to a\\nsimple and homogenous architecture design (Mamba) incorporating selective state spaces.  \\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that\\nmake them suitable as the backbone of general foundation models operating on sequences. (i) High quality:\\nselectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and\\ninference: computation and memory scales linearly in sequence length during training, and unrolling the model\\nautoregressively during inference requires only constant time per step since it does not require a cache of previous\\nelements. (iii) Long context: the quality and eﬃciency together yield performance improvements on real data up\\nto sequence length 1M.  \\nWe empirically validate Mamba’s potential as a general sequence FM backbone, in both pretraining quality and\\ndomain-speciﬁc task performance, on several types of modalities and settings:  \\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being\\nkey to large language models, Mamba not only solves them easily but can extrapolate solutions indeﬁnitely long\\n( &gt; 1M tokens).  \\n- Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-\\ners on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g.\\nreducing FID on a challenging speech generation dataset by more than half). In both settings, its performance\\nimproves with longer context up to million-length sequences.  \\n- Language Modeling. Mamba is the ﬁrst linear-time sequence model that truly achieves Transformer-quality\\nperformance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,\\nwe show that Mamba exceeds the performance of a large range of baselines, including very strong modern\\nTransformer training recipes based on LLaMa (Touvron et al. 2023 ). Our Mamba language model has 5 ×\\ngeneration throughput compared to Transformers of similar size, and Mamba-3B’s quality matches that of\\nTransformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and\\neven exceeding Pythia-7B).  \\nModel code and pre-trained checkpoints are open-sourced at `https://github.com/state-spaces/mamba` .  \\n2  \\n-----', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '1 ## Introduction'}),\n",
       " Document(page_content='### Selective State Space Model  \\n**_with Hardware-aware State Expansion_**', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '1 ## Introduction', 'Header 3': 'Selective State Space Model'}),\n",
       " Document(page_content='### 𝐴', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '1 ## Introduction', 'Header 3': '𝐴'}),\n",
       " Document(page_content='### ℎ !\"# ### ℎ !  \\n𝑥 !', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '1 ## Introduction', 'Header 3': 'ℎ !\"# ### ℎ !'}),\n",
       " Document(page_content='### 𝑦 !', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '1 ## Introduction', 'Header 3': '𝑦 !'}),\n",
       " Document(page_content='### 𝐶 ! ### 𝐵 !  \\n**GPU**', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '1 ## Introduction', 'Header 3': '𝐶 ! ### 𝐵 !'}),\n",
       " Document(page_content='### ∆ !  \\n**Discretize**  \\n**SRAM**  \\n**Project**  \\n**GPU HBM**  \\n**Selection Mechanism**  \\nFigure 1: ( **Overview** .) Structured SSMs independently map each channel (e.g. 퐷= 5 ) of an input 푥 to output 푦 through a higher\\ndimensional latent state ℎ (e.g. 푁= 4 ). Prior SSMs avoid materializing this large efective state ( 퐷푁 , times batch size 퐵 and sequence\\nlength 퐿 ) through clever alternate computation paths requiring time-invariance: the (∆, **_A_** , **_B_** , **_C_** ) parameters are constant across\\ntime. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to\\nonly materialize the expanded states in more efcient levels of the GPU memory hierarchy.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '1 ## Introduction', 'Header 3': '∆ !'}),\n",
       " Document(page_content='## 2 ## State Space Models  \\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are\\nbroadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous\\nsystem ( 1 ) that maps a 1-dimensional function or sequence 푥(푡) ∈ℝ↦푦(푡) ∈ℝ through an implicit latent state\\nℎ(푡) ∈ℝ 푁 .  \\nConcretely, S4 models are deﬁned with four parameters (∆, **_A_** , **_B_** , **_C_** ) , which deﬁne a sequence-to-sequence trans-\\nformation in two stages.  \\nℎ ′ (푡) = **_A_** ℎ(푡) + **_B_** 푥(푡)\\n(1a)  \\nℎ 푡 = **_A_** ℎ 푡−1 + **_B_** 푥 푡\\n(2a)  \\n푲= ( **_C_** 푩, **_C_** 푨푩, … , **_C_** 푨\\n푘  \\n푩, … )\\n(3a)  \\n푦(푡) = **_C_** ℎ(푡)\\n(1b)  \\n푦 푡 = **_C_** ℎ 푡\\n(2b)  \\n푦= 푥∗푲\\n(3b)  \\n**Discretization.**\\nThe ﬁrst stage transforms the “continuous parameters” (∆, **_A_** , **_B_** ) to “discrete parameters” ( **_A_** , **_B_** )\\nthrough ﬁxed formulas **_A_** = 푓 퐴 (∆, **_A_** ) and **_B_** = 푓 퐵 (∆, **_A_** , **_B_** ) , where the pair (푓 퐴 , 푓 퐵 ) is called a discretization rule.\\nVarious rules can be used such as the zero-order hold (ZOH) deﬁned in equation ( 4 ) .  \\n**_A_** = exp(∆ **_A_** ) **_B_** = (∆ **_A_** ) −1 (exp(∆ **_A_** ) − **_I_** ) ⋅∆ **_B_**\\n(4)  \\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties\\nsuch as resolution invariance (Nguyen, Goel, et al. 2022 ) and automatically ensuring that the model is properly\\nnormalized (Gu, Johnson, Timalsina, et al. 2023 ; Orvieto et al. 2023 ). It also has connections to gating mechanisms\\nof RNNs (Gu, Gulcehre, et al. 2020 ; Tallec and Ollivier 2018 ) which we will revisit in Section 3.5 . However, from\\na mechanical point of view discretization can simply be viewed as the ﬁrst step of the computation graph in the\\nforward pass of an SSM. Alternate ﬂavors of SSMs can bypass the discretization step and parameterize ( **_A_** , **_B_** )\\ndirectly instead (Zhang et al. 2023 ), which may be easier to reason about.  \\n**Computation.**\\nAfter the parameters have been transformed from (∆, **_A_** , **_B_** , **_C_** ) ↦( **_A_** , **_B_** , **_C_** ) , the model can be\\ncomputed in two ways, either as a linear recurrence ( 2 ) or a global convolution ( 3 ) .  \\n3  \\n-----  \\nCommonly, the model uses the convolutional mode ( 3 ) for eﬃcient parallelizable training (where the whole input\\nsequence is seen ahead of time), and switched into recurrent mode ( 2 ) for eﬃcient autoregressive inference (where\\nthe inputs are seen one timestep at a time).  \\n**Linear Time Invariance (LTI).** An important property of equations ( 1 ) to ( 3 ) is that the model’s dynamics are\\nconstant through time. In other words (∆, **_A_** , **_B_** , **_C_** ) , and consequently ( **_A_** , **_B_** ) as well, are ﬁxed for all time-steps.\\nThis property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions.\\nInformally, we think of LTI SSMs as being equivalent to any linear recurrence ( 2a ) or convolution ( 3b ) , and use\\nLTI as an umbrella term for these classes of models.  \\nThus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eﬃciency\\nconstraints, discussed in Section 3.3 . However, a core insight of this work is that LTI models have fundamental\\nlimitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint\\nwhile overcoming the eﬃciency bottlenecks.  \\n**Structure and Dimensions.** Finally, we note that structured SSMs are so named because computing them\\neﬃciently also requires imposing structure on the **_A_** matrix. The most popular form of structure is diagonal\\n(Gu, Gupta, et al. 2022 ; Gupta, Gu, and Berant 2022 ; Smith, Warrington, and Linderman 2023 ), which we also\\nuse.  \\nIn this case, the **_A_** ∈ℝ 푁×푁 , **_B_** ∈ℝ 푁×1 , **_C_** ∈ℝ 1×푁 matrices can all be represented by 푁 numbers. To operate over\\nan input sequence 푥 of batch size 퐵 and length 퐿 with 퐷 channels, the SSM is applied independently to each\\nchannel. Note that in this case, the total hidden state has dimension 퐷푁 per input, and computing it over the\\nsequence length requires 푂(퐵퐿퐷푁) time and memory; this is the root of the fundamental eﬃciency bottleneck\\naddressed in Section 3.3 .  \\n**General State Space Models.** We note that the term state space model has a very broad meaning which simply\\nrepresents the notion of any recurrent process with a latent state. It has been used to refer to many disparate\\nconcepts in diﬀerent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner\\net al. 2020 )), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003 )),\\nKalman ﬁlters (controls (Kalman 1960 )), hidden Markov models (HMM) and linear dynamical systems (LDS)\\n(machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).  \\nThroughout this entire paper we use the term “SSM” to refer exclusively to the class of structured SSMs or S4\\nmodels (Gu, Goel, and Ré 2022 ; Gu, Gupta, et al. 2022 ; Gupta, Gu, and Berant 2022 ; Hasani et al. 2023 ; Ma et al.\\n2023 ; Smith, Warrington, and Linderman 2023 ) and use these terms interchangeably. For convenience we may also\\ninclude derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution\\nviewpoints (Y. Li et al. 2023 ; Orvieto et al. 2023 ; Poli et al. 2023 ), and clarify nuances when necessary.  \\n**SSM Architectures.** SSMs are standalone sequence transformations that can be incorporated into end-to-end\\nneural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as\\nCNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of\\nwhich will also serve as our primary baselines.  \\n- Linear attention (Katharopoulos et al. 2020 ) is an approximation of self-attention involving a recurrence which\\ncan be viewed as a degenerate linear SSM.  \\n- H3 (Dao, Fu, Saab, et al. 2023 ) generalized this recurrence to use S4; it can be viewed as an architecture with\\nan SSM sandwiched by two gated connections (Figure 3 ). H3 also inserts a standard local convolution, which\\nthey frame as a shift-SSM, before the main SSM layer.  \\n- Hyena (Poli et al. 2023 ) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized\\nglobal convolution (Romero et al. 2021 ).  \\n- RetNet (Y. Sun et al. 2023 ) adds an additional gate to the architecture and uses a simpler SSM, allowing\\nan alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of\\nconvolutions.  \\n4  \\n-----  \\n- RWKV (B. Peng et al. 2023 ) is a recent RNN designed for language modeling based on another linear attention\\napproximation (attention-free Transformer (S. Zhai et al. 2021 )). Its main “WKV” mechanism involves LTI\\nrecurrences and can be viewed as the ratio of two SSMs.  \\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B ). We\\nhighlight in particular S5 (Smith, Warrington, and Linderman 2023 ), QRNN (Bradbury et al. 2016 ), and SRU (Lei\\net al. 2017 ), which we view as the most closely related methods to our core selective SSM.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '2 ## State Space Models'}),\n",
       " Document(page_content='## 3 ## Selective State Space Models  \\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1 ), then explain how to\\nincorporate this mechanism into state space models (Section 3.2 ). The resulting time-varying SSMs cannot\\nuse convolutions, presenting a technical challenge of how to compute them eﬃciently. We overcome this with\\na hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3 ). We then\\ndescribe a simple SSM architecture without attention or even MLP blocks (Section 3.4 ). Finally, we discuss some\\nadditional properties of selection mechanisms (Section 3.5 ).', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '3 ## Selective State Space Models'}),\n",
       " Document(page_content='### 3.1 ### Motivation: Selection as a Means of Compression  \\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact,\\nwe can view the tradeoﬀs of popular sequence models from this point of view. For example, attention is both\\neﬀective and ineﬃcient because it explicitly does not compress context at all. This can be seen from the fact that\\nautoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the\\nslow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are\\neﬃcient because they have a ﬁnite state, implying constant-time inference and linear-time training. However, their\\neﬀectiveness is limited by how well this state has compressed the context.  \\nTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2 ).  \\n- The Selective Copying task modiﬁes the popular Copying task (Arjovsky, Shah, and Bengio 2016 ) by varying\\nthe position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant\\ntokens (colored) and ﬁlter out the irrelevant ones (white).  \\n- The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning\\nabilities of LLMs (Olsson et al. 2022 ). It requires context-aware reasoning to know when to produce the correct\\noutput in the appropriate context (black).  \\nThese tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the\\n( **_A_** , **_B_** ) transitions in ( 2 ) ) cannot let them select the correct information from their context, or aﬀect the hidden\\nstate passed along the sequence an in input-dependent way. From the convolutional view, it is known that global\\nconvolutions can solve the vanilla Copying task (Romero et al. 2021 ) because it only requires time-awareness,\\nbut that they have diﬃculty with the Selective Copying task because of lack of content-awareness (Figure 2 ).\\nMore concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution\\nkernels.  \\nIn summary, the eﬃciency vs. eﬀectiveness tradeoﬀof sequence models is characterized by how well they compress\\ntheir state: eﬃcient models must have a small state, while eﬀective models must have a state that contains all\\nnecessary information from the context. In turn, we propose that a fundamental principle for building sequence\\nmodels is selectivity: or the context-aware ability to focus on or ﬁlter out inputs into a sequential state. In\\nparticular, a selection mechanism controls how information propagates or interacts along the sequence dimension\\n(see Section 3.5 for more discussion).', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '3 ## Selective State Space Models', 'Header 3': '3.1 ### Motivation: Selection as a Means of Compression'}),\n",
       " Document(page_content='### 3.2 ### Improving SSMs with Selection  \\nOne method of incorporating a selection mechanism into models is by letting their parameters that aﬀect\\ninteractions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be\\ninput-dependent.  \\n5  \\n-----  \\n**Copying** **Selective Copying**  \\nOutput  \\nOutput  \\nInput  \\nInput  \\n**Induction Heads** Solution  \\nPerfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs  \\n?  \\nFigure 2: ( _Left_ ) The standard version of the Copying task involves constant spacing between input and output elements and is\\neasily solved by time-invariant models such as linear recurrences and global convolutions. ( _Right Top_ ) The Selective Copying task\\nhas random spacing in between inputs and requires time-varying models that can _selectively_ remember or ignore inputs depending\\non their content. ( _Right Bottom_ ) The Induction Heads task is an example of associative recall that requires retrieving an answer\\nbased on context, a key ability for LLMs.  \\n**Algorithm 1** SSM (S4)  \\n**Algorithm 2** SSM + Selection (S6)  \\n**Input:**\\n푥∶(홱, 홻, 홳)  \\n**Input:**\\n푥∶(홱, 홻, 홳)  \\n**Output:**\\n푦∶(홱, 홻, 홳)  \\n**Output:**\\n푦∶(홱, 홻, 홳)  \\n1: **_A_** ∶(홳, 홽) ←햯햺헋햺헆햾헍햾헋\\n⊳ Represents structured 푁× 푁 matrix  \\n1: **_A_** ∶(홳, 홽) ←햯햺헋햺헆햾헍햾헋\\n⊳ Represents structured 푁× 푁 matrix  \\n2: **_B_** ∶(홳, 홽) ←햯햺헋햺헆햾헍햾헋\\n3: **_C_** ∶(홳, 홽) ←햯햺헋햺헆햾헍햾헋\\n4: ∆∶(홳) ←휏 ∆ (햯햺헋햺헆햾헍햾헋)  \\n2: **_B_** ∶ (홱, 홻, 홽) ← 푠 퐵 (푥)\\n3: **_C_** ∶ (홱, 홻, 홽) ← 푠 퐶 (푥)\\n4: ∆∶ (홱, 홻, 홳) ←휏 ∆ (햯햺헋햺헆햾헍햾헋 +푠 ∆ (푥) )  \\n5: **_A_** , **_B_** ∶(홳, 홽) ←햽헂헌햼헋햾헍헂헓햾(∆, **_A_** , **_B_** )\\n6: 푦←햲햲햬( **_A_** , **_B_** , **_C_** )(푥)\\n⊳ Time-invariant: recurrence or convolution  \\n5: **_A_** , **_B_** ∶ (홱, 홻, 홳, 홽) ←햽헂헌햼헋햾헍헂헓햾(∆, **_A_** , **_B_** )\\n6: 푦←햲햲햬( **_A_** , **_B_** , **_C_** )(푥)\\n⊳ Time-varying : recurrence ( _scan_ ) only  \\n7: **return** 푦  \\n7: **return** 푦  \\nAlgorithms 1 and 2 illustrates the main selection mechanism that we use. The main diﬀerence is simply making\\nseveral parameters ∆, **_B_** , **_C_** functions of the input, along with the associated changes to tensor shapes throughout.\\nIn particular, we highlight that these parameters now have a length dimension 퐿 , meaning that the model has\\nchanged from time-invariant to time-varying. (Note that shape annotations were described in Section 2 ). This\\nloses the equivalence to convolutions ( 3 ) with implications for its eﬃciency, discussed next.  \\nWe speciﬁcally choose 푠 퐵 (푥) = 햫헂헇햾햺헋 푁 (푥) , 푠 퐶 (푥) = 햫헂헇햾햺헋 푁 (푥) , 푠 ∆ (푥) = 햡헋허햺햽햼햺헌헍 퐷 (햫헂헇햾햺헋 1 (푥)) , and 휏 ∆ = 헌허햿헍헉헅헎헌 ,\\nwhere 햫헂헇햾햺헋 푑 is a parameterized projection to dimension 푑 . The choice of 푠 ∆ and 휏 ∆ is due to a connection to\\nRNN gating mechanisms explained in Section 3.5 .', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '3 ## Selective State Space Models', 'Header 3': '3.2 ### Improving SSMs with Selection'}),\n",
       " Document(page_content='### 3.3 ### Efcient Implementation of Selective SSMs  \\nHardware-friendly architectures such as convolutions (Krizhevsky, Sutskever, and Hinton 2012 ) and Transform-\\ners (Vaswani et al. 2017 ) enjoy widespread application. Here we aim to make selective SSMs eﬃcient on modern\\nhardware (GPU) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate\\nspecial cases of selection, such as letting ∆ vary over time in recurrent SSMs (Gu, Dao, et al. 2020 ). However, as\\npreviously mentioned a core limitation in the usage of SSMs is their computational eﬃciency, which was why S4\\nand all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions.  \\n**3.3.1** **Motivation of Prior Models**  \\nWe ﬁrst revisit this motivation and overview our approach to overcome limitations of prior methods.  \\n- At a high level, recurrent models such as SSMs always balance a tradeoﬀbetween expressivity and speed: as\\ndiscussed in Section 3.1 , models with larger hidden state dimension should be more eﬀective but slower. Thus  \\n6  \\n-----  \\nwe want to maximize hidden state dimension without paying speed and memory costs.  \\n- Note that the recurrent mode is more ﬂexible than the convolution mode, since the latter ( 3 ) is derived from\\nexpanding the former ( 2 ) (Gu, Goel, and Ré 2022 ; Gu, Johnson, Goel, et al. 2021 ). However, this would require\\ncomputing and materializing the latent state ℎ with shape (홱, 홻, 홳, 홽) , much larger (by a factor of 푁 , the SSM\\nstate dimension) than the input 푥 and output 푦 of shape (홱, 홻, 홳) . Thus the more eﬃcient convolution mode was\\nintroduced which could bypass the state computation and materializes a convolution kernel ( 3a ) of only (홱, 홻, 홳) .  \\n**3.3.2** **Overview of Selective Scan: Hardware-Aware State Expansion**  \\n- Prior LTI SSMs leverage the dual recurrent-convolutional forms to increase the eﬀective state dimension by a\\nfactor of 푁 ( ≈10 −100 ), much larger than traditional RNNs, without eﬃciency penalties.  \\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore\\nneed to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion,\\nparallel scan, and recomputation. We make two main observations:  \\n- The naive recurrent computation uses 푂(퐵퐿퐷푁) FLOPs while the convolutional computation uses 푂(퐵퐿퐷log(퐿))\\nFLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension\\n푁 , the recurrent mode can actually use fewer FLOPs.  \\n- The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter,\\njust like the convolutional mode, we can attempt to not actually materialize the full state ℎ .  \\nThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state ℎ only in more\\neﬃcient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded\\nby memory bandwidth (Dao, Fu, Ermon, et al. 2022 ; Ivanov et al. 2021 ; Williams, Waterman, and Patterson\\n2009 ). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to\\na signiﬁcant speedup compared to a standard implementation.  \\nConcretely, instead of preparing the scan input ( **_A_** , **_B_** ) of size (홱, 홻, 홳, 홽) in GPU HBM (high-bandwidth memory),\\nwe load the SSM parameters (∆, **_A_** , **_B_** , **_C_** ) directly from slow HBM to fast SRAM, perform the discretization and\\nrecurrence in SRAM, and then write the ﬁnal outputs of size (홱, 홻, 홳) back to HBM.  \\nTo avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a\\nwork-eﬃcient parallel scan algorithm (Blelloch 1990 ; Martin and Cundy 2018 ; Smith, Warrington, and Linderman\\n2023 ).  \\nFinally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully\\napply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not\\nstored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the\\nfused selective scan layer has the same memory requirements as an optimized transformer implementation with\\nFlashAttention.  \\nDetails of the fused kernel and recomputation are in Appendix D . The full Selective SSM layer and algorithm is\\nillustrated in Figure 1 .', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '3 ## Selective State Space Models', 'Header 3': '3.3 ### Efcient Implementation of Selective SSMs'}),\n",
       " Document(page_content='### 3.4 ### A Simplifed SSM Architecture  \\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be ﬂexibly incorporated\\ninto neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2 ), which\\nare generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron)\\nblock. We simplify this architecture by combining these two components into one, which is stacked homogenously\\n(Figure 3 ). This is inspired by the gated attention unit (GAU) (Hua et al. 2022 ), which did something similar for\\nattention.  \\nThis architecture involves expanding the model dimension 퐷 by a controllable expansion factor 퐸 . For each\\nblock, most of the parameters ( 3퐸퐷 2 ) are in the linear projections ( 2퐸퐷 2  for input projections, 퐸퐷 2  for output\\nprojection) while the inner SSM contributes less.\\nThe number of SSM parameters (projections for ∆, **_B_** , **_C_** , and  \\n7  \\n-----  \\n|SS Co|X M X nv|\\n|---|---|  \\n|Col1|X !|\\n|---|---|\\n|||\\n|||  \\n|Col1|X M ! ! nv|\\n|---|---|\\n|SS Co||\\n|||  \\n|SS|M|\\n|---|---|  \\n|SS|M|\\n|---|---|  \\n! X ! !  \\nSequence\\ntransformation  \\n|Co|nv|\\n|---|---|  \\n|Co|nv|\\n|---|---|  \\nNonlinearity\\n(activation or\\nmultiplication)\\nX  \\n⨂  \\n**H3** **Gated MLP** **Mamba**  \\nFigure 3: ( **Architecture** .) Our simplifed block design combines the H3 block, which is the basis of most SSM architectures, with\\nthe ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block\\nhomogenously. Compared to the H3 block, Mamba replaces the frst multiplicative gate with an activation function. Compared to\\nthe MLP block, Mamba adds an SSM to the main branch. For 휎 we use the SiLU / Swish activation (Hendrycks and Gimpel 2016 ;\\nRamachandran, Zoph, and Quoc V Le 2017 ).  \\nthe matrix **_A_** ) are much smaller in comparison. We repeat this block, interleaved with standard normalization\\nand residual connections, to form the Mamba architecture. We always ﬁx to 퐸= 2 in our experiments and use two\\nstacks of the block to match the 12퐷 2  parameters of a Transformer’s interleaved MHA (multi-head attention) and\\nMLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016 ; Ramachandran, Zoph,\\nand Quoc V Le 2017 ), motivated so that the Gated MLP becomes the popular “SwiGLU” variant (Chowdhery\\net al. 2023 ; Shazeer 2020 ; Touvron et al. 2023 ). Finally, we additionally use an optional normalization layer (we\\nchoose LayerNorm (J. L. Ba, Kiros, and Hinton 2016 )), motivated by RetNet’s usage of a normalization layer in a\\nsimilar location (Y. Sun et al. 2023 ).', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '3 ## Selective State Space Models', 'Header 3': '3.4 ### A Simplifed SSM Architecture'}),\n",
       " Document(page_content='### 3.5 ### Properties of Selection Mechanisms  \\n**3.5.1** **Connection to Gating Mechanisms**  \\nThe selection mechanism is a broader concept that can be applied in diﬀerent ways, such as to more traditional\\nRNNs or CNNs, to diﬀerent parameters (e.g. **_A_** in Algorithm 2 ), or using diﬀerent transformations 푠(푥) .  \\nWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection\\nmechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time\\nsystems is well established (Funahashi and Nakamura 1993 ; Tallec and Ollivier 2018 ). In fact, Theorem 1 is\\nan improvement of Gu, Johnson, Goel, et al. ( 2021 , Lemma 3.1) generalizing to the ZOH discretization and\\ninput-dependent gates (proof in Appendix C ). More broadly, ∆ in SSMs can be seen to play a generalized role\\nof the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the\\nprincipled foundation of heuristic gating mechanisms.  \\n**Theorem 1.** _When_ 푁= 1, **_A_** = −1, **_B_** = 1, 푠 ∆ = 햫헂헇햾햺헋(푥) _, and_ 휏 ∆ = 헌허햿헍헉헅헎헌 _, then the selective SSM recurrence_\\n_(Algorithm_ _2_ _) takes the form_\\n푔 푡 = 휎(햫헂헇햾햺헋(푥 푡 ))  \\nℎ 푡 = (1 −푔 푡 )ℎ 푡−1 + 푔 푡 푥 푡 .\\n(5)  \\nAs mentioned in Section 3.2 , our speciﬁc choices of 푠 ∆ , 휏 ∆ is from this connection. In particular, note that if a\\ngiven input 푥 푡 should be completely ignored (as necessary in the synthetic tasks), all 퐷 channels should ignore it,\\nand so we project the input down to 1 dimension before repeating/broadcasting with ∆ .  \\n8  \\n-----  \\n**3.5.2** **Interpretation of Selection Mechanisms**  \\nWe elaborate on two particular mechanistic eﬀects of selection.  \\n**Variable Spacing.** Selectivity allows ﬁltering out irrelevant noise tokens that may occur between inputs of\\ninterest. This is exempliﬁed by the Selective Copying task, but occurs ubiquitously in common data modalities,\\nparticularly for discrete data – for example the presence of language ﬁllers such as “um”. This property arises\\nbecause the model can mechanistically ﬁlter out any particular input 푥 푡 , for example in the gated RNN case\\n(Theorem 1 ) when 푔 푡 →0 .  \\n**Filtering Context.** It has been empirically observed that many sequence models do not improve with longer\\ncontext (F. Shi et al. 2023 ), despite the principle that more context should lead to strictly better performance. An\\nexplanation is that many sequence models cannot eﬀectively ignore irrelevant context when necessary; an intuitive\\nexample are global convolutions (and general LTI models). On the other hand, selective models can simply reset\\ntheir state at any time to remove extraneous history, and thus their performance in principle improves monotonicly\\nwith context length (e.g. Section 4.3.2 ).  \\n**Boundary Resetting.** In settings where multiple independent sequences are stitched together, Transformers\\ncan keep them separate by instantiating a particular attention mask, while LTI models will bleed information\\nbetween the sequences. Selective SSMs can also reset their state at boundaries (e.g. ∆ 푡 →∞ or Theorem 1 when\\n푔 푡 →1 ). These settings may occur artiﬁcially (e.g. packing documents together to improve hardware utilization)\\nor naturally (e.g. episode boundaries in reinforcement learning (Lu et al. 2023 )).  \\nAdditionally, we elaborate on eﬀects of each selective parameter.  \\n**Interpretation of** ∆ **.** In general, ∆ controls the balance between how much to focus or ignore the current input\\n푥 푡 . It generalizes RNN gates (e.g. 푔 푡 in Theorem 1 ), mechanically, a large ∆ resets the state ℎ and focuses on the\\ncurrent input 푥 , while a small ∆ persists the state and ignores the current input. SSMs ( 1 ) - ( 2 ) can be interpreted as\\na continuous system discretized by a timestep ∆ , and in this context the intuition is that large ∆→∞ represents\\nthe system focusing on the current input for longer (thus “selecting” it and forgetting its current state) while a\\nsmall ∆→0 represents a transient input that is ignored.  \\n**Interpretation of** **_A_** **.** We remark that while the **_A_** parameter could also be selective, it ultimately aﬀects the\\nmodel only through its interaction with ∆ via **_A_** = exp(∆ **_A_** ) (the discretization ( 4 ) ). Thus selectivity in ∆ is\\nenough to ensure selectivity in ( **_A_** , **_B_** ) , and is the main source of improvement. We hypothesize that making **_A_**\\nselective in addition to (or instead of) ∆ would have similar performance, and leave it out for simplicity.  \\n**Interpretation of** **_B_** **and** **_C_** **.** As discussed in Section 3.1 , the most important property of selectivity is ﬁltering\\nout irrelevant information so that a sequence model’s context can be compressed into an eﬃcient state. In an SSM,\\nmodifying **_B_** and **_C_** to be selective allows ﬁner-grained control over whether to let an input 푥 푡 into the state ℎ 푡 or\\nthe state into the output 푦 푡 . These can be interpreted as allowing the model to modulate the recurrent dynamics\\nbased on content (input) and context (hidden states) respectively.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '3 ## Selective State Space Models', 'Header 3': '3.5 ### Properties of Selection Mechanisms'}),\n",
       " Document(page_content='### 3.6 ### Additional Model Details  \\n**Real vs. Complex.**\\nMost prior SSMs use complex numbers in their state ℎ , which is necessary for strong\\nperformance on many tasks (Gu, Goel, and Ré 2022 ). However, it has been empirically observed that completely\\nreal-valued SSMs seem to work ﬁne, and possibly even better, in some settings (Ma et al. 2023 ). We use real\\nvalues as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoﬀis\\nrelated to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous\\nmodalities (e.g. audio, video) but not discrete (e.g. text, DNA).  \\n9  \\n-----  \\n**Initialization.** Most prior SSMs also suggest special initializations, particularly in the complex-valued case,\\nwhich can help in several settings such as low-data regimes. Our default initialization for the complex case is\\nS4D-Lin and for the real case is S4D-Real (Gu, Gupta, et al. 2022 ), which is based on the HIPPO theory (Gu,\\nDao, et al. 2020 ). These deﬁne the 푛 -th element of **_A_** as −1∕2 + 푛푖 and −(푛+ 1) respectively. However, we expect\\nmany initializations to work ﬁne, particularly in the large-data and real-valued SSM regimes; some ablations are\\nconsidered in Section 4.6 .  \\n**Parameterization of** ∆ **.** We deﬁned the selective adjustment to ∆ as 푠 ∆ (푥) = 햡헋허햺햽햼햺헌헍 퐷 (햫헂헇햾햺헋 1 (푥)) , which was\\nmotivated by the mechanics of ∆ (Section 3.5 ). We observe that it can be generalized from dimension 1 to a larger\\ndimension 횁 . We set this to be a small fraction of 홳 , which uses a negligible number of parameters compared to\\nthe main Linear projections in the block. We additionally note that the broadcasting operation can instead be\\nviewed as another Linear projection, initialized to a speciﬁc pattern of 1 ’s and 0 ’s; if this projection is trainable,\\nthis leads to the alternative 푠 ∆ (푥) = 햫헂헇햾햺헋 퐷 (햫헂헇햾햺헋 푅 (푥)) , which can be viewed as a low-rank projection.  \\nIn our experiments, the ∆ parameter (which can be viewed as a bias term) is initialized to 휏 −1 ∆ (햴헇헂햿허헋헆([0.001, 0.1])) ,\\nfollowing prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023 ).  \\n**Remark 3.1.** _For brevity in our experimental results, we sometimes abbreviate selective SSMs as_ S6 models _, because they_\\n_are S4 models with a_ selection _mechanism and computed with a_ scan _._', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '3 ## Selective State Space Models', 'Header 3': '3.6 ### Additional Model Details'}),\n",
       " Document(page_content='## 4 ## Empirical Evaluation  \\nIn Section 4.1 we test Mamba’s ability to solve the two synthetic tasks motivated in Section 3.1 . We then evaluate\\non three domains, each evaluated on autoregressive pretraining as well as downstream tasks.  \\n- Section 4.2 : language model pretraining (scaling laws), and zero-shot downstream evaluation.  \\n- Section 4.3 : DNA sequence pretraining, and ﬁne-tuning on a long-sequence classiﬁcation task.  \\n- Section 4.4 : audio waveform pretraining, and the quality of autoregressively generated speech clips.  \\nFinally, Section 4.5 shows Mamba’s computational eﬃciency at both training and inference time, and Section 4.6\\nablates various components of the architecture and selective SSMs.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '4 ## Empirical Evaluation'}),\n",
       " Document(page_content='### 4.1 ### Synthetic Tasks  \\nFull experiment details for these tasks including task details and training protocol are in Appendix E.1 .  \\n**4.1.1** **Selective Copying**  \\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\\nthe memorization abilities of recurrent models. As discussed in Section 3.1 , LTI SSMs (linear recurrences and\\nglobal convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for\\nexample, by constructing a convolution kernel of exactly the right length (Figure 2 ). This was explicitly validated\\nin earlier work on global convolutions (Romero et al. 2021 ). The Selective Copying task prevents this shortcut\\nby randomizing the spacing between tokens. Note that this task has been introduced before as the Denoising\\ntask (Jing et al. 2019 ).  \\nNote that many previous works argue that adding architecture gating (multiplicative interactions) can endow\\nmodels with “data-dependence” and solve related tasks (Dao, Fu, Saab, et al. 2023 ; Poli et al. 2023 ). However,\\nwe ﬁnd this explanation insuﬃcient intuitively because such gating does not interact along the sequence axis,\\nand cannot aﬀect the spacing between tokens. In particular architecture gating is not an instance of a selection\\nmechanism (Appendix A ).  \\nTable 1 conﬁrms that gated architectures such as H3 and Mamba only partially improve performance, while the\\nselection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more\\npowerful architectures.  \\n10  \\n-----  \\nInduction Heads Extrapolation  \\nModel Arch. Layer Acc.  \\n1.0  \\n0.8  \\nS4 No gate S4 18.3\\n- No gate S6 **97.0**  \\n0.6  \\n0.4  \\nH3 H3 S4 57.0\\nHyena H3 Hyena 30.1\\n- H3 S6 **99.7**  \\n0.2  \\nMHA-Absolute\\nMHA-RoPE\\nMHA-xPos\\nH3\\nHyena\\nMamba\\nRandom\\nTrain Length  \\n0.0  \\n10 2 10 3 10 4 10 5 10 6  \\n- Mamba S4 56.4\\n- Mamba Hyena 28.4\\nMamba Mamba S6 **99.8**  \\nTest Sequence Length  \\nTable 1: ( **Selective Copying** .)\\nAccuracy for combinations of architectures\\nand inner sequence layers.  \\nTable 2: ( **Induction Heads** .) Models are trained on sequence length\\n2 8  = 256 , and tested on increasing sequence lengths of 2 6  = 64 up to\\n2 20  = 1048576 . Full numbers in Table 11 .  \\n**4.1.2** **Induction Heads**  \\nInduction heads (Olsson et al. 2022 ) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021 )\\nthat is surprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative\\nrecall and copy: for example, if the model has seen a bigram such as “Harry Potter” in the sequence, then the\\nnext time “Harry” appears in the same sequence, the model should be able to predict “Potter” by copying from\\nhistory.  \\n**Dataset.**\\nWe train a 2-layer model on the induction heads task at sequence length 256 , with a vocab size of\\n16 , which is comparable to prior work on this task (Dao, Fu, Saab, et al. 2023 ) but with longer sequences. We\\nadditionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths\\nfrom 2 6  = 64 up to 2 20  = 1048576 at test time.  \\n**Models.** Following established work on induction heads, we use 2 layer models, which allows attention to\\nmechanistically solve the induction heads task (Olsson et al. 2022 ). We test both multi-head attention (8 heads,\\nwith various positional encodings) and SSM variants. We use a model dimension 퐷 of 64 for Mamba and 128 for\\nthe other models.  \\n**Results.** Table 2 shows that Mamba—or more precisely, its selective SSM layer—has the ability to solve the\\ntask perfectly because of its ability to selectively remember the relevant token while ignoring everything else in\\nbetween. It generalizes perfectly to million-length sequences, or 4000× longer than it saw during training, while no\\nother method goes beyond 2× .  \\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation)\\nis slightly better than the others; also note that all attention models were only tested up to sequence length\\n2 14  = 16384 due to memory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the ﬁndings in\\nPoli et al. ( 2023 ).', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '4 ## Empirical Evaluation', 'Header 3': '4.1 ### Synthetic Tasks'}),\n",
       " Document(page_content='### 4.2 ### Language Modeling  \\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on\\nboth pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to\\nmirror GPT3 speciﬁcations. We use the Pile dataset (L. Gao, Biderman, et al. 2020 ), and follow the training\\nrecipe described in Brown et al. ( 2020 ). All training details are in Appendix E.2 .  \\n**4.2.1** **Scaling Laws**  \\nFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the\\nstrongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa  \\n11  \\n-----  \\n|Scaling Laws on The Pile (Sequence Length 2048) 2 × 101 Hyena RWKV scale) Transformer RetNet (log H3++ Transformer++ 101 Perplexity Mamba 6 × 100 1019 1020 FLOPs (log scale)|Scaling Laws on The Pile (Sequence Length 8192) 2 × 101 Hyena RWKV scale) Transformer RetNet (log H3++ Transformer++ 101 Perplexity Mamba 6 × 100 1019 1020 FLOPs (log scale)|\\n|---|---|  \\nFigure 4: ( **Scaling Laws** .) Models of size ≈125푀 to ≈1.3퐵 parameters, trained on the Pile. Mamba scales better than all other\\nattention-free models and is the frst to match the performance of a very strong “Transformer++” recipe that has now become\\nstandard, particularly as the sequence length grows.  \\narchitectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher\\nlearning rates). We also compare against other recent subquadratic architectures (Figure 4 ). All model details are\\nin Appendix E.2 .  \\nFigure 4 shows scaling laws under the standard Chinchilla (Hoﬀmann et al. 2022 ) protocol, on models from\\n≈125푀 to ≈1.3퐵 parameters. Mamba is the ﬁrst attention-free model to match the performance of a very\\nstrong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length\\ngrows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior\\nstrong recurrent models that can also be interpreted as SSMs, due to a lack of eﬃcient implementation leading to\\nout-of-memory or unrealistic computation requirements.  \\n**4.2.2** **Downstream Evaluations**  \\nTable 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We\\ncompare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al.\\n2023 ) and RWKV (B. Peng et al. 2023 ) which were trained with the same tokenizer, dataset, and training length\\n(300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV\\nwas trained with context length 1024.)', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '4 ## Empirical Evaluation', 'Header 3': '4.2 ### Language Modeling'}),\n",
       " Document(page_content='### 4.3 ### DNA Modeling  \\nMotivated by the success of large language models, there has been recent exploration into using the foundation\\nmodel paradigm for genomics. DNA has been likened to language in that it consists of sequences of discrete\\ntokens with a ﬁnite vocab. It is also known for requiring long-range dependencies to model (Avsec et al. 2021 ).\\nWe investigate Mamba as a FM backbone for pretraining and ﬁne-tuning in the same setting as recent works on\\nlong-sequence models for DNA (Nguyen, Poli, et al. 2023 ). In particular, we focus on two explorations of scaling\\nlaws across model size and sequence length (Figure 5 ), and a diﬃcult downstream synthetic classiﬁcation task\\nrequiring long context (Figure 6 ).  \\nFor pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training\\nand model details (see also Appendix E.2 ). For the dataset, we largely follow the setup of HyenaDNA (Nguyen,\\nPoli, et al. 2023 ), which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5\\nbillion tokens (DNA base pairs) in the training split.  \\n**4.3.1** **Scaling: Model Size**  \\nIn this experiment, we investigate the scaling properties of genomics foundation models with various model\\nbackbones (Figure 5 Left).  \\n**Training.**\\nTo advantage the baselines, we train on a short sequence length of 1024 ; as shown in Section 4.3.2 , we\\nexpect results to favor Mamba even more at longer sequence lengths. We ﬁx a global batch size of 1024 , for a  \\n12  \\n-----  \\nTable 3: ( **Zero-shot Evaluations** .) Best results for each size in bold. We compare against open source LMs with various tokenizers,\\ntrained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and\\ntokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches\\nbaselines at twice the model size.  \\nModel Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande Average\\nppl ↓ ppl ↓ acc ↑ acc ↑ acc ↑ acc ↑ acc ↑ acc ↑ acc ↑  \\nHybrid H3-130M GPT2 — 89.48 25.77 31.7 64.2 44.4 24.2 50.6 40.1\\nPythia-160M NeoX 29.64 38.10 33.0 30.2 61.4 43.2 24.1 **51.9** 40.6\\n**Mamba-130M** NeoX **10.56** **16.07** **44.3** **35.3** **64.5** **48.0** **24.3** **51.9** **44.7**  \\nHybrid H3-360M GPT2 — 12.58 48.0 41.5 68.1 51.4 24.7 54.1 48.0\\nPythia-410M NeoX 9.95 10.84 51.4 40.6 66.9 52.1 24.6 53.8 48.2\\n**Mamba-370M** NeoX **8.28** **8.14** **55.6** **46.5** **69.5** **55.1** **28.0** **55.3** **50.0**  \\nPythia-1B NeoX 7.82 7.92 56.1 47.2 70.7 57.0 27.1 53.5 51.9\\n**Mamba-790M** NeoX **7.33** **6.02** **62.7** **55.1** **72.1** **61.2** **29.5** **56.1** **57.1**  \\nGPT-Neo 1.3B GPT2 — 7.50 57.2 48.9 71.1 56.2 25.9 54.9 52.4\\nHybrid H3-1.3B GPT2 — 11.25 49.6 52.6 71.3 59.2 28.1 56.9 53.0\\nOPT-1.3B OPT — 6.64 58.0 53.7 72.4 56.7 29.6 59.5 55.0\\nPythia-1.4B NeoX 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 55.2\\nRWKV-1.5B NeoX 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 54.3\\n**Mamba-1.4B** NeoX **6.80** **5.04** **64.9** **59.1** **74.2** **65.5** **32.8** **61.5** **59.7**  \\nGPT-Neo 2.7B GPT2 — 5.63 62.2 55.8 72.1 61.1 30.2 57.6 56.5\\nHybrid H3-2.7B GPT2 — 7.92 55.7 59.7 73.3 65.6 32.3 61.4 58.0\\nOPT-2.7B OPT — 5.12 63.6 60.6 74.8 60.8 31.3 61.0 58.7\\nPythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 59.1\\nRWKV-3B NeoX 7.00 5.24 63.9 59.6 73.7 67.8 33.1 59.6 59.6\\n**Mamba-2.8B** NeoX **6.22** **4.23** **69.2** **66.1** **75.2** **69.7** **36.3** **63.5** **63.3**  \\nGPT-J-6B GPT2 – 4.10 68.3 66.3 75.4 67.0 36.6 64.1 63.0\\nOPT-6.7B OPT – 4.25 67.7 67.2 76.3 65.6 34.9 65.5 62.9\\nPythia-6.9B NeoX 6.51 4.45 67.1 64.0 75.2 67.3 35.5 61.3 61.7\\nRWKV-7.4B NeoX 6.31 4.38 67.2 65.5 76.1 67.8 37.5 61.0 62.5  \\ntotal of 2 20  ≈1푀 tokens per batch. Models were trained for 10퐾 gradient steps for a total of 10퐵 tokens.  \\n**4.3.2** **Scaling: Context Length**  \\n**Results.** Figure 5 (Left) shows that Mamba’s pretraining perplexity improves smoothly with model size, and\\nthat Mamba scales better than both HyenaDNA and Transformer++. For example, at the largest model size of\\n≈40푀 parameters, the curve shows that Mamba can match the Transformer++ and HyenaDNA models with\\nroughly 3× to 4× fewer parameters.  \\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length.\\nWe only compare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at\\nlonger sequence lengths. We pretrain models on sequence lengths 2 10  = 1024 , 2 12  = 4096 , 2 14  = 16384 , 2 16  = 65536 ,\\n2 18  = 262144 , 2 20  = 1048576 . We ﬁx a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models\\nwere trained for 20퐾 gradient steps for a total of ≈330퐵 tokens. The longer sequence lengths used sequence length\\nwarmup similar to (Nguyen, Poli, et al. 2023 ).  \\n**Results.** Figure 5 (Right) shows that Mamba is able to make use of longer context even up to extremely long\\nsequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand,\\nthe HyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on\\nproperties of the selection mechanism. In particular, LTI models cannot selectively ignore information; from a\\nconvolutional perspective, a very long convolution kernel is aggregating all information across a long sequence  \\n13  \\n-----  \\n|Scaling Laws on the Human Genome (HG38) HyenaDNA 3.1 Mamba Transformer++ 3.0 Perplexity 2.9 2.8 2.7 106 107 Parameters (log scale)|Scaling Laws - Sequence Length (HG38) 3.00 HyenaDNA 1.4M Mamba 1.4M 2.95 Mamba 7M 2.90 Perplexity 2.85 2.80 2.75 103 104 105 106 Sequence Length|\\n|---|---|  \\nFigure 5: ( **DNA Scaling Laws** .) Pretraining on the HG38 (human genome) dataset. ( _Left_ ) Fixing short context length 2 10  = 1024\\nand increasing size from ≈200퐾 to ≈40푀 parameters, Mamba scales better than baselines. ( _Right_ ) Fixing model size and increasing\\nsequence lengths while keeping tokens/batch and total training tokens fxed. Unlike baselines, the selection mechanism of Mamba\\nfacilitates better performance with increasing context length.  \\nFinetuning Accuracy (Species DNA Classification)  \\nScaling Laws - Sequence Length (YouTubeMix)  \\n1.475  \\n0.8  \\n1.450  \\nS4+FFN\\nMamba  \\n0.7  \\n1.425  \\nHyenaDNA 1.4M\\nMamba 1.4M\\nMamba 7M\\nRandom  \\n0.6  \\n1.400  \\n0.5  \\n1.375  \\n0.4  \\n1.350  \\n0.3  \\n1.325  \\n0.2  \\n1.300  \\n10 3 10 4 10 5 10 6  \\n10 4 10 5 10 6  \\nSequence Length  \\nSequence Length  \\nFigure 7: ( **Audio Pretraining** .) Mamba improves performance\\nover prior state-of-the-art (Sashimi) in autoregressive audio mod-\\neling, while improving up to minute-long context or million-\\nlength sequences (controlling for computation).  \\nFigure 6: ( **Great Apes DNA Classifcation** .) Accuracy after\\nfne-tuning on sequences of length 2 10  = 1024 up to 2 20  =\\n1048576 using pretrained models of the same context length. Nu-\\nmerical results in Table 13 .  \\nwhich may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not\\ncontrol for computation time.  \\n**4.3.3** **Synthetic Species Classifcation**  \\nWe evaluate models on a downstream task of classifying between 5 diﬀerent species by randomly sampling a contigu-\\nous segment of their DNA. This task is adapted from HyenaDNA, which used the species { `human` , `lemur` , `mouse` , `pig` , `hippo` } .\\nWe modify the task to be signiﬁcantly more challenging by classifying between the ﬁve great apes species\\n{ `human` , `chimpanzee` , `gorilla` , `orangutan` , `bonobo` } , which are known to share 99% of their DNA.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '4 ## Empirical Evaluation', 'Header 3': '4.3 ### DNA Modeling'}),\n",
       " Document(page_content='### 4.4 ### Audio Modeling and Generation  \\nFor the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel\\net al. 2022 ). This model comprises  \\n2. alternating S4 and MLP blocks in each stage.  \\n1. a U-Net backbone with two stages of pooling by a factor 푝 that doubles the model dimension 퐷 per stage,  \\nWe consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4 .  \\n**4.4.1** **Long-Context Autoregressive Pretraining**  \\nWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017 ), a\\nstandard piano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of  \\n14  \\n-----  \\n16000 Hz Pretraining details largely follow the standard language modeling setup (Section 4.2 ). Figure 7 evaluates\\nthe eﬀect of increasing training sequence lengths from 2 13  = 8192 to 2 20  ≈10 6 , while keeping computation ﬁxed.\\n(There are some slight edge cases to the way the data is curated, which may lead to kinks in the scaling curves.\\nFor example, only minute-long clips were available so the maximum sequence length is actually bounded by\\n60푠⋅16000퐻푧= 960000 .)  \\nBoth Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is\\nbetter throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a\\nconstant factor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.  \\nWe note one important detail: this is the only experiment in this paper in which we switched from the real\\nparameterization to complex (Section 3.6 ). We show additional ablations in Appendix E.4 .  \\n**4.4.2** **Autoregressive Speech Generation**  \\nSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019 ; Warden 2018 ), consisting\\nof 1 -second clips sampled at 16000 Hz of the digits “zero” through “nine” with highly variable characteristics. We\\nlargely follow the autoregressive training setup and generation protocol of Goel et al. ( 2022 ).  \\nTable 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al.\\n( 2022 ): WaveNet (Oord et al. 2016 ), SampleRNN (Mehri et al. 2017 ), WaveGAN (Donahue, McAuley, and Puckette\\n2019 ), DiﬀWave (Z. Kong et al. 2021 ), and SaShiMi. A small Mamba model outperforms the state-of-the-art\\n(and much larger) GAN- and diﬀusion- based models. A larger model parameter-matched to the baselines further\\nimproves on ﬁdelity metrics dramatically.  \\nTable 5 takes the small Mamba model and investigates combinations of diﬀerent architectures for the outer stages\\nand center stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba &gt;\\nS4+MLP &gt; MHA+MLP in the center blocks.  \\nTable 4: ( **SC09** ) Automated metrics for unconditional generation\\non a challenging dataset of fxed-length speech clips. ( _Top to_\\n_Bottom_ ) Autoregressive baselines, non-autoregressive baselines,\\nMamba, and dataset metrics.  \\nModel Params\\nNLL ↓ FID ↓ IS ↑ mIS ↑ AM ↓  \\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters.\\nIn SaShiMi’s U-Net backbone, there are 8 center blocks operat-\\ning on sequence length 1000 , sandwiched on each side by 8 outer\\nblocks on sequence length 4000 , sandwiched by 8 outer blocks\\non sequence length 16000 (40 blocks total). The architecture of\\nthe 8 center blocks are ablated independently of the rest. Note\\nthat Transformers (MHA+MLP) were not tested in the more im-\\nportant outer blocks because of efciency constraints.  \\nSampleRNN 35.0M 2.042 8.96 1.71 3.02 1.76\\nWaveNet 4.2M 1.925 5.08 2.27 5.80 1.47\\nSaShiMi 5.8M 1.873 1.99 5.13 42.57 0.74  \\nWaveGAN 19.1M - 2.03 4.90 36.10 0.80\\nDifWave 24.1M - 1.92 5.26 51.21 0.68\\n+ SaShiMi 23.0M - 1.42 5.94 69.17 0.59  \\nOuter Center\\nNLL ↓ FID ↓ IS ↑ mIS ↑ AM ↓  \\n**Mamba** 6.1M **1.852** 0.94 6.26 88.54 0.52  \\n**Mamba** 24.3M 1.860 **0.67** **7.33** **144.9** **0.36**  \\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55  \\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**  \\nTrain - -\\n0.00 8.56 292.5 0.16\\nTest - -\\n0.02 8.33 257.6 0.19', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '4 ## Empirical Evaluation', 'Header 3': '4.4 ### Audio Modeling and Generation'}),\n",
       " Document(page_content='### 4.5 ### Speed and Memory Benchmarks  \\nWe benchmark the speed of the SSM scan operation (state expansion 푁= 16 ), as well as the end-to-end inference\\nthroughput of Mamba, in Figure 8 . Our eﬃcient SSM scan is faster than the best attention implementation that\\nwe know of (FlashAttention-2 (Dao 2023 )) beyond sequence length 2K, and up to 20-40 × faster than a standard\\nscan implementation in PyTorch. Mamba achieves 4-5 × higher inference throughput than a Transformer of similar\\nsize, since without the KV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained)\\nwould have higher inference throughput than a 5× smaller Transformer-1.3B. Details in Appendix E.5 , which\\nadditionally includes a benchmark of memory consumption.  \\n15  \\n-----  \\nInference throughput on A100 80GB (prompt length 2048)  \\nScan vs Convolution vs Attention time (A100 80GB PCIe)  \\n1814  \\nFlashAttention-2 1688  \\nMamba 1.4B\\nTransformer 1 3B  \\n|Transformer 1.3B Mamba 6.9B 1445 Transformer 6.7B 1089 744 441 443 323364 364 140 247 132 199172 265261 109 120|Col2|490|Col4|515|\\n|---|---|---|---|---|  \\n101  \\n79  \\n58  \\n46 66 91 109  \\nOOM OOM OOM  \\nOOM OOM  \\n512 1k 2k 4k 8k 16k 32k 64k 128k 256k 512k  \\n1 2 4 8 16 32 64 128  \\nSequence length  \\nBatch size  \\nFigure 8: ( **Efciency Benchmarks** .) ( _Left_ ) Training: our efcient scan is 40× faster than a standard implementation. ( _Right_ )\\nInference: as a recurrent model, Mamba can achieve 5× higher throughput than Transformers.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '4 ## Empirical Evaluation', 'Header 3': '4.5 ### Speed and Memory Benchmarks'}),\n",
       " Document(page_content='### 4.6 ### Model Ablations  \\n**4.6.1** **Architecture**  \\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling\\nwith size ≈350 M models at Chinchilla token counts (same setting as Figure 4 ).  \\nTable 6 investigates the eﬀects of the architecture (block) and its inner SSM layer (Figure 3 ). We ﬁnd that  \\n- Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very\\nsimilar.  \\n- Replacing the complex-valued S4 variant from previous work with a real-valued one does not aﬀect performance\\nmuch, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware\\neﬃciency.  \\n- Replacing any of these with a selective SSM (S6) signiﬁcantly improves performance, validating the motivation\\nof Section 3 .  \\n- The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a\\nselective layer).  \\nWe also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA\\n(a hybrid attention architecture) in Appendix E.2.2 .  \\n**4.6.2** **Selective SSM**  \\nTable 7 ablates the selective SSM layer by considering diﬀerent combinations of selective ∆ , **_B_** , and **_C_** param-\\neters (Algorithm 2 ), showing that ∆ is the most important parameter due to its connection to RNN gating\\n(Theorem 1 ).  \\nTable 8 considers diﬀerent initializations of the SSM, which have been shown to make a large diﬀerence in some\\ndata modalities and settings (Gu, Goel, and Ré 2022 ; Gu, Gupta, et al. 2022 ). On language modeling, we ﬁnd\\nthat simpler real-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued\\nparameterizations (S4D-Lin, row 1) perform better. Random initializations also work well, consistent with ﬁndings\\nfrom prior work (Mehta et al. 2023 ).  \\nTable 9 and Table 10 consider varying the dimension of the ∆ and ( **_B_** , **_C_** ) projections respectively. Changing\\nthem from static to selective provides the most beneﬁt, while increasing the dimensions further generally improves\\nperformance modestly with a small increase in parameter count.  \\nOf particular note is the dramatic improvement of the selective SSM when the state size 푁 is increased, with over\\na 1.0 perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in\\nSections 3.1 and 3.3 .  \\n16  \\n-----  \\nTable 6: ( **Ablations: Architecture and SSM layer** .) The Mamba block performs similarly to H3 while being simpler. In the\\ninner layer, there is little diference among diferent parameterizations of LTI models, while selective SSMs (S6) provide a large\\nimprovement. More specifcally, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.  \\nModel Arch. SSM Layer Perplexity  \\nModel Arch. SSM Layer Perplexity  \\nHyena H3 Hyena 10.24\\nH3 H3 S4 (complex) 10.30\\n- H3 S4 (real) 10.34\\n- H3 S6 **8.95**  \\n- Mamba Hyena 10.75\\n- Mamba S4 (complex) 10.54\\n- Mamba S4 (real) 10.56\\nMamba Mamba S6 **8.69**  \\nTable 7: ( **Ablations: Selective parameters** .) ∆ is the most im-\\nportant parameter (Theorem 1 ), but using multiple selective pa-\\nrameters together synergizes.  \\nTable 8: ( **Ablations: Parameterization of** **_A_** .) The more\\nstandard initializations based on S4D-Lin (Gu, Gupta, et al.\\n2022 ) perform worse than S4D-Real or a random initializa-\\ntion, when the SSM is selective.  \\nSelective ∆\\nSelective **_B_** Selective **_C_** Perplexity  \\n**_A_** 푛 Initialization\\nField Perplexity  \\n**_A_** 푛 = − 1  \\n\\x17 \\x17 \\x17 10.93\\n\\x17 \\x13 \\x17 10.15\\n\\x17 \\x17 \\x13 9.98\\n\\x13 \\x17 \\x17 9.81\\n\\x13 \\x13 \\x13 8.71  \\n2  + 푛푖\\nComplex 9.16\\n**_A_** 푛 = −1∕2\\nReal 8.85\\n**_A_** 푛 = −(푛+ 1)\\nReal 8.71\\n**_A_** 푛 ∼exp(풩(0, 1))\\nReal 8.71  \\nTable 10: ( **Ablations: SSM state dimension** .) ( _Top_ ) Constant **_B_** and **_C_** ( _Bottom_ )\\nSelective **_B_** and **_C_** . Increasing the SSM state dimension 푁 , which can be viewed as\\nan expansion factor on the dimension of the recurrent state, can signifcantly improve\\nperformance for a negligible cost in parameters/FLOPs, but only when **_B_** and **_C_** are\\nalso selective. Size of ∆ projection fxed to 64 .  \\nState dimension 푁\\nParams (M) Perplexity  \\nTable 9: ( **Ablations: Expressivity of** ∆ .)\\nThe selection mechanism of ∆ constructs\\nit with a projection of the input. Project-\\ning it even to dim. 1 provides a large in-\\ncrease in performance; increasing it fur-\\nther provides further improvements at the\\ncost of a modest increase in parameters.\\nState size fxed to 푁= 16 .  \\nSize of ∆ proj.\\nParams (M) Perplexity  \\n1\\n367.1 9.88\\n2\\n367.4 9.86\\n4\\n368.0 9.82\\n8\\n369.1 9.82\\n16\\n371.5 9.81  \\n1\\n367.1 9.73\\n2\\n367.4 9.40\\n4\\n368.0 9.09\\n8\\n369.1 8.84\\n16\\n371.5 8.71  \\n- 358.9 9.12\\n1\\n359.1 8.97\\n2\\n359.3 8.97\\n4\\n359.7 8.91\\n8\\n360.5 8.83\\n16\\n362.1 8.84\\n32\\n365.2 8.80\\n64\\n371.5 8.71', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '4 ## Empirical Evaluation', 'Header 3': '4.6 ### Model Ablations'}),\n",
       " Document(page_content='## 5 ## Discussion  \\nWe discuss related work, limitations, and some future directions.  \\n**Related Work.** Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has\\nan extended related work of SSMs and other related models.  \\n**No Free Lunch: Continuous-Discrete Spectrum.** Structured SSMs were originally deﬁned as discretizations\\nof continuous systems ( 1 ) , and have had a strong inductive bias toward continuous-time data modalities such as\\nperceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5 , the selection mechanism overcomes\\ntheir weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance  \\n17  \\n-----  \\non data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeoﬀin more detail.  \\n**Downstream Afordances.** Transformer-based foundation models (particularly LLMs) have a rich ecosystem of\\nproperties and modes of interaction with pretrained models, such as ﬁne-tuning, adaptation, prompting, in-context\\nlearning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer\\nalternatives such as SSMs have similar properties and aﬀordances.  \\n**Scaling.** Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source\\nLLMs (e.g. Llama (Touvron et al. 2023 )) as well as other recurrent models such as RWKV (B. Peng et al. 2023 )\\nand RetNet (Y. Sun et al. 2023 ), which have been evaluated at the 7B parameter scale and beyond. It remains to\\nassess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve\\nfurther engineering challenges and adjustments to the model that are not discussed in this paper.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '5 ## Discussion'}),\n",
       " Document(page_content='## 6 ## Conclusion  \\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture,\\nMamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance\\nof strong Transformer models. We are excited about the broad applications of selective state space models to\\nbuild foundation models for diﬀerent domains, especially in emerging modalities requiring long context such as\\ngenomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\\nbackbone.  \\n**Acknowledgments**  \\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': '6 ## Conclusion'}),\n",
       " Document(page_content='## References  \\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. “Unitary Evolution Recurrent Neural Networks”. In: _The_\\n_International Conference on Machine Learning (ICML)_ . 2016, pp. 1120–1128.  \\n[2] iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. “Efective Gene Expression Prediction from\\nSequence by Integrating Long-range Interactions”. In: _Nature Methods_ 18.10 (2021), pp. 1196–1203.  \\n[3] Jimmy Ba, Geofrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. “Using Fast Weights to\\nAttend to the Recent Past”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 29 (2016).  \\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geofrey E Hinton. “Layer Normalization”. In: _arXivpreprintarXiv:1607.06450_\\n(2016).  \\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural Machine Translation by Jointly Learning to\\nAlign and Translate”. In: _The International Conference on Learning Representations (ICLR)_ . 2015.  \\n[6] David Balduzzi and Muhammad Ghifary. “Strongly-typed Recurrent Neural Networks”. In: _International Con-_\\n_ference on Machine Learning_ . PMLR. 2016, pp. 1292–1300.  \\n[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,\\nMohammad Afah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raf, et al. “Pythia: A Suite for\\nAnalyzing Large Language Models across Training and Scaling”. In: _The International Conference on Machine_\\n_Learning (ICML)_ . PMLR. 2023, pp. 2397–2430.  \\n[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. “PIQA: Reasoning about Physical Commonsense\\nin Natural Language”. In: _Proceedings of the AAAI conference on Artifcial Intelligence_ . Vol. 34. 05. 2020, pp. 7432–\\n7439.  \\n[9] Guy E Blelloch. “Prefx Sums and Their Applications”. In: (1990).  \\n[10] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. “Quasi-recurrent Neural Networks”. In:\\n_arXiv preprint arXiv:1611.01576_ (2016).  \\n18  \\n-----  \\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are Few-shot Learners”. In:\\n_Advances in Neural Information Processing Systems (NeurIPS)_ 33 (2020), pp. 1877–1901.  \\n[12] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. “Scaling Transformer to 1M tokens and Beyond with RMT”.\\nIn: _arXiv preprint arXiv:2304.11062_ (2023).  \\n[13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. “Generating Long Sequences with Sparse Trans-\\nformers”. In: _arXiv preprint arXiv:1904.10509_ (2019).  \\n[14] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe-\\nter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. “Rethinking Attention with Performers”. In:\\n_The International Conference on Learning Representations (ICLR)_ . 2021.  \\n[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. “PaLM: Scaling Language Modeling\\nwith Pathways”. In: _Journal of Machine Learning Research_ 24.240 (2023), pp. 1–113. url: `http://jmlr.org/`\\n`papers/v24/22-1144.html` .  \\n[16] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. “Empirical Evaluation of Gated Re-\\ncurrent Neural Networks on Sequence Modeling”. In: _arXiv preprint arXiv:1412.3555_ (2014).  \\n[17] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\\nTafjord. “Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge”. In: _arXiv_\\n_preprint arXiv:1803.05457_ (2018).  \\n[18] Tri Dao. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”. In: (2023).  \\n[19] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “FlashAttention: Fast and Memory-\\nEfcient Exact Attention with IO-Awareness”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ .\\n2022.  \\n[20] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. “Hungry Hungry\\nHippos: Towards Language Modeling with State Space Models”. In: _The International Conference on Learning_\\n_Representations (ICLR)_ . 2023.  \\n[21] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. “Language Modeling with Gated Convolu-\\ntional Networks”. In: _The International Conference on Machine Learning (ICML)_ . PMLR. 2017, pp. 933–941.  \\n[22] DeepSound. _SampleRNN_ . `https://github.com/deepsound-project/samplernn-pytorch` . 2017.  \\n[23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. “LongNet:\\nScaling Transformers to 1,000,000,000 Tokens”. In: _arXiv preprint arXiv:2307.02486_ (2023).  \\n[24] Chris Donahue, Julian McAuley, and Miller Puckette. “Adversarial Audio Synthesis”. In: _The International_\\n_Conference on Learning Representations (ICLR)_ . 2019.  \\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. “An Image is Worth 16x16 Words:\\nTransformers for Image Recognition at Scale”. In: _The International Conference on Learning Representations_\\n_(ICLR)_ . 2020.  \\n[26] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\\nYuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny\\nHernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack\\nClark, Jared Kaplan, Sam McCandlish, and Chris Olah. “A Mathematical Framework for Transformer Circuits”.\\nIn: _Transformer Circuits Thread_ (2021). https://transformer-circuits.pub/2021/framework/index.html.  \\n[27] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. “Block-\\nState Transformer”. In: _arXiv preprint arXiv:2306.09539_ (2023).  \\n[28] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,\\nYangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. “Multi-Head State Space Model for Sequence Modeling”. In:\\n_INTERSPEECH_ . 2023.  \\n[29] Karl J Friston, Lee Harrison, and Will Penny. “Dynamic Causal Modelling”. In: _Neuroimage_ 19.4 (2003), pp. 1273–\\n1302.  \\n[30] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christo-\\npher Ré. “Simple Hardware-efcient Long Convolutions for Sequence Modeling”. In: _The International Confer-_\\n_ence on Machine Learning (ICML)_ (2023).  \\n[31] Ken-ichi Funahashi and Yuichi Nakamura. “Approximation of Dynamical Systems by Continuous Time Recur-\\nrent Neural Networks”. In: _Neural Networks_ 6.6 (1993), pp. 801–806.  \\n19  \\n-----  \\n[32] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\\nAnish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. “The Pile: An 800GB Dataset of Diverse Text\\nfor Language Modeling”. In: _arXiv preprint arXiv:2101.00027_ (2020).  \\n[33] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPof, Charles Foster, Laurence Golding, Jefrey\\nHsu, Kyle McDonell, Niklas Muennighof, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,\\nKevin Wang, and Andy Zou. _A Framework for Few-shot Language Model Evaluation_ . Version v0.0.1. Sept. 2021.\\ndoi: `10.5281/zenodo.5371628` . url: `https://doi.org/10.5281/zenodo.5371628` .  \\n[34] Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. “It’s Raw! Audio Generation with State-Space\\nModels”. In: _The International Conference on Machine Learning (ICML)_ . 2022.  \\n[35] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. “HIPPO: Recurrent Memory with Optimal\\nPolynomial Projections”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ . 2020.  \\n[36] Albert Gu, Karan Goel, and Christopher Ré. “Efciently Modeling Long Sequences with Structured State Spaces”.\\nIn: _The International Conference on Learning Representations (ICLR)_ . 2022.  \\n[37] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hofman, and Razvan Pascanu. “Improving the Gating Mech-\\nanism of Recurrent Neural Networks”. In: _The International Conference on Machine Learning (ICML)_ . 2020.  \\n[38] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. “On the Parameterization and Initialization of Diag-\\nonal State Space Models”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ . 2022.  \\n[39] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. “Combining Recur-\\nrent, Convolutional, and Continuous-time Models with the Linear State Space Layer”. In: _Advances in Neural_\\n_Information Processing Systems (NeurIPS)_ . 2021.  \\n[40] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. “How to Train Your HIPPO: State\\nSpace Models with Generalized Basis Projections”. In: _The International Conference on Learning Representations_\\n_(ICLR)_ . 2023.  \\n[41] Ankit Gupta, Albert Gu, and Jonathan Berant. “Diagonal State Spaces are as Efective as Structured State\\nSpaces”. In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 22982–22994.  \\n[42] David Ha, Andrew Dai, and Quoc V. Le. “HyperNetworks”. In: _The International Conference on Learning Rep-_\\n_resentations (ICLR)_ . 2017.  \\n[43] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. “Dream to Control: Learning Behav-\\niors by Latent Imagination”. In: _The International Conference on Learning Representations (ICLR)_ . 2020.  \\n[44] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus.\\n“Liquid Structural State-Space Models”. In: _The International Conference on Learning Representations (ICLR)_ .\\n2023.  \\n[45] Mikael Henaf, Arthur Szlam, and Yann LeCun. “Recurrent Orthogonal Networks and Long-Memory Tasks”.\\nIn: _The International Conference on Machine Learning (ICML)_ . 2016.  \\n[46] Dan Hendrycks and Kevin Gimpel. “Gaussian Error Linear Units (GELUs)”. In: _arXiv preprint arXiv:1606.08415_\\n(2016).  \\n[47] Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In: _Neural Computation_ 9.8 (1997),\\npp. 1735–1780.  \\n[48] Jordan Hofmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. “An Empirical Analysis of Compute-\\nOptimal Large Language Model Training”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 35\\n(2022), pp. 30016–30030.  \\n[49] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. “Transformer Quality in Linear Time”. In: _The Interna-_\\n_tional Conference on Machine Learning (ICML)_ . PMLR. 2022, pp. 9099–9117.  \\n[50] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. “Deep\\nLearning for Time Series Classifcation: A Review”. In: _Data Mining and Knowledge Discovery_ 33.4 (2019),\\npp. 917–963.  \\n[51] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefer. “Data Movement is All You Need:\\nA Case Study on Optimizing Transformers”. In: _Proceedings of Machine Learning and Systems_ 3 (2021), pp. 711–\\n732.  \\n[52] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. “Gated\\nOrthogonal Recurrent Units: On Learning to Forget”. In: _Neural Computation_ 31.4 (2019), pp. 765–783.  \\n[53] Rudolph Emil Kalman. “A New Approach to Linear Filtering and Prediction Problems”. In: (1960).  \\n20  \\n-----  \\n[54] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast\\nAutoregressive Transformers with Linear Attention”. In: _International Conference on Machine Learning_ . PMLR.\\n2020, pp. 5156–5165.  \\n[55] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. “DifWave: A Versatile Difusion Model\\nfor Audio Synthesis”. In: _International Conference on Learning Representations_ . 2021.  \\n[56] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. “Time-Parameterized Convolutional Neu-\\nral Networks for Irregularly Sampled Time Series”. In: _arXiv preprint arXiv:2308.03210_ (2023).  \\n[57] Alex Krizhevsky, Ilya Sutskever, and Geofrey E Hinton. “ImageNet Classifcation with Deep Convolutional\\nNeural Networks”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 25 (2012).  \\n[58] Tao Lei. “When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute”. In:\\n_Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_ . 2021, pp. 7633–7648.  \\n[59] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. “Simple Recurrent Units for Highly Parallelizable\\nRecurrence”. In: _arXiv preprint arXiv:1709.02755_ (2017).  \\n[60] Mario Lezcano-Casado and David Martínez-Rubio. “Cheap Orthogonal Constraints in Neural Networks: A\\nSimple Parametrization of the Orthogonal and Unitary Group”. In: _The International Conference on Machine_\\n_Learning (ICML)_ . 2019.  \\n[61] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. “What Makes Convolutional Models\\nGreat on Long Sequence Modeling?” In: _The International Conference on Learning Representations (ICLR)_ . 2023.  \\n[62] Vasileios Lioutas and Yuhong Guo. “Time-aware Large Kernel Convolutions”. In: _The International Conference_\\n_on Machine Learning (ICML)_ . PMLR. 2020, pp. 6172–6183.  \\n[63] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behba-\\nhani. “Structured State Space Models for In-Context Reinforcement Learning”. In: _Advances in Neural Informa-_\\n_tion Processing Systems (NeurIPS)_ . 2023.  \\n[64] Shahar Lutati, Itamar Zimerman, and Lior Wolf. “Focus Your Attention (with Adaptive IIR Filters)”. In: _arXiv_\\n_preprint arXiv:2305.14952_ (2023).  \\n[65] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke\\nZettlemoyer. “Mega: Moving Average Equipped Gated Attention”. In: _The International Conference on Learning_\\n_Representations (ICLR)_ . 2023.  \\n[66] Eric Martin and Chris Cundy. “Parallelizing Linear Recurrent Neural Nets Over Sequence Length”. In: _The_\\n_International Conference on Learning Representations (ICLR)_ . 2018.  \\n[67] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville,\\nand Yoshua Bengio. “SampleRNN: An Unconditional End-to-End Neural Audio Generation Model”. In: _The_\\n_International Conference on Learning Representations (ICLR)_ . 2017.  \\n[68] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. “Long Range Language Modeling via\\nGated State Spaces”. In: _The International Conference on Learning Representations (ICLR)_ . 2023.  \\n[69] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. “Efcient Orthogonal Parametri-\\nsation of Recurrent Neural Networks using Householder Refections”. In: _International Conference on Machine_\\n_Learning_ . PMLR. 2017, pp. 2401–2409.  \\n[70] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Ré.\\n“S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces”. In: _Advances in Neural_\\n_Information Processing Systems (NeurIPS)_ . 2022.  \\n[71] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Pa-\\ntel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. “HyenaDNA: Long-range Genomic Sequence\\nModeling at Single Nucleotide Resolution”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ .\\n2023.  \\n[72] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny\\nHernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom\\nBrown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. “In-context Learning and Induction Heads”.\\nIn: _Transformer Circuits Thread_ (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-\\nheads/index.html.  \\n[73] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch-\\nbrenner, Andrew Senior, and Koray Kavukcuoglu. “WaveNet: A Generative Model for Raw Audio”. In: _arXiv_\\n_preprint arXiv:1609.03499_ (2016).  \\n21  \\n-----  \\n[74] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and So-\\nham De. “Resurrecting Recurrent Neural Networks for Long Sequences”. In: _The International Conference on_\\n_Machine Learning (ICML)_ . 2023.  \\n[75] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Rafaella Bernardi, Sandro Pezzelle,\\nMarco Baroni, Gemma Boleda, and Raquel Fernández. “The LAMBADA Dataset: Word Prediction Requiring\\na Broad Discourse Context”. In: _Proceedings of the 54th Annual Meeting of the Association for Computational_\\n_Linguistics_ . 2016, pp. 1525–1534.  \\n[76] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. “On the Difculty of Training Recurrent Neural Net-\\nworks”. In: _International Conference on Machine Learning_ . 2013, pp. 1310–1318.  \\n[77] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael\\nChung, Matteo Grella, Kranthi Kiran GV, et al. “RWKV: Reinventing RNNs for the Transformer Era”. In: _arXiv_\\n_preprint arXiv:2305.13048_ (2023).  \\n[78] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. “Random\\nFeature Attention”. In: _The International Conference on Learning Representations (ICLR)_ . 2021.  \\n[79] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano\\nErmon, and Christopher Ré. “Hyena Hierarchy: Towards Larger Convolutional Language Models”. In: _The_\\n_International Conference on Machine Learning (ICML)_ . 2023.  \\n[80] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and\\nYiran Zhong. “Toeplitz Neural Network for Sequence Modeling”. In: _The International Conference on Learning_\\n_Representations (ICLR)_ . 2023.  \\n[81] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. “The devil\\nin linear transformer”. In: _arXiv preprint arXiv:2210.10340_ (2022).  \\n[82] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and\\nYiran Zhong. “CosFormer: Rethinking Softmax in Attention”. In: _The International Conference on Learning_\\n_Representations (ICLR)_ . 2022.  \\n[83] Ali Rahimi and Benjamin Recht. “Random features for large-scale kernel machines”. In: _Advances in neural_\\n_information processing systems_ 20 (2007).  \\n[84] Prajit Ramachandran, Barret Zoph, and Quoc V Le. “Swish: A Self-gated Activation Function”. In: _arXiv preprint_\\n_arXiv:1710.05941_ 7.1 (2017), p. 5.  \\n[85] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. “CKConv: Con-\\ntinuous Kernel Convolution For Sequential Data”. In: _arXiv preprint arXiv:2102.02611_ (2021).  \\n[86] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. “Winogrande: An Adversarial Wino-\\ngrad Schema Challenge at Scale”. In: _Communications of the ACM_ 64.9 (2021), pp. 99–106.  \\n[87] George Saon, Ankit Gupta, and Xiaodong Cui. “Diagonal State Space Augmented Transformers for Speech\\nRecognition”. In: _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing_\\n_(ICASSP)_ . IEEE. 2023, pp. 1–5.  \\n[88] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. “Linear Transformers are Secretly Fast Weight Program-\\nmers”. In: _The International Conference on Machine Learning (ICML)_ . PMLR. 2021, pp. 9355–9366.  \\n[89] Noam Shazeer. “GLU Variants Improve Transformer”. In: _arXiv preprint arXiv:2002.05202_ (2020).  \\n[90] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and\\nDenny Zhou. “Large Language Models can be Easily Distracted by Irrelevant Context”. In: _The International_\\n_Conference on Machine Learning (ICML)_ . PMLR. 2023, pp. 31210–31227.  \\n[91] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. “Sequence Modeling with Multiresolution Convolutional Mem-\\nory”. In: _The International Conference on Machine Learning (ICML)_ . PMLR. 2023, pp. 31312–31327.  \\n[92] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. “Simplifed State Space Layers for Sequence\\nModeling”. In: _The International Conference on Learning Representations (ICLR)_ . 2023.  \\n[93] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. “Roformer: Enhanced Trans-\\nformer with Rotary Position Embedding”. In: _arXiv preprint arXiv:2104.09864_ (2021).  \\n[94] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\\n“Retentive network: A successor to transformer for large language models”. In: _arXiv preprint arXiv:2307.08621_\\n(2023).  \\n[95] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. “Sequence to Sequence Learning with Neural Networks”. In:\\n_Advances in Neural Information Processing Systems (NeurIPS)_ 27 (2014).  \\n22  \\n-----  \\n[96] Corentin Tallec and Yann Ollivier. “Can Recurrent Neural Networks Warp Time?” In: _The International Con-_\\n_ference on Learning Representations (ICLR)_ . 2018.  \\n[97] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se-\\nbastian Ruder, and Donald Metzler. “Long Range Arena: A Benchmark for Efcient Transformers”. In: _Inter-_\\n_national Conference on Learning Representations (ICLR)_ . 2021.  \\n[98] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. “Efcient Transformers: A Survey”. In: _ACM Com-_\\n_puting Surveys_ 55.6 (2022), pp. 1–28.  \\n[99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-\\ntiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. “Llama: Open and Efcient Foundation Language\\nModels”. In: _arXiv preprint arXiv:2302.13971_ (2023).  \\n[100] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. “Attention Is All You Need”. In: _AdvancesinNeuralInformationProcessingSystems(NeurIPS)_ .\\n2017.  \\n[101] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. “On Orthogonality and Learning Recur-\\nrent Networks with Long Term Dependencies”. In: _International Conference on Machine Learning_ . PMLR. 2017,\\npp. 3570–3578.  \\n[102] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Rafay Hamid. “Selective\\nStructured State-Spaces for Long-form Video Understanding”. In: _Proceedings of the IEEE/CVF Conference on_\\n_Computer Vision and Pattern Recognition_ . 2023, pp. 6387–6397.  \\n[103] Pete Warden. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition”. In: _ArXiv_ abs/1804.03209\\n(2018).  \\n[104] Samuel Williams, Andrew Waterman, and David Patterson. “Roofine: An Insightful Visual Performance Model\\nfor Multicore Architectures”. In: _Communications of the ACM_ 52.4 (2009), pp. 65–76.  \\n[105] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. “CondConv: Conditionally Parameterized Con-\\nvolutions for Efcient Inference”. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 32 (2019).  \\n[106] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. “HellaSwag: Can a Machine Really\\nFinish Your Sentence?” In: _Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-_\\n_tics_ . 2019.  \\n[107] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.\\n“An Attention Free Transformer”. In: _arXiv preprint arXiv:2105.14103_ (2021).  \\n[108] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Ré. “Efectively Modeling\\nTime Series with Simple Discrete State Spaces”. In: _The International Conference on Learning Representations_\\n_(ICLR)_ . 2023.  \\n[109] Lin Zheng, Chong Wang, and Lingpeng Kong. “Linear complexity randomized self-attention mechanism”. In:\\n_International Conference on Machine Learning_ . PMLR. 2022, pp. 27011–27041.  \\n[110] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. “Efcient\\nLong Sequence Modeling via State Space Augmented Transformer”. In: _arXiv preprint arXiv:2212.08136_ (2022).  \\n23  \\n-----', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'References'}),\n",
       " Document(page_content='## A ## Discussion: Selection Mechanism  \\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence.\\nIt can also be viewed as related to “fast weights” (J. Ba et al. 2016 ), which connects classical RNNs with the\\nmechanism of linear attention (Schlag, Irie, and Schmidhuber 2021 ). However, we believe that it is a distinct\\nconcept that is worth clarifying.  \\n**Gating.** Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and\\nSchmidhuber 1997 ) and GRU (J. Chung et al. 2014 ), or the gated equation ( 5 ) n Theorem 1 . This was interpreted\\nas a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular,\\nthis aﬀects the propagation of signal through time and causes inputs to interact along the sequence length\\ndimension.  \\nHowever, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative\\ninteraction (often with an activation function). For example, elementwise multiplicative components of neural\\nnetwork architectures (that do not interact along sequence length) are now commonly referred to as gated\\narchitectures (Hua et al. 2022 ; Mehta et al. 2023 ), despite a very diﬀerent meaning than the original RNN sense.\\nThus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually\\nhave a very diﬀerent semantic meaning.  \\n**Hypernetworks.** Hypernetworks refer to neural networks whose parameters are themselves generated by smaller\\nneural networks. The original idea (Ha, Dai, and Quoc V. Le 2017 ) used it in a narrow sense to deﬁne a large\\nRNN whose recurrent parameters are generated by a smaller RNN.  \\n**Data-dependence.** Similar to hypernetworks, data-dependence can refer to any notion where some parameters\\nof the model depend on the data (Poli et al. 2023 ).  \\n**Example: GLU Activation.** To illustrate the issues with these concepts, consider a simple diagonal linear\\nlayer 푦= **_D_** 푥 , where **_D_** is a diagonal weight parameter. Now suppose that **_D_** is itself generated from a linear\\ntransformation of 푥 , with an optional nonlinearity: **_D_** = 휎( **_W_** 푥) . Since it is diagonal, the multiplication becomes\\nan elementwise product: 푦= 휎( **_W_** 푥)◦푥 .  \\nThis is a rather trivial transformation, yet it technically satisﬁes the common meanings of gating (since it has a\\nmultiplicative “branch”), hypernetworks (since the parameter **_D_** is generated by another layer), and data-dependent\\n(since **_D_** depends on the data 푥 ). However, this in fact simply deﬁnes a GLU function, which is so simple that\\nit is often considered just an activation function (Dauphin et al. 2017 ; Shazeer 2020 ) instead of a meaningful\\nlayer.  \\n**Selection.** Thus, while selection mechanisms could be considered a special case of ideas such as architectural\\ngating, hypernetworks, or data-dependence, so can an enormous range of other constructions—essentially anything\\nwith a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015 ; Vaswani et al.\\n2017 ) as well—and we ﬁnd it uninformative to think of them as such.  \\nInstead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case\\n(Theorem 1 ) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization\\nof ∆ (Funahashi and Nakamura 1993 ; Gu, Dao, et al. 2020 ; Tallec and Ollivier 2018 ). We also eschew the term\\n“gating” in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to\\nthe mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence\\nlength (Section 3.1 ). Beyond selective SSMs and gated RNNs, other examples may include input-dependent\\nconvolutions (Kosma, Nikolentzos, and Vazirgiannis 2023 ; Lioutas and Guo 2020 ; Lutati, Zimerman, and Wolf\\n2023 ; Yang et al. 2019 ) and even attention.  \\n24  \\n-----', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'A ## Discussion: Selection Mechanism'}),\n",
       " Document(page_content='## B ## Related Work  \\nWe overview several prior works related to our methods. We mention that some of the most closely related models\\ninclude recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet,\\nand RWKV.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'B ## Related Work'}),\n",
       " Document(page_content='### B.1 ### S4 Variants and Derivatives  \\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to\\nour method.  \\n- S4 (Gu, Goel, and Ré 2022 ; Gu, Johnson, Goel, et al. 2021 ) introduced the ﬁrst structured SSM, describing\\ndiagonal structure and diagonal plus low-rank (DPLR). It focused on eﬃcient convolutional algorithms for\\nDPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020 ).  \\n- DSS (Gupta, Gu, and Berant 2022 ) ﬁrst discovered the empirical eﬀectiveness of diagonal structured SSMs by\\napproximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022 ).  \\n- S5 (Smith, Warrington, and Linderman 2023 ) independently discovered the diagonal SSM approximation, and\\nis the ﬁrst S4 model to be computed recurrently with the parallel scan. However, this required lowering the\\neﬀective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input\\nsingle-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but diﬀers\\nby (i) keeping the SISO dimensions, which provides a larger eﬀective recurrent state, (ii) using a hardware-aware\\nalgorithm to overcome the computation issue, (iii) adding the selection mechanism.  \\nLu et al. ( 2023 ) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories.\\nTheir mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where **_A_** is\\nmanually set to 0 , instead of our learnable mechanism that depends on the input. It would be interesting to\\napply selective SSMs generically to this setting and probe if the model has learned to automatically reset its\\nstate on episode boundaries.  \\n- Mega (Ma et al. 2023 ) introduced a simpliﬁcation of S4 to be real- instead of complex- valued, giving it an\\ninterpretation of being an exponential moving average (EMA). They additionally make an interesting connection\\nof the discretization step of SSMs to an EMA damping term. Contrary to ﬁndings in the original S4 papers, this\\nwas the ﬁrst model to show that real-valued SSMs are empirically eﬀective in certain settings or when combined\\nwith diﬀerent architectural components.  \\n- Liquid S4 (Hasani et al. 2023 ) is also motivated by augmenting S4 with an input-dependent state transition.\\nFrom this perspective it shares similarity to selection mechanisms, although in a limited form which is still\\ncomputed convolutionally and close to LTI.  \\n- SGConv (Y. Li et al. 2023 ), Hyena (Poli et al. 2023 ), LongConv (Fu et al. 2023 ), MultiresConv (J. Shi, K. A.\\nWang, and Fox 2023 ), and Toeplitz Neural Network (Qin, Han, W. Sun, He, et al. 2023 ) all focus on the\\nconvolutional representation of S4 and create global or long convolution kernels with diﬀerent parameterizations.\\nHowever, these methods cannot do fast autoregressive inference directly.  \\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and\\nusually strictly LTI (linear time invariant).', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'B ## Related Work', 'Header 3': 'B.1 ### S4 Variants and Derivatives'}),\n",
       " Document(page_content='### B.2 ### SSM Architectures  \\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures\\nincorporating one of the previous SSMs as a black box layer.  \\n- GSS (Mehta et al. 2023 ) was the ﬁrst gated neural network architecture incorporating SSMs. It is motivated by\\nthe gated attention unit (GAU) of Hua et al. ( 2022 ) and looks quite similar to our block, except with additional\\nprojections. Most importantly, its projection contracts the model dimension to reduce the state size of the\\nSSM, while ours expands the model dimension in order to increase the state size, based on the motivation in\\nSection 3.1 .  \\n25  \\n-----  \\n- Mega (Ma et al. 2023 ) combined the EMA simpliﬁcation of S4 described above into a hybrid architecture using\\nan eﬃcient attention approximation.  \\n- H3 (Dao, Fu, Saab, et al. 2023 ) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020 ).\\nIt is the ﬁrst to generalize this formulation of linear attention to more general recurrences, which is also the\\nbasis of later architectures.  \\n- Selective S4 (J. Wang et al. 2023 ) incorporates S4 as a black box to generate a binary mask which is multiplied\\non the input. While sharing the “selection” name, we consider this an architectural modiﬁcation that is closer to\\narchitectural gating than a selection mechanism (Appendix A ). For example, we hypothesize that it would not\\nsolve the Selective Copying task because simply masking out the irrelevant inputs does not aﬀect the spacing\\nbetween the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the\\nnoise tokens are embedded to 0).  \\n- RetNet (Y. Sun et al. 2023 ) is also based on Linear Attention and very similar to H3, but reduces the inner S4\\nlayer to a special case where the state dimension is 푁= 1 . Although not framed as such, its recurrence can be\\nviewed as a special case of a linear SSM.  \\nIts primary source of improvement is using a linear attention with large head dimension, which can be viewed as\\nanother method to perform input-dependent state expansion. Using a larger head dimension in the context\\nof linear attention variants was ﬁrst done by H3, but not extensively used since this requires a proportional\\namount of extra computation. RetNet avoids this with an alternate way to parallelize the computation with a\\nvariant of standard multi-head attention instead of convolutions, made feasible by their particular special case\\nof SSMs which acts as a simple EMA.  \\n- RWKV (B. Peng et al. 2023 ) is another recent RNN designed for language modeling. It is based on AFT\\n(attention-free Transformer (S. Zhai et al. 2021 )), another variant of linear attention. Its main “WKV” mechanism\\ninvolves LTI recurrences and can be seen as the ratio of two SSMs.  \\nWe also highlight the gated attention unit (GAU) from Hua et al. ( 2022 ), which was motivated by combining the\\nTransformer’s MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4 ) combining\\nthe H3 and MLP blocks.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'B ## Related Work', 'Header 3': 'B.2 ### SSM Architectures'}),\n",
       " Document(page_content='### B.3 ### Relationship to RNNs  \\nRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state.  \\nSeveral older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016 ), quasi-RNN (QRNN) (Bradbury\\net al. 2016 ), and simple recurrent unit (SRU) (Lei 2021 ; Lei et al. 2017 ) involve forms of gated RNNs without\\ntime-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be\\nviewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs\\nabove. The main diﬀerences are:  \\n- They do not use state expansion ( 푁= 1 ) or selective **_B_** , **_C_** parameters, both of which are important for\\nperformance (Section 4.6 ).  \\n- They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +\\ndiscretization (Theorem 1 ). The connections to principled SSM theory provides better parameterizations\\nand initializations (Section 3.6 ).  \\nAdditionally, older RNNs famously suﬀered from eﬃciency issues and the vanishing gradients problem (Pascanu,\\nMikolov, and Bengio 2013 ), both caused by their sequential nature. The latter could be solved for some of the\\nabove RNNs by leveraging the parallel scan (Martin and Cundy 2018 ), but the former was diﬃcult without theory\\nlater developed for SSMs. For example, modern structured SSMs diﬀer in more careful parameterization of the\\nrecurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al. 2021 ;\\nGu, Johnson, Timalsina, et al. 2023 )), or direct analysis (Orvieto et al. 2023 )).  \\nWe also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016 ; Henaﬀ,\\nSzlam, and LeCun 2016 ; Lezcano-Casado and Martínez-Rubio 2019 ; Mhammedi et al. 2017 ; Vorontsov et al. 2017 )  \\n26  \\n-----  \\nwhich are motivated by constraining the **_A_** transition matrix to be orthogonal or unitary, in order to control\\nits eigenvalues and prevent the vanishing gradient problem. However, these had other limitations; we believe\\nthat these stem from the fact that orthogonal/unitary RNNs are also LTI. For example, they are almost always\\nevaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying\\ntask (Jing et al. 2019 ).', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'B ## Related Work', 'Header 3': 'B.3 ### Relationship to RNNs'}),\n",
       " Document(page_content='### B.4 ### Linear Attention  \\nThe Linear Attention (LA) (Katharopoulos et al. 2020 ) framework is an important result popularizing kernel\\nattention and showing how it relates to recurrent autoregressive models. Many variants have proposed alternative\\nkernels and other modiﬁcations. Random Feature Attention (RFA) (H. Peng et al. 2021 ) chooses the kernel feature\\nmap to approximate softmax attention (i.e. the exp feature map) using the random Fourier feature approximation\\nof Gaussian kernels (Rahimi and Recht 2007 ). Performer (Choromanski et al. 2021 ) ﬁnds an approximation\\nto the exponential kernel involving only positive features, which also allows the softmax normalization term.\\nTransNormer (Qin, Han, W. Sun, D. Li, et al. 2022 ) showed that the LA denominator term can be unstable\\nand proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al. 2022 ) augments RFA with a\\ncosine reweighting mechanism that incorporates positional information to emphasize locality. Linear Randomized\\nAttention (Zheng, C. Wang, and L. Kong 2022 ) generalize RFA from the perspective of importance sampling,\\nand generalize it to provide better estimates of the full softmax kernel (rather than just the exp -transformed\\nnumerator).  \\nAside from kernel attention, many other variants of eﬃcient attention exist; the survey Tay, Dehghani, Bahri,\\net al. ( 2022 ) oﬀers an extensive categorization of many of these.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'B ## Related Work', 'Header 3': 'B.4 ### Linear Attention'}),\n",
       " Document(page_content='### B.5 ### Long Context Models  \\nLong context has become a popular subject, and several recent models have claimed to scale to longer and longer\\nsequences. However, these are often from a computational standpoint and have not been extensively validated.\\nThese include:  \\n- Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023 ), a lightweight wrapper around a\\nTransformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization\\ntasks; their main result is similar to our Induction Heads extrapolation experiment (Table 2 ).  \\n- LongNet (Ding et al. 2023 ), which claimed to scale to 1B length but only evaluated on length &lt; 100퐾 for actual\\ntasks.  \\n- Hyena and HyenaDNA (Nguyen, Poli, et al. 2023 ; Poli et al. 2023 ), which claimed to leverage up to 1M context.\\nHowever, their experiments trained on proportionally more data at longer contexts, making it hard to conclude\\nif quality improvements at 1M context are due to context length or due to more data and computation.  \\n- Sparse Transformer (Child et al. 2019 ) showed a proof-of-concept of using a strided sparse attention Transformer\\nto model audio waveforms of length 2 20  = 1048576 , although did not discuss performance tradeoﬀs when\\ncontrolling for computation and model size.  \\nIn contrast, we believe this work presents one of the ﬁrst approaches to meaningfully demonstrate increasing\\nperformance with longer context.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'B ## Related Work', 'Header 3': 'B.5 ### Long Context Models'}),\n",
       " Document(page_content='## C ## Mechanics of Selective SSMs  \\n_Proof of Theorem_ _1_ _._ Consider a selective SSM (Algorithm 2 ) with 푁= 1, **_A_** = −1, **_B_** = 1, 푠 ∆ = 햫헂헇햾햺헋(푥), 휏 ∆ = 헌허햿헍헉헅헎헌 .\\nThe corresponding continuous-time SSM ( 1 ) is  \\nwhich is also called a _leaky integrator_ .  \\nℎ(푡) = −ℎ(푡) + 푥(푡)  \\n27  \\n-----  \\nThe discretization step size is  \\n∆ 푡 = 휏 ∆ (햯햺헋햺헆햾헍햾헋+ 푠 ∆ (푥 푡 ))  \\n= 헌허햿헍헉헅헎헌(햯햺헋햺헆햾헍햾헋+ 햫헂헇햾햺헋(푥 푡 ))  \\nwhere we observe that the parameter can be viewed as a learnable bias and folded into the linear projection.  \\n= 헌허햿헍헉헅헎헌(햫헂헇햾햺헋(푥 푡 ))  \\nNow applying the zero-order hold (ZOH) discretization formulas:  \\n**_A_** 푡 = exp(∆ **_A_** ) =\\n1  \\n1 + exp(햫헂헇햾햺헋(푥 푡 ) = 휎(−햫헂헇햾햺헋(푥 푡 ))  \\n= 1 −휎(햫헂헇햾햺헋(푥 푡 ))  \\n**_B_** 푡 = (∆ **_A_** ) −1 (exp(∆ **_A_** ) − **_I_** ) ⋅∆ **_B_** = −(exp(∆ **_A_** ) − **_I_** ) = 1 − **_A_**  \\n= 휎(햫헂헇햾햺헋(푥 푡 )).  \\nThus the fnal discrete recurrence ( 2a ) is  \\n푔 푡 = 휎(햫헂헇햾햺헋(푥 푡 ))  \\nas desired.  \\nℎ 푡 = (1 −푔 푡 )ℎ 푡−1 + 푔 푡 푥 푡', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'C ## Mechanics of Selective SSMs'}),\n",
       " Document(page_content='## D ## Hardware-aware Algorithm For Selective SSMs  \\nWithout input-dependent selectivity, SSMs can be eﬃciently implemented as a convolution (Dao, Fu, Saab, et al.  \\n2023 ; Gu, Goel, and Ré 2022 ), which leverages the fast Fourier transform (FFT) as primitive. With selectivity,\\nSSMs are no-longer equivalent to convolution, but we leverage the parallel associative scan. While SSM scans\\nare theoretically eﬃcient ( 푂(퐵퐿퐷푁) FLOPs, scaling linear in 퐿 ), training foundation models with selective SSMs\\nrequires them to be eﬃcient on modern hardware (GPUs) as well. We describe how we use kernel fusion and\\nrecomputation to make SSM scan fast and memory-eﬃcient. We evaluate the speed of our scan implementation\\ncompared to convolution and attention in Section 4.5 , showing that it is up to 7 × times faster than attention at\\nsequence length 32K, and is as memory-eﬃcient as the best attention implementation (FlashAttention).  \\n**Speed.** On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by\\nmemory-bandwidth (Dao, Fu, Ermon, et al. 2022 ; Ivanov et al. 2021 ; Williams, Waterman, and Patterson 2009 ).\\nThis the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to\\nsigniﬁcant speedup compared to a standard implementation.  \\nThe standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input **_A_** , **_B_** of size\\n(퐵, 퐿, 퐷, 푁) in GPU HBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel\\nassociative scan implementation to write the scan output of size (퐵, 퐿, 퐷, 푁) to GPU HBM, then multiply that scan\\noutput with **_C_** to produce an output of size (퐵, 퐿, 퐷) . However, this requires the number of memory reads/writes\\non the order of 푂(퐵퐿퐷푁) . We can instead fuse the discretization step, the scan, and the multiplication with **_C_**\\ninto one kernel:  \\n1. We read in 푂(퐵퐿퐷+ 퐷푁) bytes of memory ( ∆, **_A_** , **_B_** , **_C_** ) from slow HBM to fast SRAM.  \\n2. We discretize to produce **_A_** , **_B_** of size (퐵, 퐿, 퐷, 푁) in SRAM.  \\n3. We perform a parallel associative scan, yielding intermediate states of size (퐵, 퐿, 퐷, 푁) in SRAM.  \\n4. We multiply and sum with **_C_** , producing outputs of size (퐵, 퐿, 퐷) and write it to HBM.  \\nThis way, we reduce IOs by a factor of 푂(푁) (the state dimension), which in practice speeds up the operation by\\n20-40 times (Section 4.5 ).  \\n28  \\n-----  \\nModel Params Test Accuracy (%) at Sequence Length  \\nTable 11: ( **Induction heads** .) Models are trained on sequence length 2 8  = 256 , and tested on various sequence lengths of 2 6  = 64\\nup to 2 20  = 1048576 . \\x13 denotes perfect generalization accuracy, while \\x17 denotes out of memory.  \\n2 6 2 7\\n**2** **8**\\n2 9 2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 2 18 2 19 2 20  \\nMHA-Abs 137K \\x13 99.6 100.0 58.6 26.6 18.8 9.8 10.9 7.8 \\x17 \\x17 \\x17 \\x17 \\x17 \\x17\\nMHA-RoPE 137K \\x13 \\x13 100.0 83.6 31.3 18.4 8.6 9.0 5.5 \\x17 \\x17 \\x17 \\x17 \\x17 \\x17\\nMHA-xPos 137K \\x13 \\x13 100.0 99.6 67.6 25.4 7.0 9.0 7.8 \\x17 \\x17 \\x17 \\x17 \\x17 \\x17\\nH3 153K \\x13 \\x13 100.0 80.9 39.5 23.8 14.8 8.2 5.9 6.6 8.2 4.7 8.2 6.3 7.4\\nHyena\\n69M ∗\\n97.7 \\x13 100.0 \\x13 44.1 12.5 6.6 5.1 7.0 5.9 6.6 6.6 5.9 6.3 9.8\\nMamba 74K \\x13 \\x13 100.0 \\x13 \\x13 \\x13 \\x13 \\x13 \\x13 \\x13 \\x13 \\x13 \\x13 \\x13 \\x13  \\n∗ Most of the parameters are in learnable positional encodings.  \\nFor sequence length 퐿 too long where we cannot ﬁt the sequence in SRAM (which is much smaller than HBM), we\\nsplit the sequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate\\nscan states, we can continue the scan with the next chunk.  \\n**Memory.** We describe how we use the classical technique of recomputation to reduce the total amount of memory\\nrequired to train selective SSM layers.  \\nFrom the way we fuse the forward pass, we do not save the intermediate states of size (퐵, 퐿, 퐷, 푁) to avoid memory\\nblowup. However, these intermediate states are necessary for the backward pass to compute gradients. We instead\\nrecompute those intermediate states in the backward pass. Since the inputs ∆, **_A_** , **_B_** , **_C_** and output gradient\\nread from HBM to SRAM are of size 푂(퐵퐿푁+ 퐷푁) , and the input gradients are also of size 푂(퐵퐿푁+ 퐷푁) ,\\nrecomputation avoids the cost of reading 푂(퐵퐿푁퐷) elements from HBM. This means that recomputation of the\\nSSM states in the backward pass speeds up the computation compared to storing them and reading them from\\nHBM.  \\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize\\nthe memory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output\\nprojection). In particular, we do not save intermediate activations that take a lot of memory but are fast to\\nrecompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the\\nsame memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each\\nattention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around\\n20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16)).\\nEach selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have\\naround the same activation memory as an attention layer and an MLP layer.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'D ## Hardware-aware Algorithm For Selective SSMs'}),\n",
       " Document(page_content='## E ## Experimental Details and Additional Results  \\n### E.1 ### Synthetic Tasks  \\n**Selective Copying.** Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including\\nthe white “noise” token from Figure 2 ) and requiring models to memorize 16 “data” tokens. We use 2 layer models\\nwith a model dimension of 퐷= 64 .  \\nModels are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64 .  \\n**Induction Heads.**\\nTraining consists of randomly generating data every step, with a batch size of 8 . We choose\\nan “epoch” size of 8192 steps, and track the accuracy on ﬁxed validation sets (also randomly generated) of\\neach target sequence length. For the MHA-Abs and Mamba models, results are reported after the 25th epoch\\n( 8192 × 25 = 204800 steps). For the MHA-RoPE and MHA-xPos models, results are reported after the 50th epoch\\n( 8192 × 50 = 409600 steps). For the LTI H3 and Hyena models, results are reported after the 10th epoch ( 81920\\nsteps) because they had converged by then and failed to improve further.  \\n29  \\n-----  \\nTable 12: ( **Scaling Law Model Sizes** .) Our model sizes and hyperparameters for scaling experiments. (Model dimension and\\nnumber of heads applies only to Transformer models.)  \\nParams\\n횗 _ 횕횊횢횎횛횜 획 _ 횖횘획횎횕 횗 _ 횑횎횊획횜 / 획 _ 횑횎횊획\\nTraining steps Learning Rate Batch Size Tokens  \\n125M 12 768 12 / 64 4800 6e-4 0.5M tokens 2.5B\\n350M 24 1024 16 / 64 13500 3e-4 0.5M tokens 7B\\n760M 24 1536 16 / 96 29000 2.5e-4 0.5M tokens 15B\\n1.3B 24 2048 32 / 64 50000 2e-4 0.5M tokens 26B  \\nWe use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2푒−4 and\\n1푒−3 , and the better results are reported for each model ( 2푒−4 for all models except Mamba). The attention\\nand Hyena models did not learn at LR 1푒−3 . H3 learned at both LRs, but interestingly generalized better to\\nshorter sequences at the smaller LR of 2푒−4 . Mamba learned at both LRs, but extrapolated better at the larger\\nLR of 1푒−3 .', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'E ## Experimental Details and Additional Results', 'Header 3': 'E.1 ### Synthetic Tasks'}),\n",
       " Document(page_content='### E.2 ### Language Modeling  \\n**E.2.1** **Scaling Law Details**  \\nAll models were trained on the Pile.  \\n**Model Sizes.** Table 12 speciﬁes the model sizes we use for scaling laws. This is taken directly from the GPT3\\nspeciﬁcations (Brown et al. 2020 ), with very minor modiﬁcations. First, we changed the batch size of the 1.3B\\nmodel from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch\\nsize. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling\\nlaws (Hoﬀmann et al. 2022 ), which specify that training tokens should increase proportionally to model size.  \\n**Training Recipes.** All models used the AdamW optimizer with  \\n- gradient clip value 1.0  \\n- no dropout  \\n- weight decay 0.1  \\n- linear learning rate warmup with cosine decay  \\nBy default, the peak learning rate is the GPT3 speciﬁcation.  \\nWe give several models an “improved recipe”, inspired by changes adopted by popular large language models such\\nas PaLM (Chowdhery et al. 2023 ) and LLaMa (Touvron et al. 2023 ). These include:  \\n- no linear bias terms  \\n- linear learning rate warmup with cosine decay to 1푒−5 , with a peak value of 5× the GPT3 value  \\n- RMSNorm instead of LayerNorm  \\n**Architecture and Training Details.** Our models are:  \\n- AdamW hyperparameter 훽= (.9, .95) (the GPT3 value) instead of the PyTorch default of 훽= (.9, .999)  \\n- Transformer: The standard Transformer based on GPT3 (Table 12 ).  \\n- Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al.  \\n2021 ) and SwiGLU MLP (Shazeer 2020 ), and the improved training recipe above.  \\n- Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an\\nMLP) with standard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of\\nlayers is correspondingly increased by 1.5× to preserve parameter count.  \\n30  \\n-----  \\n- H3++: The H3 architecture with a few modiﬁcations, including (i) using the same “thin” Hyena dimensions\\nabove (ii) the improved training recipe above (iii) a linear attention head dimension of 8.  \\n- RWKV: The default RWKV model from B. Peng et al. ( 2023 ), including its modiﬁed MLP block. We also used\\nas much of its speciﬁed training recipe as possible, such as increasing the learning rates by 2× or 3× on certain\\nparameters.  \\n- RetNet: The default RetNet model from Y. Sun et al. ( 2023 ). We also gave it the improved training recipe\\nabove.  \\n- Mamba: The standard Mamba architecture, with the improved training recipe.  \\n**E.2.2** **Additional Scaling Law Ablations**  \\nWe perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws\\nin Figure 4 (Left).  \\n**Mamba Architecture: Interleaving Blocks.** We test the eﬀect of diﬀerent architectural blocks combined with\\nthe Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with\\nan extra 햼허헇헏→햲햲햬 path added. This leads to two natural ablations:  \\n- What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This\\ncan also be interpreted as taking Mamba and removing half of the SSMs.  \\n- What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted\\nas taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the\\nMLP blocks.  \\nFigure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly,\\nneither change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than\\nall models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat\\nsurprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can\\nlead to substantial improvements (Dao, Fu, Saab, et al. 2023 ; Fathi et al. 2023 ; Fathullah et al. 2023 ; Saon, Gupta,\\nand Cui 2023 ; Zuo et al. 2022 ).  \\n**H3 Architecture: Training Recipes.** Next we ablate diﬀerences between the Hyena and H3++ models, our\\nweakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the eﬀect of training\\nrecipes.  \\n- Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4 ).  \\n- Hyena+: The same architecture but with the improved training recipe described above.  \\n- H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution\\nkernel.  \\n- H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside\\nthe SSM recurrence but does not increase parameters.  \\nOur general convention is that “Model+” represents the base model with the improved training recipe, and\\n“Model++” also allows for architectural changes.  \\nFigure 9 (Right) shows that  \\n- A large improvement is achieved by the improved training recipe, which was used for many of the models in the\\nmain Figure 4 (RetNet, H3++, Transformer++, Mamba).  \\n- The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with ﬁndings throughout this\\npaper.  \\n- The head dimension expansion improves performance, consistent with one of our main themes that expanded\\nstate dimension improves performance for SSMs (Section 3 ).  \\n31  \\n-----  \\n|Scaling Laws on The Pile (Sequence Length 2048) Mamba Mamba-MLP scale) Mamba-MHA 101 (log 9 × 100 Perplexity 8 × 100 7 × 100 1019 1020 FLOPs (log scale)|Scaling Laws on The Pile (Sequence Length 2048) Hyena Hyena+ scale) H3+ H3++ (log Perplexity 101 1019 1020 FLOPs (log scale)|\\n|---|---|  \\nFigure 9: ( **Scaling laws: extra ablations** .) ( _Left_ ) Instead of ( _Right_ ) Instead of  \\n**E.2.3** **Downstream Evaluation Details**  \\nThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens. For the 1.3B\\nmodel, we use a batch size of 1M tokens to be consistent with the GPT3 speciﬁcations. We report the perplexity\\non the Pile validation set, and for this metric only compare to models trained on the same dataset and with the\\nsame tokenizer, in particular Pythia and RWKV.  \\nFor downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021 ),\\nas done by most work in this area. We evaluate on the following tasks/datasets that measure common sense\\nreasoning:  \\n- LAMBADA (Paperno et al. 2016 ).  \\n- HellaSwag (Zellers et al. 2019 ).  \\n- PIQA (Bisk et al. 2020 ).  \\n- ARC-challenge (P. Clark et al. 2018 ).  \\n- ARC-easy: an easy subset of ARC-challenge.  \\n- WinoGrande (Sakaguchi et al. 2021 ).  \\nWe report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence\\nlength for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these\\ntask).', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'E ## Experimental Details and Additional Results', 'Header 3': 'E.2 ### Language Modeling'}),\n",
       " Document(page_content='### E.3 ### DNA Modeling  \\n**E.3.1** **Pretraining Details**  \\nWe describe the dataset and training procedure of the HG38 pretraining task in more detail.  \\nThe dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021 ); the training split\\ncontains a total of 푆= 34021 segments of length 2 17  = 131072 that cover the genome, for a total of approximately\\n4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending\\nindex), and can be extended if necessary (e.g. to get longer segments).  \\nWe deviate from HyenaDNA when the training sequence length is not 2 17 . HyenaDNA always takes a ﬁxed\\nsub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length\\neach epoch is ﬁxed to 34021 samples and doesn’t necessarily go through the whole genome. On the other hand, we\\nuse the entire training data:  \\n- When the context length 퐿 is less than (or equal to) 2 17 , we divide up each segment into non-overlapping  \\nsub-segments of length 퐿 , so that there are 푆×  2 17  \\n퐿 total samples and 푆× 2 17 ≈4.5퐵 tokens per epoch.  \\n- When the context length 퐿 is greater than 2 17 , we turn each segment into two samples, one that begins with the\\nprescribed segment and one that ends with the prescribed segment. Thus each epoch has 2푆 items and 2푆퐿  \\n32  \\n-----  \\ntokens per epoch. For example, at sequence length 2 18  = 262144 there are 4× as many tokens as the default,\\nand at sequence length 2 20  there are 16× as many tokens.  \\nOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2 ).\\nFor example, we use the AdamW with (훽 1 , 훽 2 ) = (0.9, 0.95) , no dropout, weight decay 0.1 . We use a cosine learning\\nrate scheduler with linear warmup for 10% of total steps.  \\n**E.3.2** **Scaling: Model Size Details**  \\n**Models.** The models we consider are:  \\n- Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su\\net al. 2021 ). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani\\net al. 2017 ).  \\n- HyenaDNA: the Hyena model from Nguyen, Poli, et al. ( 2023 ) and Poli et al. ( 2023 ), which is roughly a\\nTransformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP.  \\n- Mamba: the standard Mamba architecture.  \\n**Model Sizes.** We use the following model sizes.  \\nBlocks 4 5 6 7 8 10 12\\nModel Dimension 64 96 128 192 256 384 512\\nParams (Approx.) 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M  \\nNote that the number of blocks for Mamba is doubled, because one Transformer “layer” includes both the MHA and\\nMLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4 ).  \\n**Training.**\\nFor each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1푒−3, 2푒−\\n3, 4푒−3, 8푒−3} . The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal\\nMamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates\\n(2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the\\nupper range of the sweep, it is possible that our results are still suboptimal.)  \\nNote that, in contrast to standard LM scaling laws (Table 12 ), our LR held constant across model sizes for\\nsimplicity. The optimal LR should go down for larger models, but we didn’t ﬁnd a noticeable eﬀect at the small\\nmodel sizes (at most a few million parameters) we considered.  \\n**E.3.3** **Scaling: Context Length Details**  \\nWe use a total batch size of 2 24  ≈16푀 tokens per training step, for every sequence length (e.g. at length 2 20  \\nthere are 16 segments per batch and at length 2 10  there are 16384 segments per batch). This is a large batch size\\nrelative to the model size by usual LM standards, but note that a batch size of 2 23  is the minimum possible on a\\nmachine with 8 GPUs and sequence length of 2 2 0 , and that HyenaDNA used much larger batches of 2 28 .  \\nThe learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same\\nlearning rate of 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest\\ncontext length.  \\n**Sequence Length Warmup.** Following (Nguyen, Poli, et al. 2023 ), we use sequence length warmup (SLW)\\nduring pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from\\n2 10  = 1024 . (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are\\nspent proportionally. In particular, each stage up to length 2 17  processes the same number of tokens, but 4× as\\nmany tokens are processed at length 2 18 , 8× as many at length 2 19 , and 16× as many at length 2 20 .)  \\nUnlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively\\nhalved as the sequence lengths are doubled in each stage.  \\n33  \\n-----  \\nTable 13: ( **Great Apes DNA Classifcation** .) Accuracy after fne-tuning on sequences of length 2 10  = 1024 up to 2 20  = 1048576\\nusing pretrained models of the same context length. Random guessing is 20%.  \\nModel Params Accuracy (%) at Sequence Length  \\n2 10 2 12 2 14 2 16 2 18 2 20  \\nHyenaDNA 1.4M 28.04 28.43 41.17 42.22 31.10 54.87\\nMamba 1.4M 31.47 27.50 27.66 40.72 42.41 **71.67**  \\nMamba 7M 30.00 29.01 31.48 43.73 56.60 **81.31**  \\n**Remark E.1.** _We also note that the schedule was not tuned, and we never experimented with turning of sequence length_\\n_warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at_\\n_similar lengths (Section_ _4.4_ _), and it is possible that it is not necessary for DNA pretraining either._  \\n**E.3.4** **Species (Great Apes) Classifcation**  \\nModels are causal and therefore only the last element (across the sequence length) of the model’s output is used for\\nthe classiﬁcation head. Note that we control for the total number of elements in the loss function per gradient step.\\nThe pretraining objective includes all positions across the sequence length, so that 횋횊횝회횑 _ 횜횒횣횎×횜횎횚횞횎횗회횎 _ 횕횎횗횐횝횑\\nis held constant; in other words, the batch size decreases as the sequence length increases. However, for a\\nclassiﬁcation task, since only the last position enters the loss, the batch size itself is held constant. Note that this\\nalso means that ﬁne-tuning models with longer sequence lengths is more computationally expensive.  \\nTraining consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which\\nare all independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then\\nuniformly picking a contiguous segment of DNA.  \\nFollowing (Nguyen, Poli, et al. 2023 ), models with a maximum context length greater than 2 14  = 16384 use\\nsequence length warmup with 1 epoch at length 2 14  = 16384 , 1 epoch at length 2 15  = 32768 , 1 epoch at length\\n2 16  = 65536 , and so on up to the maximum sequence length. For example, the model with 2 20  = 1048576 context\\nundergoes 6 epochs of sequence length warmup before 4 more epochs at its maximum sequence length.  \\nResults for the Species classiﬁcation task are in Table 13 .  \\nThe learning rate for all Hyena models is ퟺ횎−ퟻ , while the learning rate for all Mamba models is ퟷ횎−ퟺ . These\\nwere found by performing learning rate sweeps for each model among {1푒−5, 2푒−5, 4푒−5, 1푒−4, 2푒−4} for\\nthe smaller sequence lengths (2 10 , 2 12 , 2 14 , 2 16 ) , and these values were consistently found to be the best for each\\nmodel. An abridged learning rate sweep was done at length 2 18 , which agreed with these values, and a single run\\nat length 2 20  was performed (as described above, the computational cost of these experiments is proportional to\\nthe sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear\\nwarmup to the maximum learning rate, and 5 epochs of cosine decay down to 1푒−6 . The unusually long learning\\nrate warmup schedule was chosen because the sequence length warmup was also long (e.g. comprising 6 out of 10\\nepochs for the model with context length 2 20 ); we did not experiment with this choice.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'E ## Experimental Details and Additional Results', 'Header 3': 'E.3 ### DNA Modeling'}),\n",
       " Document(page_content='### E.4 ### Audio Details  \\n**E.4.1** **YouTubeMix Audio Pretraining**  \\n**Model.**\\nWe use a model with 3 blocks per stage ( 3 × 5 = 15 total Mamba blocks), pooling factor 푝= 16 , and\\nouter dimension 퐷= 64 , for about 3.5M parameters.  \\n**Dataset.** The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of\\n256 .  \\nThe dataset consists of clips of up to 1 minute long, or length 960000 , which is subsampled and divided into\\nsegments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16 ,  \\n34  \\n-----  \\nTable 14: YouTubeMix length scaling sequence lengths and batch sizes.  \\nSequence length Batch size Tokens / batch  \\nAudio Waveforms - SSM Parameterization  \\nAudio Waveforms - SSM Parameterization  \\n468 × 2048 = 958464 1 958464\\n234 × 2048 = 479232 2 958464\\n117 × 2048 = 239616 4 958464\\n59 × 2048 = 120832 8 966656\\n30 × 2048 = 61440 16 983040\\n15 × 2048 = 30720 32 983040\\n8 × 2048 = 16384 64 1048576\\n4 × 2048 = 8192 128 1048576  \\n1.50  \\n1.45  \\n1.45  \\n1.40  \\n1.40  \\nMamba (S6)\\n+complex\\n-selective B/C\\n-selective\\n(Mamba-S4)  \\n1.35  \\nS4+MLP\\nMamba (S6)\\n+complex\\n-selective B/C\\n-selective\\n(Mamba-S4)  \\n1.35  \\n1.30  \\n1.30  \\n1.25  \\n1.25  \\n10 4 10 5  \\n10 4 10 5  \\nSequence Length  \\nSequence Length  \\nFigure 10: ( **Audio Pretraining (YouTubeMix) Ablations** .) As a uniformly-sampled “continuous” signal modality, audio wave-\\nforms actually beneft from LTI models which have matching inductive bias. ( _Left_ ) Homogenous models (all blocks have the same\\nparameterization) ( _Right_ ) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as fgure\\non left.  \\nand we want the resulting sequence length to be a a multiple of 8 for hardware eﬃciency, the longest possible\\nsequence is 468 × 2048 = 958464 . The rest of our sequence lengths are deﬁned by successively halving this and\\nrounding up to the nearest multiple of 2048 .  \\nTable 14 lists the speciﬁcations used in Figure 7 . Beyond the varying batch sizes, the number of valid segments in\\nthe training set varied between diﬀerent sequence lengths (e.g. the number of training steps per epoch was not\\nconstant for diﬀerent points in the graph), which may have contributed to kinks in the scaling curves.  \\n**Training.**\\nModels were trained for 200퐾 training steps with a maximum learning rate of 0.002 , 20퐾 (10%)\\nwarmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains).  \\n**Additional Ablations: SSM Parameterizations.** We investigate SSM parameterizations on long-form audio\\nwaveform pretraining in the setting of Figure 7 . The setting is modiﬁed slightly to use larger models ( 8 layers and\\n퐷= 64 for 6M params, the SaShiMi default), shorter sequences ( 2 11  = 2048 to 2 18  = 262144 instead of 2 13  to 2 20 ),\\nlower LR ( 0.001 from 0.002 ), and shorter training cycles (100K instead of 200K steps).  \\nFigure 10 shows that the change from S4 → S6 (i.e. the selection mechanism) is not always beneﬁcial. On long-form\\naudio waveforms, it in fact signiﬁcantly hampers performance, which may be intuitive from the point of view\\nthat audio is uniformly sampled and very smooth, and therefore beneﬁts from continuous linear time-invariant\\n(LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer\\ninside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture\\nMamba-S6.  \\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers.\\nThe performance diﬀerences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio\\nsignal should be LTI, but once they are “tokenized” and compressed by the outer layers, the inner layers no longer\\nneed to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.  \\n35  \\n-----  \\n**E.4.2** **SC09 Speech Generation**  \\nAutoregressive training largely followed the autoregressive language modeling protocol, such as  \\n- Learning rate warmup for 10% of total steps  \\n- Weight decay 0.1  \\n- AdamW optimizer with 훽= (0.9, 0.95)  \\n- Gradient clip value 0.1  \\nWe used a learning rate of 0.002 and 200000 training steps at a batch size of 16 .  \\nThe large Mamba model in Table 4 has 15 layers per stage with an outer dimension of 퐷= 96 and pooling factor\\n4 . We note that this dataset is small (training went through 100 epochs) and for this large model, there was\\nsigniﬁcant overﬁtting of the BPB or NLL. However, automated metrics of generated samples continually improving\\nthroughout training.  \\nThe models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of 홳= 64\\nand pooling factor 4 . The S4+MLP block has roughly 2퐷 2  + 4퐷 2  parameters (expansion factor 2 in the MLP).\\nThe Transformer block has 4퐷 2  + 2퐷 2  parameters (expansion factor 1 in the MLP). The Mamba block has the\\nusual ≈6퐷 2  parameters. All models have roughly 6M total parameters.', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'E ## Experimental Details and Additional Results', 'Header 3': 'E.4 ### Audio Details'}),\n",
       " Document(page_content='### E.5 ### Efciency Benchmark  \\n**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3 ),\\nagainst convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost\\nof other operations outside of this core operation, such as computing the convolutional kernel in global-convolution\\nmodels, or computing the QKV projections in attention.  \\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing\\nthe parameters **_A_** , **_B_** , **_C_** in HBM.  \\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all\\nthe large parameters in HBM.  \\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs\\nand the ﬁlters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The\\ntheoretical complexity is 푂(퐿log(퐿)) for sequence length 퐿 .  \\nFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023 )),\\nwith causal mask. Note that FlashAttention-2 with causal mask is about 1.7 × faster than without causal mask,\\nsince approximately only half of the attention entries are computed.  \\nWe use batch size of 1 and increase the sequence length from 2 9  = 512 , 2 10  ≈1퐾 , 2 11  ≈2퐾 , up to 2 19  ≈500퐾\\n(some of the baselines run out of memory before reaching 500K). We use a model dimension of 퐷= 1024 and state\\ndimension 푁= 16 . We measure with BF16 inputs, which is the data type most commonly used for large scale\\ntraining.  \\n**End-to-end Inference.** We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba\\n6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard\\nTransformer implementation in the Huggingface `transformers` library.  \\nWe set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,\\n32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s)\\nas batch size × 128∕ time taken . We repeat the measurements 3 times and take the average. Measurements are\\ndone on an A100 80GB PCIe GPU.  \\n**Memory Benchmark.** The memory usage simply scales proportionally to the size of the activation tensors, as\\nwith most deep sequence models. We report measurements of the training memory requirements of 125M models  \\n36  \\n-----  \\nTable 15: ( **Memory benchmark** .) Mamba’s memory footprint is comparable to the most optimized Transformer. Results for 125M\\nmodels.  \\nBatch size Transformer (w/ FlashAttention-2) Mamba  \\n1 4.6GB 4.8GB\\n2 5.2GB 5.8GB\\n4 6.9GB 7.3GB\\n8 11.5GB 12.3GB\\n16 20.7GB 23.1GB\\n32 34.5GB 38.2GB  \\non 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-eﬃcient\\nTransformer implementation we are aware of (with kernel fusion from `torch.compile` and with FlashAttention-2).\\nTable 15 shows that Mamba’s memory requirement is comparable to a similar-sized Transformer with an extremely\\noptimized implementation, and we expect further improvement in Mamba’s memory footprint in the future.  \\n37  \\n-----', metadata={'Header 1': 'Mamba: Linear-Time Sequence Modeling with Selective State Spaces', 'Header 2': 'E ## Experimental Details and Additional Results', 'Header 3': 'E.5 ### Efciency Benchmark'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the document based on markdown headers\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed PDF Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Embedding\n",
    "response = oai_client.embeddings.create(\n",
    "    input=\"This is a sample text\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "\n",
    "for i, chunk in tqdm(enumerate(md_header_splits), total=len(md_header_splits)):\n",
    "    response = oai_client.embeddings.create(\n",
    "        input=chunk.page_content,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "    _chunk = {\n",
    "        \"id\": i,\n",
    "        \"embedding\": response.data[0].embedding,\n",
    "        \"metadata\": {\n",
    "            \"text\": chunk.page_content\n",
    "        }\n",
    "    }\n",
    "    chunks.append(_chunk)\n",
    "\n",
    "# Note:\n",
    "# This is a simple example of how to index the nodes using openai embeddings one by one.\n",
    "# In practice, you should batch the embeddings and index them in bulk and not use a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the Embeddings to a Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upstash_vector import Index, Vector\n",
    "\n",
    "index = Index.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the chunks to vector objects\n",
    "vectors = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    vector = Vector(\n",
    "        id=chunk['id'],\n",
    "        vector=chunk['embedding'],\n",
    "        metadata=chunk['metadata']\n",
    "    )\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsert the vectors to the index\n",
    "index.upsert(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the Jamba language model?\"\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings for the query\n",
    "query_vector = oai_client.embeddings.create(\n",
    "    input=query,\n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "query_result = index.query(\n",
    "    vector=query_vector,\n",
    "    include_metadata=True,\n",
    "    include_vectors=False,\n",
    "    top_k=TOP_K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7320111\n",
      "ID: 38\n",
      "Metadata: {'text': '### E.4 ### Audio Details  \\n**E.4.1** **YouTubeMix Audio Pretraining**  \\n**Model.**\\nWe use a model with 3 blocks per stage ( 3 × 5 = 15 total Mamba blocks), pooling factor 푝= 16 , and\\nouter dimension 퐷= 64 , for about 3.5M parameters.  \\n**Dataset.** The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of\\n256 .  \\nThe dataset consists of clips of up to 1 minute long, or length 960000 , which is subsampled and divided into\\nsegments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16 ,  \\n34  \\n-----  \\nTable 14: YouTubeMix length scaling sequence lengths and batch sizes.  \\nSequence length Batch size Tokens / batch  \\nAudio Waveforms - SSM Parameterization  \\nAudio Waveforms - SSM Parameterization  \\n468 × 2048 = 958464 1 958464\\n234 × 2048 = 479232 2 958464\\n117 × 2048 = 239616 4 958464\\n59 × 2048 = 120832 8 966656\\n30 × 2048 = 61440 16 983040\\n15 × 2048 = 30720 32 983040\\n8 × 2048 = 16384 64 1048576\\n4 × 2048 = 8192 128 1048576  \\n1.50  \\n1.45  \\n1.45  \\n1.40  \\n1.40  \\nMamba (S6)\\n+complex\\n-selective B/C\\n-selective\\n(Mamba-S4)  \\n1.35  \\nS4+MLP\\nMamba (S6)\\n+complex\\n-selective B/C\\n-selective\\n(Mamba-S4)  \\n1.35  \\n1.30  \\n1.30  \\n1.25  \\n1.25  \\n10 4 10 5  \\n10 4 10 5  \\nSequence Length  \\nSequence Length  \\nFigure 10: ( **Audio Pretraining (YouTubeMix) Ablations** .) As a uniformly-sampled “continuous” signal modality, audio wave-\\nforms actually beneft from LTI models which have matching inductive bias. ( _Left_ ) Homogenous models (all blocks have the same\\nparameterization) ( _Right_ ) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as fgure\\non left.  \\nand we want the resulting sequence length to be a a multiple of 8 for hardware eﬃciency, the longest possible\\nsequence is 468 × 2048 = 958464 . The rest of our sequence lengths are deﬁned by successively halving this and\\nrounding up to the nearest multiple of 2048 .  \\nTable 14 lists the speciﬁcations used in Figure 7 . Beyond the varying batch sizes, the number of valid segments in\\nthe training set varied between diﬀerent sequence lengths (e.g. the number of training steps per epoch was not\\nconstant for diﬀerent points in the graph), which may have contributed to kinks in the scaling curves.  \\n**Training.**\\nModels were trained for 200퐾 training steps with a maximum learning rate of 0.002 , 20퐾 (10%)\\nwarmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains).  \\n**Additional Ablations: SSM Parameterizations.** We investigate SSM parameterizations on long-form audio\\nwaveform pretraining in the setting of Figure 7 . The setting is modiﬁed slightly to use larger models ( 8 layers and\\n퐷= 64 for 6M params, the SaShiMi default), shorter sequences ( 2 11  = 2048 to 2 18  = 262144 instead of 2 13  to 2 20 ),\\nlower LR ( 0.001 from 0.002 ), and shorter training cycles (100K instead of 200K steps).  \\nFigure 10 shows that the change from S4 → S6 (i.e. the selection mechanism) is not always beneﬁcial. On long-form\\naudio waveforms, it in fact signiﬁcantly hampers performance, which may be intuitive from the point of view\\nthat audio is uniformly sampled and very smooth, and therefore beneﬁts from continuous linear time-invariant\\n(LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer\\ninside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture\\nMamba-S6.  \\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers.\\nThe performance diﬀerences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio\\nsignal should be LTI, but once they are “tokenized” and compressed by the outer layers, the inner layers no longer\\nneed to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.  \\n35  \\n-----  \\n**E.4.2** **SC09 Speech Generation**  \\nAutoregressive training largely followed the autoregressive language modeling protocol, such as  \\n- Learning rate warmup for 10% of total steps  \\n- Weight decay 0.1  \\n- AdamW optimizer with 훽= (0.9, 0.95)  \\n- Gradient clip value 0.1  \\nWe used a learning rate of 0.002 and 200000 training steps at a batch size of 16 .  \\nThe large Mamba model in Table 4 has 15 layers per stage with an outer dimension of 퐷= 96 and pooling factor\\n4 . We note that this dataset is small (training went through 100 epochs) and for this large model, there was\\nsigniﬁcant overﬁtting of the BPB or NLL. However, automated metrics of generated samples continually improving\\nthroughout training.  \\nThe models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of 홳= 64\\nand pooling factor 4 . The S4+MLP block has roughly 2퐷 2  + 4퐷 2  parameters (expansion factor 2 in the MLP).\\nThe Transformer block has 4퐷 2  + 2퐷 2  parameters (expansion factor 1 in the MLP). The Mamba block has the\\nusual ≈6퐷 2  parameters. All models have roughly 6M total parameters.'}\n",
      "\n",
      "Score: 0.72657394\n",
      "ID: 0\n",
      "Metadata: {'text': '# Mamba: Linear-Time Sequence Modeling with Selective State Spaces  \\n### Albert Gu *\\n1 and Tri Dao * 2  \\n1 Machine Learning Department, Carnegie Mellon University\\n2 Department of Computer Science, Princeton University\\n`agu@cs.cmu.edu` , `tri@tridao.me`  \\nAbstract  \\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally\\nbased on the Transformer architecture and its core attention module. Many subquadratic-time architectures\\nsuch as linear attention, gated convolution and recurrent models, and structured state space models (SSMs)\\nhave been developed to address Transformers’ computational ineﬃciency on long sequences, but they have not\\nperformed as well as attention on important modalities such as language. We identify that a key weakness of\\nsuch models is their inability to perform content-based reasoning, and make several improvements. First, simply\\nletting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing\\nthe model to selectively propagate or forget information along the sequence length dimension depending on\\nthe current token. Second, even though this change prevents the use of eﬃcient convolutions, we design a\\nhardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliﬁed\\nend-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast\\ninference (5 × higher throughput than Transformers) and linear scaling in sequence length, and its performance\\nimproves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves\\nstate-of-the-art performance across several modalities such as language, audio, and genomics. On language\\nmodeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice\\nits size, both in pretraining and downstream evaluation.'}\n",
      "\n",
      "Score: 0.72598743\n",
      "ID: 18\n",
      "Metadata: {'text': '### 4.2 ### Language Modeling  \\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on\\nboth pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to\\nmirror GPT3 speciﬁcations. We use the Pile dataset (L. Gao, Biderman, et al. 2020 ), and follow the training\\nrecipe described in Brown et al. ( 2020 ). All training details are in Appendix E.2 .  \\n**4.2.1** **Scaling Laws**  \\nFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the\\nstrongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa  \\n11  \\n-----  \\n|Scaling Laws on The Pile (Sequence Length 2048) 2 × 101 Hyena RWKV scale) Transformer RetNet (log H3++ Transformer++ 101 Perplexity Mamba 6 × 100 1019 1020 FLOPs (log scale)|Scaling Laws on The Pile (Sequence Length 8192) 2 × 101 Hyena RWKV scale) Transformer RetNet (log H3++ Transformer++ 101 Perplexity Mamba 6 × 100 1019 1020 FLOPs (log scale)|\\n|---|---|  \\nFigure 4: ( **Scaling Laws** .) Models of size ≈125푀 to ≈1.3퐵 parameters, trained on the Pile. Mamba scales better than all other\\nattention-free models and is the frst to match the performance of a very strong “Transformer++” recipe that has now become\\nstandard, particularly as the sequence length grows.  \\narchitectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher\\nlearning rates). We also compare against other recent subquadratic architectures (Figure 4 ). All model details are\\nin Appendix E.2 .  \\nFigure 4 shows scaling laws under the standard Chinchilla (Hoﬀmann et al. 2022 ) protocol, on models from\\n≈125푀 to ≈1.3퐵 parameters. Mamba is the ﬁrst attention-free model to match the performance of a very\\nstrong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length\\ngrows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior\\nstrong recurrent models that can also be interpreted as SSMs, due to a lack of eﬃcient implementation leading to\\nout-of-memory or unrealistic computation requirements.  \\n**4.2.2** **Downstream Evaluations**  \\nTable 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We\\ncompare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al.\\n2023 ) and RWKV (B. Peng et al. 2023 ) which were trained with the same tokenizer, dataset, and training length\\n(300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV\\nwas trained with context length 1024.)'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in query_result:\n",
    "    print(\"Score:\", result.score)\n",
    "    print(\"ID:\", result.id)\n",
    "    print(\"Metadata:\", result.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_result_str(metadata):\n",
    "    '''\n",
    "    Build a string from the metadata dictionary of a query result for adding to the context of the LLM.\n",
    "    '''\n",
    "    text = metadata['text']\n",
    "    _meta = {\n",
    "        k: v for k, v in metadata.items() if k != 'text'\n",
    "    }\n",
    "    \n",
    "    meta_str = \"\\n\".join([f\"{k}: {v}\" for k, v in _meta.items()])\n",
    "    return f\"{meta_str}\\n\\n{text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Mamba: Linear-Time Sequence Modeling with Selective State Spaces  \n",
      "### Albert Gu *\n",
      "1 and Tri Dao * 2  \n",
      "1 Machine Learning Department, Carnegie Mellon University\n",
      "2 Department of Computer Science, Princeton University\n",
      "`agu@cs.cmu.edu` , `tri@tridao.me`  \n",
      "Abstract  \n",
      "Foundation models, now powering most of the exciting applications in deep learning, are almost universally\n",
      "based on the Transformer architecture and its core attention module. Many subquadratic-time architectures\n",
      "such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs)\n",
      "have been developed to address Transformers’ computational ineﬃciency on long sequences, but they have not\n",
      "performed as well as attention on important modalities such as language. We identify that a key weakness of\n",
      "such models is their inability to perform content-based reasoning, and make several improvements. First, simply\n",
      "letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing\n",
      "the model to selectively propagate or forget information along the sequence length dimension depending on\n",
      "the current token. Second, even though this change prevents the use of eﬃcient convolutions, we design a\n",
      "hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliﬁed\n",
      "end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast\n",
      "inference (5 × higher throughput than Transformers) and linear scaling in sequence length, and its performance\n",
      "improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves\n",
      "state-of-the-art performance across several modalities such as language, audio, and genomics. On language\n",
      "modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice\n",
      "its size, both in pretraining and downstream evaluation.\n"
     ]
    }
   ],
   "source": [
    "print(build_result_str(query_result[1].metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = \"\"\"Retrieved context to answer the query is as follows:\n",
    "{context_str}\n",
    "\"\"\"\n",
    "\n",
    "def build_context_prompt(retrieval_results):\n",
    "    context_str = \"\\n\\n---------------------\\n\\n\".join([build_result_str(r.metadata) for r in retrieval_results])\n",
    "    return context_prompt.format(\n",
    "        context_str=context_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context to answer the query is as follows:\n",
      "\n",
      "\n",
      "### E.4 ### Audio Details  \n",
      "**E.4.1** **YouTubeMix Audio Pretraining**  \n",
      "**Model.**\n",
      "We use a model with 3 blocks per stage ( 3 × 5 = 15 total Mamba blocks), pooling factor 푝= 16 , and\n",
      "outer dimension 퐷= 64 , for about 3.5M parameters.  \n",
      "**Dataset.** The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of\n",
      "256 .  \n",
      "The dataset consists of clips of up to 1 minute long, or length 960000 , which is subsampled and divided into\n",
      "segments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16 ,  \n",
      "34  \n",
      "-----  \n",
      "Table 14: YouTubeMix length scaling sequence lengths and batch sizes.  \n",
      "Sequence length Batch size Tokens / batch  \n",
      "Audio Waveforms - SSM Parameterization  \n",
      "Audio Waveforms - SSM Parameterization  \n",
      "468 × 2048 = 958464 1 958464\n",
      "234 × 2048 = 479232 2 958464\n",
      "117 × 2048 = 239616 4 958464\n",
      "59 × 2048 = 120832 8 966656\n",
      "30 × 2048 = 61440 16 983040\n",
      "15 × 2048 = 30720 32 983040\n",
      "8 × 2048 = 16384 64 1048576\n",
      "4 × 2048 = 8192 128 1048576  \n",
      "1.50  \n",
      "1.45  \n",
      "1.45  \n",
      "1.40  \n",
      "1.40  \n",
      "Mamba (S6)\n",
      "+complex\n",
      "-selective B/C\n",
      "-selective\n",
      "(Mamba-S4)  \n",
      "1.35  \n",
      "S4+MLP\n",
      "Mamba (S6)\n",
      "+complex\n",
      "-selective B/C\n",
      "-selective\n",
      "(Mamba-S4)  \n",
      "1.35  \n",
      "1.30  \n",
      "1.30  \n",
      "1.25  \n",
      "1.25  \n",
      "10 4 10 5  \n",
      "10 4 10 5  \n",
      "Sequence Length  \n",
      "Sequence Length  \n",
      "Figure 10: ( **Audio Pretraining (YouTubeMix) Ablations** .) As a uniformly-sampled “continuous” signal modality, audio wave-\n",
      "forms actually beneft from LTI models which have matching inductive bias. ( _Left_ ) Homogenous models (all blocks have the same\n",
      "parameterization) ( _Right_ ) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as fgure\n",
      "on left.  \n",
      "and we want the resulting sequence length to be a a multiple of 8 for hardware eﬃciency, the longest possible\n",
      "sequence is 468 × 2048 = 958464 . The rest of our sequence lengths are deﬁned by successively halving this and\n",
      "rounding up to the nearest multiple of 2048 .  \n",
      "Table 14 lists the speciﬁcations used in Figure 7 . Beyond the varying batch sizes, the number of valid segments in\n",
      "the training set varied between diﬀerent sequence lengths (e.g. the number of training steps per epoch was not\n",
      "constant for diﬀerent points in the graph), which may have contributed to kinks in the scaling curves.  \n",
      "**Training.**\n",
      "Models were trained for 200퐾 training steps with a maximum learning rate of 0.002 , 20퐾 (10%)\n",
      "warmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains).  \n",
      "**Additional Ablations: SSM Parameterizations.** We investigate SSM parameterizations on long-form audio\n",
      "waveform pretraining in the setting of Figure 7 . The setting is modiﬁed slightly to use larger models ( 8 layers and\n",
      "퐷= 64 for 6M params, the SaShiMi default), shorter sequences ( 2 11  = 2048 to 2 18  = 262144 instead of 2 13  to 2 20 ),\n",
      "lower LR ( 0.001 from 0.002 ), and shorter training cycles (100K instead of 200K steps).  \n",
      "Figure 10 shows that the change from S4 → S6 (i.e. the selection mechanism) is not always beneﬁcial. On long-form\n",
      "audio waveforms, it in fact signiﬁcantly hampers performance, which may be intuitive from the point of view\n",
      "that audio is uniformly sampled and very smooth, and therefore beneﬁts from continuous linear time-invariant\n",
      "(LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer\n",
      "inside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture\n",
      "Mamba-S6.  \n",
      "However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers.\n",
      "The performance diﬀerences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio\n",
      "signal should be LTI, but once they are “tokenized” and compressed by the outer layers, the inner layers no longer\n",
      "need to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.  \n",
      "35  \n",
      "-----  \n",
      "**E.4.2** **SC09 Speech Generation**  \n",
      "Autoregressive training largely followed the autoregressive language modeling protocol, such as  \n",
      "- Learning rate warmup for 10% of total steps  \n",
      "- Weight decay 0.1  \n",
      "- AdamW optimizer with 훽= (0.9, 0.95)  \n",
      "- Gradient clip value 0.1  \n",
      "We used a learning rate of 0.002 and 200000 training steps at a batch size of 16 .  \n",
      "The large Mamba model in Table 4 has 15 layers per stage with an outer dimension of 퐷= 96 and pooling factor\n",
      "4 . We note that this dataset is small (training went through 100 epochs) and for this large model, there was\n",
      "signiﬁcant overﬁtting of the BPB or NLL. However, automated metrics of generated samples continually improving\n",
      "throughout training.  \n",
      "The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of 홳= 64\n",
      "and pooling factor 4 . The S4+MLP block has roughly 2퐷 2  + 4퐷 2  parameters (expansion factor 2 in the MLP).\n",
      "The Transformer block has 4퐷 2  + 2퐷 2  parameters (expansion factor 1 in the MLP). The Mamba block has the\n",
      "usual ≈6퐷 2  parameters. All models have roughly 6M total parameters.\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "\n",
      "# Mamba: Linear-Time Sequence Modeling with Selective State Spaces  \n",
      "### Albert Gu *\n",
      "1 and Tri Dao * 2  \n",
      "1 Machine Learning Department, Carnegie Mellon University\n",
      "2 Department of Computer Science, Princeton University\n",
      "`agu@cs.cmu.edu` , `tri@tridao.me`  \n",
      "Abstract  \n",
      "Foundation models, now powering most of the exciting applications in deep learning, are almost universally\n",
      "based on the Transformer architecture and its core attention module. Many subquadratic-time architectures\n",
      "such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs)\n",
      "have been developed to address Transformers’ computational ineﬃciency on long sequences, but they have not\n",
      "performed as well as attention on important modalities such as language. We identify that a key weakness of\n",
      "such models is their inability to perform content-based reasoning, and make several improvements. First, simply\n",
      "letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing\n",
      "the model to selectively propagate or forget information along the sequence length dimension depending on\n",
      "the current token. Second, even though this change prevents the use of eﬃcient convolutions, we design a\n",
      "hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliﬁed\n",
      "end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast\n",
      "inference (5 × higher throughput than Transformers) and linear scaling in sequence length, and its performance\n",
      "improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves\n",
      "state-of-the-art performance across several modalities such as language, audio, and genomics. On language\n",
      "modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice\n",
      "its size, both in pretraining and downstream evaluation.\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "\n",
      "### 4.2 ### Language Modeling  \n",
      "We evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on\n",
      "both pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to\n",
      "mirror GPT3 speciﬁcations. We use the Pile dataset (L. Gao, Biderman, et al. 2020 ), and follow the training\n",
      "recipe described in Brown et al. ( 2020 ). All training details are in Appendix E.2 .  \n",
      "**4.2.1** **Scaling Laws**  \n",
      "For baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the\n",
      "strongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa  \n",
      "11  \n",
      "-----  \n",
      "|Scaling Laws on The Pile (Sequence Length 2048) 2 × 101 Hyena RWKV scale) Transformer RetNet (log H3++ Transformer++ 101 Perplexity Mamba 6 × 100 1019 1020 FLOPs (log scale)|Scaling Laws on The Pile (Sequence Length 8192) 2 × 101 Hyena RWKV scale) Transformer RetNet (log H3++ Transformer++ 101 Perplexity Mamba 6 × 100 1019 1020 FLOPs (log scale)|\n",
      "|---|---|  \n",
      "Figure 4: ( **Scaling Laws** .) Models of size ≈125푀 to ≈1.3퐵 parameters, trained on the Pile. Mamba scales better than all other\n",
      "attention-free models and is the frst to match the performance of a very strong “Transformer++” recipe that has now become\n",
      "standard, particularly as the sequence length grows.  \n",
      "architectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher\n",
      "learning rates). We also compare against other recent subquadratic architectures (Figure 4 ). All model details are\n",
      "in Appendix E.2 .  \n",
      "Figure 4 shows scaling laws under the standard Chinchilla (Hoﬀmann et al. 2022 ) protocol, on models from\n",
      "≈125푀 to ≈1.3퐵 parameters. Mamba is the ﬁrst attention-free model to match the performance of a very\n",
      "strong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length\n",
      "grows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior\n",
      "strong recurrent models that can also be interpreted as SSMs, due to a lack of eﬃcient implementation leading to\n",
      "out-of-memory or unrealistic computation requirements.  \n",
      "**4.2.2** **Downstream Evaluations**  \n",
      "Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We\n",
      "compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al.\n",
      "2023 ) and RWKV (B. Peng et al. 2023 ) which were trained with the same tokenizer, dataset, and training length\n",
      "(300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV\n",
      "was trained with context length 1024.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(build_context_prompt(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_retrieval(search_query: str) -> str:\n",
    "    '''\n",
    "    This function let's you semantically retrieve relevant context chunks from a given document based on a query.\n",
    "\n",
    "    Arguments:\n",
    "        query (str): The query to search for in the document. Based on the original user query, write a good search query\n",
    "                     which is more logically sound to retrieve the relevant information from the document.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved context chunks from the document based on the search query formatted as a string.\n",
    "    '''\n",
    "    # Get the embeddings for the search query\n",
    "    query_vector = oai_client.embeddings.create(\n",
    "        input=search_query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # Execute the query\n",
    "    query_result = index.query(\n",
    "        vector=query_vector,\n",
    "        include_metadata=True,\n",
    "        include_vectors=False,\n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "    return build_context_prompt(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context to answer the query is as follows:\n",
      "\n",
      "\n",
      "### E.5 ### Efciency Benchmark  \n",
      "**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3 ),\n",
      "against convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost\n",
      "of other operations outside of this core operation, such as computing the convolutional kernel in global-convolution\n",
      "models, or computing the QKV projections in attention.  \n",
      "As a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing\n",
      "the parameters **_A_** , **_B_** , **_C_** in HBM.  \n",
      "Our scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all\n",
      "the large parameters in HBM.  \n",
      "For convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs\n",
      "and the ﬁlters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The\n",
      "theoretical complexity is 푂(퐿log(퐿)) for sequence length 퐿 .  \n",
      "For attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023 )),\n",
      "with causal mask. Note that FlashAttention-2 with causal mask is about 1.7 × faster than without causal mask,\n",
      "since approximately only half of the attention entries are computed.  \n",
      "We use batch size of 1 and increase the sequence length from 2 9  = 512 , 2 10  ≈1퐾 , 2 11  ≈2퐾 , up to 2 19  ≈500퐾\n",
      "(some of the baselines run out of memory before reaching 500K). We use a model dimension of 퐷= 1024 and state\n",
      "dimension 푁= 16 . We measure with BF16 inputs, which is the data type most commonly used for large scale\n",
      "training.  \n",
      "**End-to-end Inference.** We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba\n",
      "6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard\n",
      "Transformer implementation in the Huggingface `transformers` library.  \n",
      "We set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,\n",
      "32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s)\n",
      "as batch size × 128∕ time taken . We repeat the measurements 3 times and take the average. Measurements are\n",
      "done on an A100 80GB PCIe GPU.  \n",
      "**Memory Benchmark.** The memory usage simply scales proportionally to the size of the activation tensors, as\n",
      "with most deep sequence models. We report measurements of the training memory requirements of 125M models  \n",
      "36  \n",
      "-----  \n",
      "Table 15: ( **Memory benchmark** .) Mamba’s memory footprint is comparable to the most optimized Transformer. Results for 125M\n",
      "models.  \n",
      "Batch size Transformer (w/ FlashAttention-2) Mamba  \n",
      "1 4.6GB 4.8GB\n",
      "2 5.2GB 5.8GB\n",
      "4 6.9GB 7.3GB\n",
      "8 11.5GB 12.3GB\n",
      "16 20.7GB 23.1GB\n",
      "32 34.5GB 38.2GB  \n",
      "on 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-eﬃcient\n",
      "Transformer implementation we are aware of (with kernel fusion from `torch.compile` and with FlashAttention-2).\n",
      "Table 15 shows that Mamba’s memory requirement is comparable to a similar-sized Transformer with an extremely\n",
      "optimized implementation, and we expect further improvement in Mamba’s memory footprint in the future.  \n",
      "37  \n",
      "-----\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "\n",
      "## 4 ## Empirical Evaluation  \n",
      "In Section 4.1 we test Mamba’s ability to solve the two synthetic tasks motivated in Section 3.1 . We then evaluate\n",
      "on three domains, each evaluated on autoregressive pretraining as well as downstream tasks.  \n",
      "- Section 4.2 : language model pretraining (scaling laws), and zero-shot downstream evaluation.  \n",
      "- Section 4.3 : DNA sequence pretraining, and ﬁne-tuning on a long-sequence classiﬁcation task.  \n",
      "- Section 4.4 : audio waveform pretraining, and the quality of autoregressively generated speech clips.  \n",
      "Finally, Section 4.5 shows Mamba’s computational eﬃciency at both training and inference time, and Section 4.6\n",
      "ablates various components of the architecture and selective SSMs.\n",
      "\n",
      "---------------------\n",
      "\n",
      "\n",
      "\n",
      "### E.4 ### Audio Details  \n",
      "**E.4.1** **YouTubeMix Audio Pretraining**  \n",
      "**Model.**\n",
      "We use a model with 3 blocks per stage ( 3 × 5 = 15 total Mamba blocks), pooling factor 푝= 16 , and\n",
      "outer dimension 퐷= 64 , for about 3.5M parameters.  \n",
      "**Dataset.** The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of\n",
      "256 .  \n",
      "The dataset consists of clips of up to 1 minute long, or length 960000 , which is subsampled and divided into\n",
      "segments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16 ,  \n",
      "34  \n",
      "-----  \n",
      "Table 14: YouTubeMix length scaling sequence lengths and batch sizes.  \n",
      "Sequence length Batch size Tokens / batch  \n",
      "Audio Waveforms - SSM Parameterization  \n",
      "Audio Waveforms - SSM Parameterization  \n",
      "468 × 2048 = 958464 1 958464\n",
      "234 × 2048 = 479232 2 958464\n",
      "117 × 2048 = 239616 4 958464\n",
      "59 × 2048 = 120832 8 966656\n",
      "30 × 2048 = 61440 16 983040\n",
      "15 × 2048 = 30720 32 983040\n",
      "8 × 2048 = 16384 64 1048576\n",
      "4 × 2048 = 8192 128 1048576  \n",
      "1.50  \n",
      "1.45  \n",
      "1.45  \n",
      "1.40  \n",
      "1.40  \n",
      "Mamba (S6)\n",
      "+complex\n",
      "-selective B/C\n",
      "-selective\n",
      "(Mamba-S4)  \n",
      "1.35  \n",
      "S4+MLP\n",
      "Mamba (S6)\n",
      "+complex\n",
      "-selective B/C\n",
      "-selective\n",
      "(Mamba-S4)  \n",
      "1.35  \n",
      "1.30  \n",
      "1.30  \n",
      "1.25  \n",
      "1.25  \n",
      "10 4 10 5  \n",
      "10 4 10 5  \n",
      "Sequence Length  \n",
      "Sequence Length  \n",
      "Figure 10: ( **Audio Pretraining (YouTubeMix) Ablations** .) As a uniformly-sampled “continuous” signal modality, audio wave-\n",
      "forms actually beneft from LTI models which have matching inductive bias. ( _Left_ ) Homogenous models (all blocks have the same\n",
      "parameterization) ( _Right_ ) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as fgure\n",
      "on left.  \n",
      "and we want the resulting sequence length to be a a multiple of 8 for hardware eﬃciency, the longest possible\n",
      "sequence is 468 × 2048 = 958464 . The rest of our sequence lengths are deﬁned by successively halving this and\n",
      "rounding up to the nearest multiple of 2048 .  \n",
      "Table 14 lists the speciﬁcations used in Figure 7 . Beyond the varying batch sizes, the number of valid segments in\n",
      "the training set varied between diﬀerent sequence lengths (e.g. the number of training steps per epoch was not\n",
      "constant for diﬀerent points in the graph), which may have contributed to kinks in the scaling curves.  \n",
      "**Training.**\n",
      "Models were trained for 200퐾 training steps with a maximum learning rate of 0.002 , 20퐾 (10%)\n",
      "warmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains).  \n",
      "**Additional Ablations: SSM Parameterizations.** We investigate SSM parameterizations on long-form audio\n",
      "waveform pretraining in the setting of Figure 7 . The setting is modiﬁed slightly to use larger models ( 8 layers and\n",
      "퐷= 64 for 6M params, the SaShiMi default), shorter sequences ( 2 11  = 2048 to 2 18  = 262144 instead of 2 13  to 2 20 ),\n",
      "lower LR ( 0.001 from 0.002 ), and shorter training cycles (100K instead of 200K steps).  \n",
      "Figure 10 shows that the change from S4 → S6 (i.e. the selection mechanism) is not always beneﬁcial. On long-form\n",
      "audio waveforms, it in fact signiﬁcantly hampers performance, which may be intuitive from the point of view\n",
      "that audio is uniformly sampled and very smooth, and therefore beneﬁts from continuous linear time-invariant\n",
      "(LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer\n",
      "inside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture\n",
      "Mamba-S6.  \n",
      "However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers.\n",
      "The performance diﬀerences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio\n",
      "signal should be LTI, but once they are “tokenized” and compressed by the outer layers, the inner layers no longer\n",
      "need to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.  \n",
      "35  \n",
      "-----  \n",
      "**E.4.2** **SC09 Speech Generation**  \n",
      "Autoregressive training largely followed the autoregressive language modeling protocol, such as  \n",
      "- Learning rate warmup for 10% of total steps  \n",
      "- Weight decay 0.1  \n",
      "- AdamW optimizer with 훽= (0.9, 0.95)  \n",
      "- Gradient clip value 0.1  \n",
      "We used a learning rate of 0.002 and 200000 training steps at a batch size of 16 .  \n",
      "The large Mamba model in Table 4 has 15 layers per stage with an outer dimension of 퐷= 96 and pooling factor\n",
      "4 . We note that this dataset is small (training went through 100 epochs) and for this large model, there was\n",
      "signiﬁcant overﬁtting of the BPB or NLL. However, automated metrics of generated samples continually improving\n",
      "throughout training.  \n",
      "The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of 홳= 64\n",
      "and pooling factor 4 . The S4+MLP block has roughly 2퐷 2  + 4퐷 2  parameters (expansion factor 2 in the MLP).\n",
      "The Transformer block has 4퐷 2  + 2퐷 2  parameters (expansion factor 1 in the MLP). The Mamba block has the\n",
      "usual ≈6퐷 2  parameters. All models have roughly 6M total parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context_retrieval(\"on what hardware was mamba trained on?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a Q&A bot. You are here to answer questions based on the context retrieved\n",
    "from a vector index of the chunks of a document. You are prohibited from using prior knowledge and you\n",
    "can only use the context given. If you need more information, please ask the user. If you cannot answer \n",
    "the question from the context, you can tell the user that you cannot answer the question. You can also \n",
    "ask for more information from the user.\"\"\"\n",
    "\n",
    "system_message = {\n",
    "    'role': 'system',\n",
    "    'content': system_prompt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mapping of the tool name to the function that should be called\n",
    "available_functions = {\n",
    "    \"context_retrieval\": context_retrieval,\n",
    "}\n",
    "\n",
    "# Here we have only one function, but you can have multiple as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a JSON schema for the tools that the LLM can use\n",
    "# Here we define a schema for the context_retrieval function\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"context_retrieval\",\n",
    "            \"description\": \"This function let's you semantically retrieve relevant context chunks from a given document based on a query. Based on the original user query, write a good search query which is more logically sound to retrieve the relevant information from the document. You might even have to break down the user query into multiple search queries and call this function multiple times separately. This function finally returns the retrieved context chunks from the document based on the search query formatted as a string.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The sub-query to search for in the document.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_turn(user_message, messages, tools, model='gpt-3.5-turbo', temperature=0.2, max_tokens=512, verbose=True, **kwargs):\n",
    "\n",
    "    # Add user message to messages list\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': user_message\n",
    "    })\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n<< User Message >>\")\n",
    "        print(user_message)\n",
    "    \n",
    "    # Send the conversation and available tools/functions to the model\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "        **kwargs\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    # Add the response to the messages list\n",
    "    messages.append(response_message)\n",
    "\n",
    "    # Check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "\n",
    "        # Call each of the functions\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\n<< Calling Function `{function_name}` with Args: {function_args} >>\")\n",
    "\n",
    "            # Call the function\n",
    "            function_response = function_to_call(**function_args)\n",
    "\n",
    "            # if verbose:\n",
    "            #     print(\"<< Function Response >>\")\n",
    "            #     print(function_response)\n",
    "\n",
    "            # Add the function response to the messages list\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Get a new response from the model based on the function response\n",
    "        second_response = oai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        second_response_message = second_response.choices[0].message\n",
    "        messages.append(second_response_message)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n<< Response >>\")\n",
    "            print(second_response_message.content)\n",
    "\n",
    "        return second_response_message, messages\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n<< Response >>\")\n",
    "        print(response_message.content)\n",
    "\n",
    "    return response_message, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    system_message\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<< User Message >>\n",
      "hardware of jamba and what is the conclusion?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<< Calling Function `context_retrieval` with Args: {'search_query': 'hardware of jamba'} >>\n",
      "\n",
      "<< Response >>\n",
      "The document provides information about benchmarking the speed and memory of a Selective State Space Model (SSM) called Mamba. The hardware used for the benchmarks included an A100 80GB PCIe GPU. The speed benchmark compared the efficiency of the SSM scan operation with convolution and attention implementations. It mentions that the Mamba SSM scan was faster than the best attention implementation known (FlashAttention-2) beyond a sequence length of 2K. Additionally, the end-to-end inference throughput of Mamba was measured against a standard Transformer (GPT3 architecture) at different sizes.\n",
      "\n",
      "However, the document does not explicitly provide a conclusion about the hardware of the Mamba system.\n"
     ]
    }
   ],
   "source": [
    "response, messages = conversation_turn(\n",
    "    \"hardware of jamba and what is the conclusion?\",\n",
    "    messages,\n",
    "    tools_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<< User Message >>\n",
      "I was talking about JAMBA with a J, anything on that you have?\n",
      "\n",
      "<< Calling Function `context_retrieval` with Args: {'search_query': 'hardware of jamba'} >>\n",
      "\n",
      "<< Response >>\n",
      "The information retrieved from the document does not contain details about any hardware related to Jamba specifically. The document primarily discusses the benchmarks, efficiency, speed, and memory of a Selective State Space Model (SSM) called Mamba. It compares the performance of Mamba against attention and convolution implementations on an A100 80GB PCIe GPU. Unfortunately, there is no mention of hardware related to Jamba in the provided context.\n"
     ]
    }
   ],
   "source": [
    "response, messages = conversation_turn(\n",
    "    \"I was talking about JAMBA with a J, anything on that you have?\",\n",
    "    messages,\n",
    "    tools_schema\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
